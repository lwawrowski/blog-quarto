[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Metody przetwarzania i analizy danych w R",
    "section": "",
    "text": "Wprowadzenie\nLiteratura podstawowa:\n\nWprowadzenie do modelowania predycyjnego\nHands-On Machine Learning with R\n\nLiteratura dodatkowa:\n\nR for Data Science\nIntroduction to Data Science\nApplied Statistics with R\nData Science Live Book\nTidy Modeling with R\n\nSkrypty z zajęć",
    "crumbs": [
      "Wprowadzenie"
    ]
  },
  {
    "objectID": "01-eda.html",
    "href": "01-eda.html",
    "title": "1  Analiza opisowa",
    "section": "",
    "text": "1.1 Wprowadzenie\nPrezentacja - quarto\nPrezentacja - EDA",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Analiza opisowa</span>"
    ]
  },
  {
    "objectID": "01-eda.html#eksploracyjna-analiza-danych",
    "href": "01-eda.html#eksploracyjna-analiza-danych",
    "title": "1  Analiza opisowa",
    "section": "1.2 Eksploracyjna analiza danych",
    "text": "1.2 Eksploracyjna analiza danych\nExplanatory Data Analysis (EDA) polega na opisie i wizualizacji danych bez zakładania hipotez badawczych. W R dostępnych jest wiele pakietów, które wspierają ten proces - The Landscape of R Packages for Automated Exploratory Data Analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Analiza opisowa</span>"
    ]
  },
  {
    "objectID": "02-testy.html",
    "href": "02-testy.html",
    "title": "2  Testowanie hipotez",
    "section": "",
    "text": "2.1 Wprowadzenie\nDo rozwiązania wybranych zagadnień analizy statystycznej wystarczą metody weryfikacji hipotez statystycznych. Taki proces można przedstawić w następujących krokach:\nWymienione powyżej nowe pojęcia zostaną wyjaśnione poniżej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#wprowadzenie",
    "href": "02-testy.html#wprowadzenie",
    "title": "2  Testowanie hipotez",
    "section": "",
    "text": "Sformułowanie dwóch wykluczających się hipotez - zerowej \\(H_0\\) oraz alternatywnej \\(H_1\\)\nWybór odpowiedniego testu statystycznego\nOkreślenie dopuszczalnego prawdopodobieństwo popełnienia błędu I rodzaju (czyli poziomu istotności \\(\\alpha\\))\nPodjęcie decyzji",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#hipoteza-statystyczna",
    "href": "02-testy.html#hipoteza-statystyczna",
    "title": "2  Testowanie hipotez",
    "section": "2.2 Hipoteza statystyczna",
    "text": "2.2 Hipoteza statystyczna\nPrzypuszczenie dotyczące własności analizowanej cechy, np. średnia w populacji jest równa 10, rozkład cechy jest normalny.\nFormułuje się zawsze dwie hipotezy: hipotezę zerową (\\(H_0\\)) i hipotezę alternatywną (\\(H_1\\)). Hipoteza zerowa jest hipotezą mówiącą o równości:\n\\(H_0: \\bar{x}=10\\)\nZ kolei hipoteza alternatywna zakłada coś przeciwnego:\n\\(H_1: \\bar{x}\\neq 10\\)\nZamiast znaku nierówności (\\(\\neq\\)) może się także pojawić znak mniejszości (\\(&lt;\\)) lub większości (\\(&gt;\\)).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#poziom-istotności-i-wartość-p",
    "href": "02-testy.html#poziom-istotności-i-wartość-p",
    "title": "2  Testowanie hipotez",
    "section": "2.3 Poziom istotności i wartość p",
    "text": "2.3 Poziom istotności i wartość p\nHipotezy statystyczne weryfikuje się przy określonym poziomie istotności \\(\\alpha\\), który wskazuje maksymalny poziom akceptowalnego błędu (najczęściej \\(\\alpha=0,05\\)).\nWiększość programów statystycznych podaje w wynikach testu wartość p. Jest to najostrzejszy poziom istotności, przy którym możemy odrzucić hipotezę \\(H_0\\). Jest to rozwiązanie bardzo popularne, ale nie pozbawione wad. Dokładny opis potencjalnych zagrożeń można znaleźć w artykule.\nGeneralnie jeśli \\(p &lt; \\alpha\\) - odrzucamy hipotezę zerową.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#testy-statystyczne",
    "href": "02-testy.html#testy-statystyczne",
    "title": "2  Testowanie hipotez",
    "section": "2.4 Testy statystyczne",
    "text": "2.4 Testy statystyczne\nW zależności od tego co chcemy weryfikować należy wybrać odpowiedni test. Tabela poniżej przedstawia dosyć wyczerpującą klasyfikację testów pobraną ze strony.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#zbiór-danych",
    "href": "02-testy.html#zbiór-danych",
    "title": "2  Testowanie hipotez",
    "section": "2.5 Zbiór danych",
    "text": "2.5 Zbiór danych\nBędziemy działać na zbiorze danych dotyczącym pracowników przedsiębiorstwa. Poniżej znajduje się opis cech znajdujących się w tym zbiorze,\n\nid - kod pracownika\nplec - płeć pracownika (0 - mężczyzna, 1 - kobieta)\ndata_urodz - data urodzenia\nedukacja - wykształcenie (w latach nauki)\nkat_pracownika - grupa pracownicza (1 - specjalista, 2 - menedżer, 3 - konsultant)\nbwynagrodzenie - bieżące wynagrodzenie\npwynagrodzenie - początkowe wynagrodzenie\nstaz - staż pracy (w miesiącach)\ndoswiadczenie - poprzednie zatrudnienie (w miesiącach)\nzwiazki - przynależność do związków zawodowych (0 - nie, 1 - tak)\nwiek - wiek (w latach)\n\n\nlibrary(tidyverse)\nlibrary(readxl)\n\npracownicy &lt;- read_excel(\"data/pracownicy.xlsx\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#test-niezależności",
    "href": "02-testy.html#test-niezależności",
    "title": "2  Testowanie hipotez",
    "section": "2.6 Test niezależności",
    "text": "2.6 Test niezależności\nZa pomocą testu niezależności \\(\\chi^2\\) (chi-kwadrat) można sprawdzić czy pomiędzy dwiema cechami jakościowymi występuje zależność. Układ hipotez jest następujący:\n\n\\(H_0:\\) zmienne są niezależne,\n\\(H_1:\\) zmienne nie są niezależne.\n\nW programie R test niezależności można wywołać za pomocą funkcji chisq.test() z pakietu stats. Jako argument tej funkcji należy podać tablicę kontyngencji. W przypadku operowania na danych jednostkowych można ją utworzyć poprzez funkcję table(). Jeżeli wprowadzamy liczebności ręcznie to należy zadbać o to, żeby wprowadzony obiekt był typu matrix.\nPrzykład\nCzy pomiędzy zmienną płeć, a zmienną przynależność do związków zawodowych istnieje zależność?\nW pierwszym kroku określamy hipotezy badawcze:\n\\(H_0\\): pomiędzy płcią a przynależnością do związków nie ma zależności\n\\(H_1\\): pomiędzy płcią a przynależnością do związków jest zależność\noraz przyjmujemy poziom istotności - weźmy standardową wartość \\(\\alpha = 0,05\\).\nW pierwszej kolejności popatrzmy na tabelę krzyżową (kontyngencji) zawierającą liczebności poszczególnych kombinacji wariantów.\n\ntable(pracownicy$plec, pracownicy$zwiazki)\n\n   \n      0   1\n  0 194  64\n  1 176  40\n\n\nWartości w tej tabeli nie wskazują na liczniejszą reprezentację jednej z płci w związkach zawodowych. Zweryfikujemy zatem wskazaną hipotezę zerową z wykorzystaniem testu \\(\\chi^2\\).\n\nchisq.test(table(pracownicy$plec, pracownicy$zwiazki))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(pracownicy$plec, pracownicy$zwiazki)\nX-squared = 2.3592, df = 1, p-value = 0.1245\n\n\nPrzy poziomie istotności \\(\\alpha = 0,05\\), wartości p (0.1245) jest większa od wartości \\(\\alpha\\), zatem nie ma podstaw do odrzucenia hipotezy zerowej. Można stwierdzić, że nie ma zależności pomiędzy zmiennymi płeć i przynależność do związków zawodowych.\nPrzykład\nCzy pomiędzy płcią, a grupami bieżącego wynagrodzenia zdefiniowanymi przez medianę istnieje zależność?\n\\(H_0\\): pomiędzy płcią a grupami wynagrodzenia nie ma zależności\n\\(H_1\\): pomiędzy płcią a grupami wynagrodzenia jest zależność\nW pierwszej kolejności tworzymy nową cechą zamieniając cechę bwynagrodzenie na zmienną jakościową posiadającą dwa warianty: poniżej mediany i powyżej mediany.\n\npracownicy &lt;- pracownicy %&gt;% \n  mutate(bwyn_mediana=cut(x = bwynagrodzenie,\n                          breaks = c(min(bwynagrodzenie),\n                                     median(bwynagrodzenie),\n                                     max(bwynagrodzenie)),\n                          include.lowest = TRUE))\n\ntable(pracownicy$plec, pracownicy$bwyn_mediana)\n\n   \n    [1.58e+04,2.89e+04] (2.89e+04,1.35e+05]\n  0                  73                 185\n  1                 164                  52\n\n\nW tym przypadku wygląd tablicy krzyżowej może sugerować występowanie zależności.\n\nchisq.test(table(pracownicy$plec, pracownicy$bwyn_mediana))\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(pracownicy$plec, pracownicy$bwyn_mediana)\nX-squared = 104.8, df = 1, p-value &lt; 2.2e-16\n\n\nTest \\(\\chi^2\\) to potwierdza - mamy podstawy do odrzucenia hipotezy zerowej na korzyść hipotezy alternatywnej - istnieje zależność pomiędzy płcią, a grupami wynagrodzenia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#test-proporcji",
    "href": "02-testy.html#test-proporcji",
    "title": "2  Testowanie hipotez",
    "section": "2.7 Test proporcji",
    "text": "2.7 Test proporcji\nTest proporcji pozwala odpowiedzieć na pytanie czy odsetki w jednej, dwóch lub więcej grupach różnią się od siebie istotnie. Dla jednej próby układ hipotez został przedstawiony poniżej:\n\n\\(H_0: p=p_0\\)\n\\(H_1: p \\neq p_0\\) lub \\(H_1: p &gt; p_0\\) lub \\(H_1: p &lt; p_0\\)\n\nUkład hipotez w przypadku dwóch prób jest następujący:\n\n\\(H_0: p_1=p_2\\)\n\\(H_1: p_1 \\neq p_2\\) lub \\(H_1: p_1 &gt; p_2\\) lub \\(H_1: p_1 &lt; p_2\\)\n\nDla \\(k\\) badanych prób hipotezę zerową i alternatywną można zapisać w następująco:\n\n\\(H_0: p_1=p_2=p3=...=p_k\\)\n\\(H_1: \\exists \\; p_i \\neq p_j\\)\n\nW takim przypadku hipoteza alternatywna oznacza, że co najmniej jeden odsetek różni się istotnie od pozostałych.\nFunkcja prop.test z pakietu stats umożliwia przeprowadzanie testu proporcji w programie R. Jako argumenty należy podać wektor, który zawiera licznik badanych odsetków - x, oraz wektor zawierający wartości mianownika - n. W przypadku jednej próby należy jeszcze dodać argument p, którego wartość oznacza weryfikowany odsetek.\nPrzykład\nWysunięto przypuszczenie, że palacze papierosów stanowią jednakowy odsetek wśród mężczyzn i kobiet. W celu sprawdzenia tej hipotezy wylosowano 500 mężczyzn i 600 kobiet. Okazało się, że wśród mężczyzn było 200 palaczy, a wśród kobiet 250.\n\\(H_0\\): odsetek palaczy wg płci jest taki sam\n\\(H_1\\): odsetek palaczy różni się wg płci\n\nprop.test(x = c(200,250), n = c(500,600))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(200, 250) out of c(500, 600)\nX-squared = 0.24824, df = 1, p-value = 0.6183\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.07680992  0.04347659\nsample estimates:\n   prop 1    prop 2 \n0.4000000 0.4166667 \n\n\nPrzy poziomie istotności 0,05 nie ma podstaw do odrzucenia H0 - odsetek palaczy jest taki sam w grupach płci.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#testowanie-normalności---test-shapiro-wilka",
    "href": "02-testy.html#testowanie-normalności---test-shapiro-wilka",
    "title": "2  Testowanie hipotez",
    "section": "2.8 Testowanie normalności - test Shapiro-Wilka",
    "text": "2.8 Testowanie normalności - test Shapiro-Wilka\nTesty parametryczne z reguły wymagają spełnienia założenia o normalności rozkładu. W celu weryfikacji tego założenia należy wykorzystać jeden z testów normalności.\nW celu formalnego zweryfikowania rozkładu cechy można wykorzystać test Shapiro-Wilka. Układ hipotez z tym teście jest następujący:\n\n\\(H_0: F(x) = F_0(x)\\) - rozkład cechy ma rozkład normalny\n\\(H_1: F(x) \\neq F_0(x)\\) - rozkład cechy nie ma rozkładu normalnego\n\nW przeprowadzonych dotychczas symulacjach wykazano, że test Shapiro-Wilka ma największą moc spośród testów normalności, niemniej jego ograniczeniem jest maksymalna liczba obserwacji, która wynosi 50001.\nW programie R test Shapiro-Wilka można uruchomić za pomocą funkcji shapiro.test() jako argument podając wektor wartości liczbowych, który chcemy zweryfikować.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#testowanie-normalności---wykres-kwantyl-kwantyl",
    "href": "02-testy.html#testowanie-normalności---wykres-kwantyl-kwantyl",
    "title": "2  Testowanie hipotez",
    "section": "2.9 Testowanie normalności - wykres kwantyl-kwantyl",
    "text": "2.9 Testowanie normalności - wykres kwantyl-kwantyl\nNormalność rozkładu może także zostać zweryfikowana poprzez utworzenie wykresu przedstawiającego porównanie wartości oryginalnych oraz odpowiadającym im wartości pochodzących z rozkładu normalnego. Dodatkowo prowadzona jest linia regresji pomiędzy otrzymanymi wartościami. Punkty przebiegające w pobliżu tej linii oznaczają, że rozkład tej cechy jest normalny.\nNa wykresie przedstawiony jest wykres kwantyl-kwantyl dla 50 wartości wylosowanych z rozkładu normalnego i z rozkładu jednostajnego.\n\n\n\n\n\n\n\n\n\nJak można zauważyć punkty na wykresie po lewej stronie nie odbiegają znacząco od linii prostej, zatem można przypuszczać, że rozkład tej cechy jest normalny. Z kolei na wykresie po prawej stronie obserwuje się odstępstwo od rozkładu normalnego - wartości na krańcach linii są od niej oddalone.\nPrzykład\nCzy cecha doświadczenie ma rozkład normalny? Sprawdź za pomocą odpowiedniego testu oraz wykresu kwantyl-kwantyl.\n\\(H_0\\): doświadczenie ma rozkład normalny\n\\(H_1\\): doświadczenie nie ma rozkładu normalnego\n\nshapiro.test(pracownicy$doswiadczenie)\n\n\n    Shapiro-Wilk normality test\n\ndata:  pracownicy$doswiadczenie\nW = 0.8136, p-value &lt; 2.2e-16\n\n\nNa poziomie \\(\\alpha = 0,05\\) Odrzucamy \\(H_0\\) (p &lt; \\(\\alpha\\)) - doświadczenie nie ma rozkładu normalnego. Sprawdźmy jeszcze jak te wartości wyglądają na wykresie kwantyl-kwantyl.\n\nggplot(pracownicy, aes(sample = doswiadczenie)) +\n  stat_qq() +\n  stat_qq_line()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#testowanie-wariancji---test-bartletta",
    "href": "02-testy.html#testowanie-wariancji---test-bartletta",
    "title": "2  Testowanie hipotez",
    "section": "2.10 Testowanie wariancji - test Bartletta",
    "text": "2.10 Testowanie wariancji - test Bartletta\nOprócz założenia o normalności, niektóre metody statystyczne wymagają także równości wariancji.\nJeśli chcemy sprawdzić homogeniczność wariancji w dwóch lub więcej grupach to należy skorzystać z testu Bartletta:\n\n\\(H_0: s^2_1=s^2_2= s^2_3 =...=s^2_k\\)\n\\(H_1: \\exists_{i,j\\in\\{1,..,k\\}} \\; s^2_i \\neq s^2_j\\)\n\nFunkcja bartlett.test() w programie R umożliwia zastosowanie tego testu. Argumenty do tej funkcji można przekazać na dwa sposoby. Pierwszy polega na przypisaniu do argumentu x wektora zawierającego wartości cechy, a do argumentu g wektora zawierającego identyfikatory poszczególnych grup. Drugi sposób to zadeklarowanie formuły w postaci zmienna_analizowa ~ zmienna_grupująca oraz podanie zbioru danych przypisanego do argumentu data.\nPrzykład\nSprawdźmy czy wariancje zmiennej doświadczenie w grupach płci są takie same.\n\\(H_0\\): wariancje doświadczenia są takie same w grupach płci\n\\(H_1\\): wariancje doświadczenia nie są takie same w grupach płci\nFunkcję weryfikującą \\(H_0\\) można zapisać na dwa sposoby - wynik zawsze będzie taki sam.\n\nbartlett.test(x = pracownicy$doswiadczenie, g = pracownicy$plec)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  pracownicy$doswiadczenie and pracownicy$plec\nBartlett's K-squared = 4.7659, df = 1, p-value = 0.02903\n\n\n\nbartlett.test(pracownicy$doswiadczenie ~ pracownicy$plec)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  pracownicy$doswiadczenie by pracownicy$plec\nBartlett's K-squared = 4.7659, df = 1, p-value = 0.02903\n\n\nPrzyjmując poziom istotności \\(\\alpha = 0,05\\) odrzucamy hipotezę zerową stwierdzając, że wariancje różnią się w grupach płci. Z kolei dopuszczając niższy poziom istotności \\(\\alpha = 0,01\\) podjęlibyśmy decyzję o braku podstaw do odrzucenia \\(H_0\\) i nieistotnej różnicy pomiędzy grupami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#testowanie-średnich",
    "href": "02-testy.html#testowanie-średnich",
    "title": "2  Testowanie hipotez",
    "section": "2.11 Testowanie średnich",
    "text": "2.11 Testowanie średnich\nW przypadku testowania wartości przeciętnych należy wprowadzić pojęcie prób zależnych i niezależnych:\n\npróby zależne (paired) - analizowane są te same jednostki, ale różne cechy.\npróby niezależne (unpaired) - analizowane są różne jednostki, ale ta sama cecha.\n\nW zależności od tego czy spełnione są odpowiednie założenia dotyczące normalności cechy oraz równości wariancji należy wybrać odpowiedni test według poniższego diagramu.\n\n\n2.11.1 Test t-średnich\nWeryfikacja równości średnich może odbywać się na zasadzie porównania wartości średniej w jednej grupie z arbitralnie przyjętym poziomem lub w dwóch różnych grupach. W pierwszym przypadku rozważamy układ hipotez:\n\n\\(H_0: m = m_0\\)\n\\(H_1: m \\neq m_0\\) lub \\(H_1: m &lt; m_0\\) lub \\(H_1: m &gt; m_0\\)\n\nnatomiast w drugim przypadku hipotezy będą wyglądać następująco:\n\n\\(H_0: m_1 = m_2\\)\n\\(H_1: m_1 \\neq m_2\\) lub \\(H_1: m_1 &lt; m_2\\) lub \\(H_1: m_1 &gt; m_2\\)\n\nAlternatywnie hipotezę zerową można zapisać jako \\(m_1 - m_2 = 0\\) czyli sprawdzamy czy różnica pomiędzy grupami istotnie różni się od zera.\nW funkcji t.test() z pakietu stats w przypadku jednej próby należy podać argument x czyli wektor z wartościami, które są analizowane oraz wartość, z którą tą średnią porównujemy (argument mu, który domyślnie jest równy 0). Dodatkowo w argumencie alternative wskazujemy jaką hipotezę alternatywną bierzemy pod uwagę.\nDla weryfikacji równości średniej w dwóch próbach należy dodać argument y z wartościami w drugiej próbie. W tym przypadku mamy także możliwość określenia czy próby są zależne (argument paired) lub czy wariancja w obu próbach jest taka sama (var.equal). Jeżeli wariancje są różne to program R przeprowadzi test t Welcha i liczba stopni swobody nie będzie liczbą całkowitą.\n\n\n2.11.2 ANOVA\nW przypadku większej liczby grup stosuje się jednoczynnikową analizę wariancji (ANOVA). Ta analiza wymaga spełnienia założenia o normalności rozkładu i równości wariancji w badanych grupach. Układ hipotez jest następujący:\n\n\\(H_0: m_1 = m_2 = m_3 = ... = m_k\\)\n\\(H_1: \\exists_{i,j\\in\\{1,..,k\\}} \\; m_i \\neq m_j\\)\n\nZa pomocą funkcji aov() można w R przeprowadzić jednoczynnikową analizę wariancji. Jako argument funkcji należy podać formułę przedstawiającą zależność zmiennej badanej do zmiennej grupującej wykorzystując w tym celu symbol tyldy (~) w następującym kontekście: zmienna_analizowana ~ zmienna_grupująca. Przy takim zapisie należy także w argumencie data podać nazwę zbioru danych.\nW porównaniu do wcześniej opisanych funkcji, aov() nie zwraca w bezpośrednim wyniku wartości p. Aby uzyskać tę wartość należy wynik działania tej funkcji przypisać do obiektu, a następnie na nim wywołać funkcję summary().\nW przypadku odrzucenia hipotezy zerowej można przeprowadzić test Tukeya w celu identyfikacji różniących się par wykorzystując funkcję TukeyHSD() i jako argument podając obiekt zawierający wynik ANOVA.\nW sytuacji, w której założenia użycia testu parametrycznego nie są spełnione, należy skorzystać z testów nieparametrycznych. W przypadku testowania miar tendencji centralnej różnica pomiędzy testami parametrycznymi a nieparametrycznymi polega na zastąpieniu wartości średniej medianą. Z punktu widzenia obliczeń w miejsce oryginalnych wartości cechy wprowadza się rangi czyli następuje osłabienie skali pomiarowej - z ilorazowej na porządkową.\n\n\n2.11.3 Test Wilcoxona\nTest Wilcoxona jest nieparametryczną wersją testu t. Hipotezy w tym teście dotyczą równości rozkładów:\n\n\\(H_0: F_1=F_2\\)\n\\(H_1: F_1 \\neq F_2\\)\n\nWartość statystyki testowej będzie zależna od typu testu, natomiast w R funkcja, której należy użyć to wilcox.test(). Argumenty tej funkcji są takie same jak w przypadku testu t.\n\n\n2.11.4 Test Kruskala-Wallisa\nZ kolei test Kruskala-Wallisa jest nieparametrycznym odpowiednikiem ANOVA. Hipotezy są następujące:\n\n\\(H_0: F_1=F_2=F_3=...=F_k\\)\n\\(H_1: \\exists_{i,j\\in\\{1,..,k\\}} \\; F_i \\neq F_j\\)\n\nW programie R korzysta się z funkcji kruskal.test(), która przyjmuje takie same argumenty jak funkcja do metody ANOVA aov(). Główną różnicą jest sposób podawania wyniku testu, ponieważ w tym przypadku od razu otrzymujemy wartość p. W przypadku odrzucenia hipotezy zerowej należy sprawdzić, które grupy różnią się między sobą. Można to zrobić za pomocą funkcji pairwise.wilcox.test().\nPrzykład\nSprawdzimy czy średnie doświadczenie w grupach płci jest takie same.\n\\(H_0\\): średnie doświadczenie w grupach płci jest takie samo\n\\(H_1\\): średnie doświadczenie w grupach płci nie jest takie samo\nW związku z tym, że badana cecha nie ma rozkładu normalnego zostanie przeprowadzony test Wilcoxona. Mamy tutaj do czynienia z testem dla prób niezależnych - badana jest jedna cecha (doświadczenie) w ramach rozłącznych grup płci.\n\nwilcox.test(pracownicy$doswiadczenie ~ pracownicy$plec)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  pracownicy$doswiadczenie by pracownicy$plec\nW = 36295, p-value = 1.372e-08\nalternative hypothesis: true location shift is not equal to 0\n\n\nPrzyjmując poziom istotności \\(\\alpha = 0,05\\) odrzucamy \\(H_0\\) - średnie doświadczenie nie jest takie samo.\nPrzykład\nCzy początkowe i bieżące wynagrodzenie różni się od siebie w sposób istotny?\n\\(H_0\\): średnie początkowe i bieżące wynagrodzenie jest takie samo\n\\(H_1\\): średnie początkowe i bieżące wynagrodzenie nie jest takie samo\nW pierwszej kolejności weryfikujemy normalność rozkładu analizowanych cech.\n\nshapiro.test(pracownicy$pwynagrodzenie)\n\n\n    Shapiro-Wilk normality test\n\ndata:  pracownicy$pwynagrodzenie\nW = 0.71535, p-value &lt; 2.2e-16\n\nshapiro.test(pracownicy$bwynagrodzenie)\n\n\n    Shapiro-Wilk normality test\n\ndata:  pracownicy$bwynagrodzenie\nW = 0.77061, p-value &lt; 2.2e-16\n\n\nWynagrodzenie w tym zbiorze danych zdecydowanie nie przypomina rozkładu normalnego. W tym przypadku analizujemy próby zależne - badamy dwie różne cechy dla tych samych jednostek (obserwacji).\n\nwilcox.test(x = pracownicy$pwynagrodzenie, \n            y = pracownicy$bwynagrodzenie,\n            paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  pracownicy$pwynagrodzenie and pracownicy$bwynagrodzenie\nV = 0, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\nNa podstawie podanej wartości p odrzucamy \\(H_0\\) - średnie początkowe i bieżące wynagrodzenie różni się od siebie istotnie statystycznie.\nPrzykład\nAnalogicznie można także sprawdzić czy np. doświadczenie różni się w ramach więcej niż dwóch grup - w takim przypadku rozpatrujemy głównie próby niezależne.\n\\(H_0\\): średnie doświadczenie w grupach kategorii pracownika jest takie same\n\\(H_1\\): średnie doświadczenie w grupach kategorii pracownika nie jest takie same - co najmniej jedna para jest różna\n\nkruskal.test(pracownicy$doswiadczenie ~ pracownicy$kat_pracownika)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  pracownicy$doswiadczenie by pracownicy$kat_pracownika\nKruskal-Wallis chi-squared = 57.466, df = 2, p-value = 3.322e-13\n\n\nPrzyjmując poziom istotności \\(\\alpha = 0,05\\) odrzucamy hipotezę zerową - co najmniej jedna para kategorii pracownika różni się pod względem średniego wynagrodzenia.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "02-testy.html#footnotes",
    "href": "02-testy.html#footnotes",
    "title": "2  Testowanie hipotez",
    "section": "",
    "text": "W przypadku liczniejszych prób można wykorzystać test Kołmogorowa-Smirnova.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Testowanie hipotez</span>"
    ]
  },
  {
    "objectID": "03-klasyfikacja.html",
    "href": "03-klasyfikacja.html",
    "title": "3  Klasyfikacja",
    "section": "",
    "text": "3.1 Wprowadzenie\nCelem analizy klasyfikacji jest zbudowanie modelu predykcyjnego, który w rezultacie zwróci prawdopodobieństwo przynależności danej obserwacji do jeden z dwóch klas. Przykładowo na podstawie danych o klientach banku można stworzyć model oceny zdolności kredytowej dla nowych klientów. W tym rozdziale posłużymy się German Credit Data do budowy takiego predyktora. Wybrane kolumny z tego zbioru znajdują się w tym pliku.\nW pierwszym kroku wczytujemy dane i dokonujemy niezbędnych przekształceń tego zbioru. Z wykorzystaniem funkcji clean_names() z pakietu janitor zamieniamy nazwy kolumn w przyjazne przetwarzaniu przez komputer (brak spacji, polskich znaków, itp.). Następnie zmienne tekstowe zamieniamy na zmienne jakościowe - faktory oraz tworzymy nową kolumnę zawierającą wysokość raty kredytu.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\ncredit &lt;- read_xlsx(\"data/german_credit_data.xlsx\") %&gt;% \n  clean_names() %&gt;% \n  mutate_if(is.character, as.factor) %&gt;% \n  mutate(job=as.factor(job),\n         installment=credit_amount/duration)\n\nsummary(credit)\n\n      age            sex      job     housing      saving_accounts\n Min.   :19.00   female:310   0: 22   free:108   little    :603   \n 1st Qu.:27.00   male  :690   1:200   own :713   moderate  :103   \n Median :33.00                2:630   rent:179   quite rich: 63   \n Mean   :35.55                3:148              rich      : 48   \n 3rd Qu.:42.00                                   NA's      :183   \n Max.   :75.00                                                    \n                                                                  \n checking_account credit_amount      duration                   purpose   \n little  :274     Min.   :  250   Min.   : 4.0   car                :337  \n moderate:269     1st Qu.: 1366   1st Qu.:12.0   radio/TV           :280  \n rich    : 63     Median : 2320   Median :18.0   furniture/equipment:181  \n NA's    :394     Mean   : 3271   Mean   :20.9   business           : 97  \n                  3rd Qu.: 3972   3rd Qu.:24.0   education          : 59  \n                  Max.   :18424   Max.   :72.0   repairs            : 22  \n                                                 (Other)            : 24  \n   risk      installment     \n bad :300   Min.   :  24.06  \n good:700   1st Qu.:  89.60  \n            Median : 130.33  \n            Mean   : 167.69  \n            3rd Qu.: 206.18  \n            Max.   :2482.67\nZamiana cech tekstowych na faktory pozwala w podsumowaniu wygenerowanym przez funkcję summary() obserwować od razu częstości poszczególnych wariantów. Możemy zaobserwować występowanie braków danych w zmiennych saving accounts i checking_account. Generalnie jest to zbyt duży problem, bo tylko niektóre algorytmy klasyfikacji nie obsługują braków danych w zmiennych objaśniających. Ważne jest, żeby braki danych nie występowały dla cechy decyzyjnej.\nObserwacje z brakami danych można usunąć, ale często spowodowałoby to znaczne zmniejszenie próby badawczej, zatem stosuje się metody mające na celu uzupełnienie braków danych. W najprostszym przypadku braki można zastąpić średnią, medianą lub dominantą. Do bardziej zaawansowanych sposobów należy metoda najbliższych sąsiadów (VIM) albo imputacja wielokrotna (mice).\nW omawianym przypadku klientów wiarygodnych jest 700, a tych, którzy nie spłacili zobowiązania 300. Mamy zatem do czynienia z niezbalansowaną próbą. W idealnym przypadku klasyfikacji, przypadków z każdej grupy powinno być tyle samo. W przeciwnym przypadku model będzie działał lepiej dla klasy większościowej. Najprostszą metodą balansowania danych jest upsampling czyli dolosowywanie obserwacji z klasy mniejszościowej, tak aby wyrównać liczebności. Przeciwieństwem tego podejścia jest downsampling. Alternatywnie można zastosować metodę SMOTE, która generuje sztuczne obserwacje dla klasy mniejszościowej (pakiety DMwR, imbalance).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Klasyfikacja</span>"
    ]
  },
  {
    "objectID": "03-klasyfikacja.html#drzewa-decyzyjne",
    "href": "03-klasyfikacja.html#drzewa-decyzyjne",
    "title": "3  Klasyfikacja",
    "section": "3.2 Drzewa decyzyjne",
    "text": "3.2 Drzewa decyzyjne\nNajpopularniejszą metodą klasyfikacji są drzewa decyzyjne, które charakteryzują się z reguły dobrą efektywnością i pozwalają na łatwą interpretację zastosowanych reguł klasyfikacji. Wykorzystamy pakiet rpart do stworzenia drzewa oraz pakiet rpart.plot do wizualizacji.\nProces tworzenia drzewa jest bardzo prosty, a argumenty w funkcji rpart() są takie same jak w regresji liniowej.\n\nlibrary(rpart)\n\nWarning: package 'rpart' was built under R version 4.3.3\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\nm1 &lt;- rpart(formula = risk ~ ., data = credit)\n\nrpart.plot(m1)\n\n\n\n\n\n\n\n\nW rezultacie uzyskujemy drzewo decyzyjne z optymalnie dobranymi zmiennymi objaśniającymi. W każdym węźle drzewa podane są następujące wartości: - prognozowana klasa, prawdopodobieństwa zaklasyfikowania do klasy pozytywnej, odsetek obserwacji w węźle. Domyślnym progiem klasyfikacji jest wartość 0,5. Jeżeli prawdopodobieństwo jest poniżej tej wartości to nastąpi przypisanie do grupy klientów niespłacających pożyczki, a jeśli powyżej to do tej drugiej grupy.\nOceny jakości klasyfikatora dokonuje się na podstawie macierzy pomyłek oraz miar wyznaczonych na jej podstawie. Najpopularniejsze z nich to:\n\ndokładność (accuracy): % poprawnie zaklasyfikowanych\nprecyzja (precison): % poprawnie rozpoznanych przypadków pozytywnych TP/(TP+FP)\nczułość (sensitivity/recall): % prawdziwie pozytywnych TP/(TP+FN)\nswoistość (specificity): % prawdziwie negatywnych TN/(TN+FP)\nF1: średnia harmoniczna z czułości i precyzji 2TP/(2TP+FP+FN)\n\nIm wyższe wartości tych miar tym lepszy klasyfikator. Do wyznaczenia tych miar w R służy funkcja z pakietu caret. W tym celu trzeba wyznaczyć wartości prognozowanej klasy na podstawie modelu.\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\npred_risk_m1 &lt;- predict(object = m1, newdata = credit, type = \"class\")\n\nArgument type określa typ predykcji: \"class\" oznacza prognozowaną klasę, a \"prob\" prawdopodobieństwo. Na tej podstawie oraz wartości rzeczywistych tworzymy macierz pomyłek:\n\nconfusionMatrix(data = pred_risk_m1, reference = credit$risk, \n                positive = \"good\", mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad  117   60\n      good 183  640\n                                          \n               Accuracy : 0.757           \n                 95% CI : (0.7292, 0.7833)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 3.553e-05       \n                                          \n                  Kappa : 0.3447          \n                                          \n Mcnemar's Test P-Value : 5.024e-15       \n                                          \n            Sensitivity : 0.9143          \n            Specificity : 0.3900          \n         Pos Pred Value : 0.7776          \n         Neg Pred Value : 0.6610          \n              Precision : 0.7776          \n                 Recall : 0.9143          \n                     F1 : 0.8404          \n             Prevalence : 0.7000          \n         Detection Rate : 0.6400          \n   Detection Prevalence : 0.8230          \n      Balanced Accuracy : 0.6521          \n                                          \n       'Positive' Class : good            \n                                          \n\n\nW naszym przykładzie spośród 1000 klientów, model jako wiarygodnych kredytobiorców zaklasyfikował 640, a 117 prawidłowo jako osoby, które nie spłaciły zobowiązania. W 60 przypadkach model uznał brak zdolności kredytowej u klienta, podczas gdy w rzeczywistości pożyczka została spłacona. Dla 183 klientów podjętoby odwrotną decyzję - model przyznałby kredyt, a w rzeczywistości osoby te nie spłaciły pożyczki. Dokładność w tym przypadku wynosi 75,7%, a precyzja 77,8%. Czułość tego predyktora jest wysoka (91,4%), ale swoistość już nie (39%), na co może mieć wpływ niezbalansowanie danych.\nPrzedstawiony powyżej przykład miał charakter analizy ekspolarycyjnej - opartej na całym zbiorze danych. W praktyce stosuje się podejście polegające na podziale zbioru danych na zbiór treningowy oraz walidacyjny. Na danych ze zbioru treningowego buduje się model, który następnie testowany jest na danych, których nigdy wcześniej “nie widział” - na zbiorze walidacyjnym. Miary klasyfikacji obliczone na podstawie zbioru walidacyjnego dostarczają realnej oceny jakości klasyfikatora.\nDo podziału zbioru służy funkcja createDataPartition() z pakietu caret. Zbiory treningowy i walidacyjny tworzone są w taki sposób, aby zachować proporcje w zmiennej decyzyjnej. Domyślnie funkcja dzieli zbiór danych w układzie 50/50, natomiast w tym przykładzie 80% obserwacji umieścimy w zbiorze treningowym.\n\nset.seed(123)\nsplit &lt;- createDataPartition(y = credit$risk, p = 0.8)\n\ntrain_credit &lt;- credit[split$Resample1,]\nvalid_credit &lt;- credit[-split$Resample1,]\n\nsummary(train_credit)\n\n      age            sex      job     housing      saving_accounts\n Min.   :19.00   female:252   0: 17   free: 80   little    :486   \n 1st Qu.:27.00   male  :548   1:159   own :576   moderate  : 85   \n Median :33.00                2:504   rent:144   quite rich: 49   \n Mean   :35.43                3:120              rich      : 34   \n 3rd Qu.:42.00                                   NA's      :146   \n Max.   :75.00                                                    \n                                                                  \n checking_account credit_amount      duration                    purpose   \n little  :228     Min.   :  276   Min.   : 4.00   car                :284  \n moderate:205     1st Qu.: 1374   1st Qu.:12.00   radio/TV           :219  \n rich    : 46     Median : 2326   Median :18.00   furniture/equipment:139  \n NA's    :321     Mean   : 3300   Mean   :21.02   business           : 76  \n                  3rd Qu.: 3965   3rd Qu.:24.00   education          : 46  \n                  Max.   :18424   Max.   :72.00   repairs            : 17  \n                                                  (Other)            : 19  \n   risk      installment     \n bad :240   Min.   :  24.06  \n good:560   1st Qu.:  87.18  \n            Median : 131.74  \n            Mean   : 168.95  \n            3rd Qu.: 206.06  \n            Max.   :2482.67  \n                             \n\nsummary(valid_credit)\n\n      age            sex      job     housing      saving_accounts\n Min.   :20.00   female: 58   0:  5   free: 28   little    :117   \n 1st Qu.:27.00   male  :142   1: 41   own :137   moderate  : 18   \n Median :34.00                2:126   rent: 35   quite rich: 14   \n Mean   :36.02                3: 28              rich      : 14   \n 3rd Qu.:42.00                                   NA's      : 37   \n Max.   :74.00                                                    \n                                                                  \n checking_account credit_amount      duration                    purpose  \n little  :46      Min.   :  250   Min.   : 4.00   radio/TV           :61  \n moderate:64      1st Qu.: 1308   1st Qu.:12.00   car                :53  \n rich    :17      Median : 2261   Median :18.00   furniture/equipment:42  \n NA's    :73      Mean   : 3157   Mean   :20.43   business           :21  \n                  3rd Qu.: 4003   3rd Qu.:24.00   education          :13  \n                  Max.   :15672   Max.   :48.00   repairs            : 5  \n                                                  (Other)            : 5  \n   risk      installment    \n bad : 60   Min.   : 38.12  \n good:140   1st Qu.: 93.12  \n            Median :127.23  \n            Mean   :162.62  \n            3rd Qu.:206.21  \n            Max.   :730.80  \n                            \n\n\nWykorzystując tak przygotowane dane możemy jeszcze raz wykorzystać drzewa decyzyjne do stworzenia klasyfikatora, ale tym razem wyłącznie na zbiorze treningowym.\n\nm2 &lt;- rpart(risk ~ ., train_credit)\n\nrpart.plot(m2)\n\n\n\n\n\n\n\n\nUtworzone drzewo będzie różnić się od tego, które powstało na podstawie całego zbioru danych. Następnie obliczamy prognozowane klasy na obu zbiorach i wyznaczamy macierze pomyłek.\n\npred_risk_m2_train &lt;- predict(object = m2, newdata = train_credit, type = \"class\")\npred_risk_m2_valid &lt;- predict(object = m2, newdata = valid_credit, type = \"class\")\n\n# train\nconfusionMatrix(data = pred_risk_m2_train, reference = train_credit$risk, \n                positive = \"good\", mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   94   38\n      good 146  522\n                                          \n               Accuracy : 0.77            \n                 95% CI : (0.7392, 0.7987)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 5.783e-06       \n                                          \n                  Kappa : 0.3716          \n                                          \n Mcnemar's Test P-Value : 3.067e-15       \n                                          \n            Sensitivity : 0.9321          \n            Specificity : 0.3917          \n         Pos Pred Value : 0.7814          \n         Neg Pred Value : 0.7121          \n              Precision : 0.7814          \n                 Recall : 0.9321          \n                     F1 : 0.8502          \n             Prevalence : 0.7000          \n         Detection Rate : 0.6525          \n   Detection Prevalence : 0.8350          \n      Balanced Accuracy : 0.6619          \n                                          \n       'Positive' Class : good            \n                                          \n\n# valid\nconfusionMatrix(data = pred_risk_m2_valid, reference = valid_credit$risk, \n                positive = \"good\", mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   18   11\n      good  42  129\n                                          \n               Accuracy : 0.735           \n                 95% CI : (0.6681, 0.7948)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 0.1579          \n                                          \n                  Kappa : 0.2598          \n                                          \n Mcnemar's Test P-Value : 3.775e-05       \n                                          \n            Sensitivity : 0.9214          \n            Specificity : 0.3000          \n         Pos Pred Value : 0.7544          \n         Neg Pred Value : 0.6207          \n              Precision : 0.7544          \n                 Recall : 0.9214          \n                     F1 : 0.8296          \n             Prevalence : 0.7000          \n         Detection Rate : 0.6450          \n   Detection Prevalence : 0.8550          \n      Balanced Accuracy : 0.6107          \n                                          \n       'Positive' Class : good            \n                                          \n\n\nGeneralnie wyniki w obu przypadkach powinny być do siebie zbliżone, przy czym na zbiorze walidacyjnym miary jakości predycji mogą być trochę gorsze. Bardzo wysoka wartość dokładność na zbiorze treningowym, a niska na zbiorze walidacyjnym jest symptomem przeuczenia modelu - algorytm nauczył się odpowiedzi “na pamięć”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Klasyfikacja</span>"
    ]
  },
  {
    "objectID": "03-klasyfikacja.html#gradient-boosting-machine",
    "href": "03-klasyfikacja.html#gradient-boosting-machine",
    "title": "3  Klasyfikacja",
    "section": "3.3 Gradient Boosting Machine",
    "text": "3.3 Gradient Boosting Machine\nSpróbujemy polepszyć jakość klasyfikacji z wykorzystaniem metody gradient boostingu. W tym celu wykorzystamy pakiet h2o, który dostarcza kompleksowych rozwiązań z zakresu machine learning. W pierwszej kolejności trzeba zmienić format danych na ten obsługiwany przez pakiet.\n\nlibrary(h2o)\n\n\n----------------------------------------------------------------------\n\nYour next step is to start H2O:\n    &gt; h2o.init()\n\nFor H2O package documentation, ask for help:\n    &gt; ??h2o\n\nAfter starting H2O, you can use the Web UI at http://localhost:54321\nFor more information visit https://docs.h2o.ai\n\n----------------------------------------------------------------------\n\n\n\nAttaching package: 'h2o'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    day, hour, month, week, year\n\n\nThe following objects are masked from 'package:stats':\n\n    cor, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n    colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log,\n    log10, log1p, log2, round, signif, trunc\n\nh2o.init()\n\n\nH2O is not running yet, starting it now...\n\nNote:  In case of errors look at the following log files:\n    C:\\Users\\lukas\\AppData\\Local\\Temp\\RtmpekkK39\\file2a8812393c3/h2o_lukas_started_from_r.out\n    C:\\Users\\lukas\\AppData\\Local\\Temp\\RtmpekkK39\\file2a8816c87bf/h2o_lukas_started_from_r.err\n\n\nStarting H2O JVM and connecting: . Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         5 seconds 833 milliseconds \n    H2O cluster timezone:       Europe/Belgrade \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.42.0.2 \n    H2O cluster version age:    10 months and 28 days \n    H2O cluster name:           H2O_started_from_R_lukas_xpm862 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   3.52 GB \n    H2O cluster total cores:    8 \n    H2O cluster allowed cores:  8 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.3.2 (2023-10-31 ucrt) \n\n\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (10 months and 28 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\nh2o.no_progress() # brak pasków postępu\n\ntrain_credit_h2o &lt;- as.h2o(train_credit)\nvalid_credit_h2o &lt;- as.h2o(valid_credit)\n\nNastępnie deklarujemy nazwy wykorzystywanych zmiennych i uruchamiamy procedurę:\n\ny_var &lt;- \"risk\"\nx_var &lt;- names(credit)[-10]\n\nm3 &lt;- h2o.gbm(x = x_var, \n              y = y_var, \n              training_frame = train_credit_h2o, \n              validation_frame = valid_credit_h2o, \n              seed = 1)\n\nm3\n\nModel Details:\n==============\n\nH2OBinomialModel: gbm\nModel ID:  GBM_model_R_1719054914797_1 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              50                       50               13762         5\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1         5    5.00000         10         26    17.38000\n\n\nH2OBinomialMetrics: gbm\n** Reported on training data. **\n\nMSE:  0.07844172\nRMSE:  0.2800745\nLogLoss:  0.2801827\nMean Per-Class Error:  0.09345238\nAUC:  0.9696838\nAUCPR:  0.9855312\nGini:  0.9393676\nR^2:  0.626468\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       bad good    Error     Rate\nbad    205   35 0.145833  =35/240\ngood    23  537 0.041071  =23/560\nTotals 228  572 0.072500  =58/800\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.569359   0.948763 236\n2                       max f2  0.478467   0.964236 262\n3                 max f0point5  0.669186   0.948864 201\n4                 max accuracy  0.569359   0.927500 236\n5                max precision  0.987622   1.000000   0\n6                   max recall  0.363744   1.000000 305\n7              max specificity  0.987622   1.000000   0\n8             max absolute_mcc  0.569359   0.825421 236\n9   max min_per_class_accuracy  0.651784   0.904167 206\n10 max mean_per_class_accuracy  0.669186   0.907738 201\n11                     max tns  0.987622 240.000000   0\n12                     max fns  0.987622 559.000000   0\n13                     max fps  0.049138 240.000000 399\n14                     max tps  0.363744 560.000000 305\n15                     max tnr  0.987622   1.000000   0\n16                     max fnr  0.987622   0.998214   0\n17                     max fpr  0.049138   1.000000 399\n18                     max tpr  0.363744   1.000000 305\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: gbm\n** Reported on validation data. **\n\nMSE:  0.1816324\nRMSE:  0.4261836\nLogLoss:  0.5493537\nMean Per-Class Error:  0.4071429\nAUC:  0.729881\nAUCPR:  0.8518075\nGini:  0.4597619\nR^2:  0.1350837\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       bad good    Error     Rate\nbad     12   48 0.800000   =48/60\ngood     2  138 0.014286   =2/140\nTotals  14  186 0.250000  =50/200\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.288120   0.846626 185\n2                       max f2  0.195621   0.928382 193\n3                 max f0point5  0.587924   0.809659 140\n4                 max accuracy  0.289167   0.750000 183\n5                max precision  0.973438   1.000000   0\n6                   max recall  0.195621   1.000000 193\n7              max specificity  0.973438   1.000000   0\n8             max absolute_mcc  0.587924   0.366055 140\n9   max min_per_class_accuracy  0.738988   0.685714 113\n10 max mean_per_class_accuracy  0.738988   0.692857 113\n11                     max tns  0.973438  60.000000   0\n12                     max fns  0.973438 139.000000   0\n13                     max fps  0.067249  60.000000 199\n14                     max tps  0.195621 140.000000 193\n15                     max tnr  0.973438   1.000000   0\n16                     max fnr  0.973438   0.992857   0\n17                     max fpr  0.067249   1.000000 199\n18                     max tpr  0.195621   1.000000 193\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n\n\nZ funkcji pakietu h2o dostajemy pokaźny wydruk podsumowania klasyfikacji wraz z macierzami pomyłek. Pakiet h2o optymalizuje jakość predycji maksymalizująć miarę F1 - średnią harmoniczną czułości i swoistości. Zatem w tym przypadku podział na klasy wg prawdopodobieństwa nie odbywa się już na podstawie wartości 0,5.\nStworzymy macierz pomyłek dla zbioru walidacyjnego w tym samym formacie, co wcześniej:\n\npred_risk_m3_valid &lt;- as.data.frame(h2o.predict(object = m3, newdata = valid_credit_h2o))\n\nconfusionMatrix(data = pred_risk_m3_valid$predict, reference = valid_credit$risk, \n                positive = \"good\", mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   12    2\n      good  48  138\n                                         \n               Accuracy : 0.75           \n                 95% CI : (0.684, 0.8084)\n    No Information Rate : 0.7            \n    P-Value [Acc &gt; NIR] : 0.06955        \n                                         \n                  Kappa : 0.2378         \n                                         \n Mcnemar's Test P-Value : 1.966e-10      \n                                         \n            Sensitivity : 0.9857         \n            Specificity : 0.2000         \n         Pos Pred Value : 0.7419         \n         Neg Pred Value : 0.8571         \n              Precision : 0.7419         \n                 Recall : 0.9857         \n                     F1 : 0.8466         \n             Prevalence : 0.7000         \n         Detection Rate : 0.6900         \n   Detection Prevalence : 0.9300         \n      Balanced Accuracy : 0.5929         \n                                         \n       'Positive' Class : good           \n                                         \n\n\nMożemy zauważyć nieznaczną poprawę dokładności predykcji w porównaniu do drzewa decyzyjnego - z 73,5% do 75%. Natomiast ten algorytm został uruchomiony z domyślnymi parametrami i być może wybór innych wartości mógłby poprawić jakość predykcji. Poszukiwania najlepszych wartości do algorytmu nosi nazwę tuningu hiperparametrów. Z racji tego, że przeszukiwana przestrzeń parametrów mogłaby być bardzo duża to z reguły przeprowadza się to w sposób losowy. Poniżej lista możliwych do zastosowania parametrów w metodzie GBM.\n\ngbm_params &lt;- list(learn_rate = seq(0.01, 0.1, 0.01),\n                   max_depth = seq(2, 10, 1),\n                   sample_rate = seq(0.5, 1.0, 0.1),\n                   col_sample_rate = seq(0.1, 1.0, 0.1),\n                   ntrees = seq(50,150,10))\n\nsearch_criteria &lt;- list(strategy = \"RandomDiscrete\", max_models = 36, seed = 1)\n\nWszystkich kombinacji parametrów jest 59400 i weryfikacja ich wszystkich zajęła by sporo czasu, zatem w sposób losowy szukamy najlepszych 36 modeli. Z tego zestawu wybieramy najlepszy według kryterium jakim jest miara AUC.\n\ngbm_grid &lt;- h2o.grid(algorithm = \"gbm\", \n                     x = x_var, \n                     y = y_var,\n                     grid_id = \"gbm_grid\",\n                     training_frame = train_credit_h2o,\n                     validation_frame = valid_credit_h2o,\n                     seed = 1,\n                     hyper_params = gbm_params,\n                     search_criteria = search_criteria)\n\ngbm_gridperf &lt;- h2o.getGrid(grid_id = \"gbm_grid\",\n                            sort_by = \"auc\",\n                            decreasing = TRUE)\n\nm4 &lt;- h2o.getModel(gbm_gridperf@model_ids[[1]])\n\nm4\n\nModel Details:\n==============\n\nH2OBinomialModel: gbm\nModel ID:  gbm_grid_model_33 \nModel Summary: \n  number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n1              80                       80                8424         2\n  max_depth mean_depth min_leaves max_leaves mean_leaves\n1         2    2.00000          3          4     3.75000\n\n\nH2OBinomialMetrics: gbm\n** Reported on training data. **\n\nMSE:  0.1442394\nRMSE:  0.3797886\nLogLoss:  0.449481\nMean Per-Class Error:  0.2833333\nAUC:  0.8456138\nAUCPR:  0.921934\nGini:  0.6912277\nR^2:  0.3131457\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       bad good    Error      Rate\nbad    119  121 0.504167  =121/240\ngood    35  525 0.062500   =35/560\nTotals 154  646 0.195000  =156/800\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.500182   0.870647 293\n2                       max f2  0.371147   0.932146 352\n3                 max f0point5  0.671700   0.864148 196\n4                 max accuracy  0.500182   0.805000 293\n5                max precision  0.956594   1.000000   0\n6                   max recall  0.292273   1.000000 379\n7              max specificity  0.956594   1.000000   0\n8             max absolute_mcc  0.643739   0.520541 214\n9   max min_per_class_accuracy  0.666048   0.770833 200\n10 max mean_per_class_accuracy  0.671700   0.775595 196\n11                     max tns  0.956594 240.000000   0\n12                     max fns  0.956594 559.000000   0\n13                     max fps  0.163448 240.000000 399\n14                     max tps  0.292273 560.000000 379\n15                     max tnr  0.956594   1.000000   0\n16                     max fnr  0.956594   0.998214   0\n17                     max fpr  0.163448   1.000000 399\n18                     max tpr  0.292273   1.000000 379\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\nH2OBinomialMetrics: gbm\n** Reported on validation data. **\n\nMSE:  0.1749705\nRMSE:  0.4182947\nLogLoss:  0.5266207\nMean Per-Class Error:  0.4154762\nAUC:  0.751369\nAUCPR:  0.8703734\nGini:  0.5027381\nR^2:  0.1668073\n\nConfusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n       bad good    Error     Rate\nbad     11   49 0.816667   =49/60\ngood     2  138 0.014286   =2/140\nTotals  13  187 0.255000  =51/200\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n                        metric threshold      value idx\n1                       max f1  0.399730   0.844037 178\n2                       max f2  0.335934   0.925926 187\n3                 max f0point5  0.624918   0.826023 127\n4                 max accuracy  0.576083   0.760000 137\n5                max precision  0.953756   1.000000   0\n6                   max recall  0.335934   1.000000 187\n7              max specificity  0.953756   1.000000   0\n8             max absolute_mcc  0.624918   0.416343 127\n9   max min_per_class_accuracy  0.696832   0.664286 105\n10 max mean_per_class_accuracy  0.624918   0.711905 127\n11                     max tns  0.953756  60.000000   0\n12                     max fns  0.953756 139.000000   0\n13                     max fps  0.247891  60.000000 191\n14                     max tps  0.335934 140.000000 187\n15                     max tnr  0.953756   1.000000   0\n16                     max fnr  0.953756   0.992857   0\n17                     max fpr  0.247891   1.000000 191\n18                     max tpr  0.335934   1.000000 187\n\nGains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`\n\n\nStandardowo utworzymy macierz pomyłek i obliczymy pozostałe miary:\n\npred_risk_m4_valid &lt;- as.data.frame(h2o.predict(object = m4, newdata = valid_credit_h2o))\n\nconfusionMatrix(data = pred_risk_m4_valid$predict, reference = valid_credit$risk, \n                positive = \"good\", mode = \"everything\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction bad good\n      bad   11    2\n      good  49  138\n                                          \n               Accuracy : 0.745           \n                 95% CI : (0.6787, 0.8039)\n    No Information Rate : 0.7             \n    P-Value [Acc &gt; NIR] : 0.09344         \n                                          \n                  Kappa : 0.2178          \n                                          \n Mcnemar's Test P-Value : 1.185e-10       \n                                          \n            Sensitivity : 0.9857          \n            Specificity : 0.1833          \n         Pos Pred Value : 0.7380          \n         Neg Pred Value : 0.8462          \n              Precision : 0.7380          \n                 Recall : 0.9857          \n                     F1 : 0.8440          \n             Prevalence : 0.7000          \n         Detection Rate : 0.6900          \n   Detection Prevalence : 0.9350          \n      Balanced Accuracy : 0.5845          \n                                          \n       'Positive' Class : good            \n                                          \n\n\nFinalny model charakteryzuje się dokładnością na poziomie 76,5%, zatem udało się poprawić jego jakość w porównaniu do drzewa decyzyjnego oraz wersji GBM z domyślnymi parametrami.\nInnym sposobem oceny jakości modelu jest symulacja wyniku biznesowego na podstawie kosztów dla każdej komórki macierzy pomyłek. W naszym przypadku przyjmiemy bardzo uproszczone miary: średnia kwota kredytu: 3500, marża na spłaconym kredycie: 20%, strata na niespłaconym kredycie: 50%.\n\n# m2\n42*-(3500*0.5)+129*(3500*0.2)\n\n[1] 16800\n\n# m3\n50*-(3500*0.5)+140*(3500*0.2)\n\n[1] 10500\n\n# m4\n43*-(3500*0.5)+136*(3500*0.2)\n\n[1] 19950\n\n\nNajwiększą korzyść uzyskamy korzystając z modelu m4.\nJakość predykcji możemy także spróbować poprawić manewrując progiem prawdopodobieństwa klasyfikacji. Narzędziem, które może w tym pomóc jest krzywa ROC, która przestawia wartości czułości i swoistości dla różnych progów.\n\nplot(h2o.performance(m4, valid = T), type = \"roc\")\n\n\n\n\n\n\n\n\nPrzekątna przedstawia klasyfikator losowy, natomiast punkty leżące powyżej to klasyfikatory lepsze od losowego. Klasyfikator idealny byłby krzywą o następującym przebiegu (0,0) -&gt; (0,1) -&gt; (1,1). Pole pod krzywą ROC jest także miarą jakości klasyfikacji - AUC, której wysokie wartości są pożądane.\nZastosowany przez nas model GBM jest pewną czarną skrzynką, zatem nie wiadomo dokładnie na podstawie jakich reguł ta klasyfikacja przebiega, ale możemy wykorzystać pewne miary tzw. Explainable AI w celu opisu modelu.\nPierwszą z nich jest ważność cech, która określa jak bardzo model wykorzystuje daną cechę do predycji.\n\nh2o.varimp(m4)\n\nVariable Importances: \n           variable relative_importance scaled_importance percentage\n1  checking_account          164.028534          1.000000   0.428749\n2          duration           53.408806          0.325607   0.139603\n3       installment           43.910915          0.267703   0.114777\n4   saving_accounts           37.021275          0.225700   0.096769\n5     credit_amount           35.085529          0.213899   0.091709\n6               age           19.486319          0.118798   0.050935\n7           purpose           15.291790          0.093226   0.039971\n8               sex           10.269917          0.062611   0.026844\n9           housing            2.307869          0.014070   0.006032\n10              job            1.764029          0.010754   0.004611\n\nh2o.varimp_plot(m4)\n\n\n\n\n\n\n\n\nNa tej podstawie możemy stwierdzić, że najważniejszą cechą mającą wpływ na klasyfikację jest checking_account. Na kolejnych miejscach jest installment oraz credit_amount. Najmniej ważna jest zmienna housing.\nInnym metodem opisu działania modelu jest partial dependency plot, przedstawiający zależność prawdopodobieństwa klasyfikacji od wartości danej cechy niezależnej.\n\nh2o.partialPlot(object = m4, data = valid_credit_h2o, cols = c(\"checking_account\",\"credit_amount\"))\n\n\n\n\n\n\n\n\nWarning in plot.window(...): \"medcol\" is not a graphical parameter\n\n\nWarning in plot.window(...): \"medlty\" is not a graphical parameter\n\n\nWarning in plot.window(...): \"staplelty\" is not a graphical parameter\n\n\nWarning in plot.window(...): \"boxlty\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"medcol\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"medlty\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"staplelty\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"boxlty\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"medcol\" is not a\ngraphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"medlty\" is not a\ngraphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"staplelty\" is not\na graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"boxlty\" is not a\ngraphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"medcol\" is not a\ngraphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"medlty\" is not a\ngraphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"staplelty\" is not\na graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"boxlty\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"medcol\" is not a graphical parameter\n\n\nWarning in box(...): \"medlty\" is not a graphical parameter\n\n\nWarning in box(...): \"staplelty\" is not a graphical parameter\n\n\nWarning in box(...): \"boxlty\" is not a graphical parameter\n\n\nWarning in title(...): \"medcol\" is not a graphical parameter\n\n\nWarning in title(...): \"medlty\" is not a graphical parameter\n\n\nWarning in title(...): \"staplelty\" is not a graphical parameter\n\n\nWarning in title(...): \"boxlty\" is not a graphical parameter\n\n\n\n\n\n\n\n\n\n[[1]]\nPartialDependence: Partial dependency plot for checking_account\n  checking_account mean_response stddev_response std_error_mean_response\n1           little      0.591274        0.152221                0.010764\n2         moderate      0.638785        0.150663                0.010654\n3             rich      0.809393        0.092115                0.006514\n\n[[2]]\nPartialDependence: Partial dependency plot for credit_amount\n   credit_amount mean_response stddev_response std_error_mean_response\n1     250.000000      0.763155        0.157128                0.011111\n2    1061.684211      0.722067        0.177397                0.012544\n3    1873.368421      0.726639        0.179054                0.012661\n4    2685.052632      0.726639        0.179054                0.012661\n5    3496.736842      0.726639        0.179054                0.012661\n6    4308.421053      0.682370        0.181120                0.012807\n7    5120.105263      0.682370        0.181120                0.012807\n8    5931.789474      0.682370        0.181120                0.012807\n9    6743.473684      0.631133        0.198735                0.014053\n10   7555.157895      0.650912        0.194416                0.013747\n11   8366.842105      0.636791        0.200403                0.014171\n12   9178.526316      0.636791        0.200403                0.014171\n13   9990.210526      0.630476        0.195998                0.013859\n14  10801.894737      0.630476        0.195998                0.013859\n15  11613.578947      0.487381        0.197673                0.013978\n16  12425.263158      0.487381        0.197673                0.013978\n17  13236.947368      0.487381        0.197673                0.013978\n18  14048.631579      0.487381        0.197673                0.013978\n19  14860.315789      0.487381        0.197673                0.013978\n20  15672.000000      0.487381        0.197673                0.013978\n\nh2o.shutdown(prompt = FALSE)\n\nW tym przypadku możemy zaobserwować, że im klient bogatszy tym większa szansa na klasyfikację do grupy wiarygodnych kredytobiorców. Z kolei wraz ze wzrostem wysokości kredytu to prawdopodobieństwo maleje.\n\n3.3.1 Zadania\n\nSprawdź jak model będzie się sprawdzał na danych, w których braki danych są traktowane jako osobna kategoria:\n\n\ncredit_wna &lt;- credit %&gt;% mutate_at(c(\"saving_accounts\", \"checking_account\"), fct_explicit_na)\n\n\nNa podstawie charakterystyki mieszkań przeprowadź klasyfikację ich umiejscowienia - San Francisco vs. New York.\n\n\nhomes &lt;- read_csv(\"http://www.wawrowski.edu.pl/data/sf_ny_homes.csv\")\n\nhomes &lt;- homes %&gt;% \n  mutate(in_sf=as.factor(in_sf))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Klasyfikacja</span>"
    ]
  },
  {
    "objectID": "04-regresja.html",
    "href": "04-regresja.html",
    "title": "4  Regresja",
    "section": "",
    "text": "4.1 Wprowadzenie\nMetody regresji pozwalają na analizowanie zależności przyczynowo-skutkowych oraz predycję nieznanych wartości. Korzystając z tej metody należy jednak pamiętać, że model regresji jest tylko przybliżeniem rzeczywistości.\nPrzykłady zastosowania regresji:\nNa początku pracy wczytujemy biblioteki tidyverse i readxl.\nlibrary(tidyverse)\nlibrary(readxl)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresja</span>"
    ]
  },
  {
    "objectID": "04-regresja.html#wprowadzenie",
    "href": "04-regresja.html#wprowadzenie",
    "title": "4  Regresja",
    "section": "",
    "text": "zależność wielkości sprzedaży od wydatków na reklamę\nzależność wynagrodzenia od lat doświadczenia",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresja</span>"
    ]
  },
  {
    "objectID": "04-regresja.html#regresja-prosta",
    "href": "04-regresja.html#regresja-prosta",
    "title": "4  Regresja",
    "section": "4.2 Regresja prosta",
    "text": "4.2 Regresja prosta\nNa podstawie danych dotyczących informacji o doświadczeniu i wynagrodzeniu pracowników zbuduj model określający ‘widełki’ dla potencjalnych pracowników o doświadczeniu równym 8, 10 i 11 lat.\nWczytujemy dane i sprawdzamy czy nie występują zera bądź braki danych z użyciem funkcji summary().\n\nsalary &lt;- read_xlsx(\"data/salary.xlsx\")\n\nsummary(salary)\n\n YearsExperience      Salary      \n Min.   : 1.100   Min.   : 37731  \n 1st Qu.: 3.200   1st Qu.: 56721  \n Median : 4.700   Median : 65237  \n Mean   : 5.313   Mean   : 76003  \n 3rd Qu.: 7.700   3rd Qu.:100545  \n Max.   :10.500   Max.   :122391  \n\n\nNastępnie stworzymy wykres.\n\nplot(salary)\n\n\n\n\n\n\n\n\nNajprostszym sposobem wizualizacji jest wykorzystanie funkcji plot(), niemniej taki wykres nie jest najpiękniejszy i trudno się go formatuje. Dużo lepiej skorzystać z pakietu ggplot2.\n\nggplot(salary, aes(x=YearsExperience, y=Salary)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"Doświadczenie\") + \n  ylab(\"Pensja\") +\n  xlim(0,12) +\n  ylim(35000, 126000) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nW argumentach funkcji ggplot() podajemy co wizualizujemy, natomiast sposób prezentacji ustalany jest przez funkcje geom. Funkcje xlab() i ylab() określają etykiety osi, a xlim() i ylim() wartości graniczne. Funkcje rozpoczynające się od theme_ określają wygląd wykresu.\nModelowanie rozpoczynamy od określenia zmiennej zależnej i niezależnej.\n\nzmienna zależna/objaśniana: pensja - \\(y\\)\nzmienna niezależna/objaśniająca: doświadczenie - \\(x\\)\n\nSzukamy teraz wzoru na prostą opisującą badane cechy.\nOgólna postać regresji prostej jest następująca:\n\\[\\hat{y}_{i}=b_{1}x_{i}+b_{0}\\]\ngdzie \\(\\hat{y}\\) oznacza wartość teoretyczną, leżącą na wyznaczonej prostej, \\(x_i\\) wartość zmiennej niezależnej, a \\(b_1\\) i \\(b_0\\) to współczynniki modelu.\nWobec tego wartości empiryczne \\(y\\) będą opisane formułą:\n\\[y_{i}=b_{1}x_{i}+b_{0}+u_{i}\\]\nw której \\(u_i\\) oznacza składnik resztowy wyliczany jako \\(u_{i}=y_{i}-\\hat{y}_{i}\\).\nWspółczynnik kierunkowy \\(b_1\\) informuje o ile przeciętne zmieni się wartość zmiennej objaśnianej \\(y\\), gdy wartość zmiennej objaśniającej \\(x\\) wzrośnie o jednostkę.\nWyraz wolny \\(b_0\\) to wartość zmiennej objaśnianej \\(y\\), w sytuacji w której wartość zmiennej objaśniającej \\(x\\) będzie równa 0. Często interpretacja tego parametru nie ma sensu.\nNastępnie w R korzystamy z funkcji lm, w której należy określić zależność funkcyjną w formie: zmienna_zalezna ~ zmienna niezalezna.\n\nsalary_model &lt;- lm(formula = Salary ~ YearsExperience, data = salary)\nsummary(salary_model)\n\n\nCall:\nlm(formula = Salary ~ YearsExperience, data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7958.0 -4088.5  -459.9  3372.6 11448.0 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      25792.2     2273.1   11.35 5.51e-12 ***\nYearsExperience   9450.0      378.8   24.95  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5788 on 28 degrees of freedom\nMultiple R-squared:  0.957, Adjusted R-squared:  0.9554 \nF-statistic: 622.5 on 1 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nNa podstawie otrzymanego wyniku dokonujemy interpretacji parametrów.\n\nb1 = 9450 - wzrost doświadczenia o rok powoduje przeciętny wzrost pensji o 9450 $\nb0 = 25792,2 - pracownik o doświadczeniu 0 lat uzyska pensję w wysokości 25792,2 $\n\nTrzy gwiazki przy współczynniku \\(b_1\\) oznaczają, że doświadczenie ma istotny wpływ na wartości pensji. Wyraz wolny także jest istotny, natomiast ogólnie nie jest wymagana jesgo istotność.\nOprócz interpretacji współczynników ważne są jeszcze inne miary jakości modelu.\nOdchylenie standardowe składnika resztowego jest pierwiastkiem z sumy kwadratów reszt podzielonej przez liczbę obserwacji pomniejszoną o 2:\n\\[S_{u}=\\sqrt{\\frac{\\sum\\limits_{i=1}^{n}{(y_{i}-\\hat{y}_{i})^2}}{n-2}}\\]\nBłąd resztowy (odchylenie standardowe składnika resztowego) \\(S_u\\) wynosi 5788\\(, co oznacza, że wartości obliczone na podstawie modelu różnią się od rzeczywistości średnio o +/- 5788\\)\nWspółczynnik determinacji określa, jaki procent wariancji zmiennej objaśnianej został wyjaśniony przez funkcję regresji. \\(R^2\\) przyjmuje wartości z przedziału \\(&lt;0;1&gt;\\) ( \\(&lt;0\\%;100\\%&gt;\\) ), przy czym model regresji tym lepiej opisuje zachowanie się badanej zmiennej objaśnianej, im \\(R^2\\) jest bliższy jedności (bliższy 100%)\n\\[R^2=1-\\frac{\\sum\\limits_{i=1}^{n}{(y_{i}-\\hat{y}_{i})^2}}{\\sum\\limits_{i=1}^{n}{(y_{i}-\\bar{y}_{i})^2}}\\]\nWspółczynnik \\(R^2\\) (multiple R-squared) wynosi 0,957 czyli doświadczenie wyjaśnia 95,7% zmienności pensji\nNa podstawie tego modelu dokonamy wyznaczenia wartości teoretycznych dla kilku wybranych wartości doświadczenia. Nowe wartości muszą być w postaci zbioru danych, zatem tworzymy nową ramkę danych.\n\nnowiPracownicy &lt;- data.frame(YearsExperience=c(8,10,11))\n\npredict(salary_model, nowiPracownicy)\n\n       1        2        3 \n101391.9 120291.8 129741.8 \n\n\nTym sposobem uzyskujemy następujące widełki:\n\npracownik o 8 latach doświadczenia - proponowana pensja 101391,9 $ +/- 5788 $\npracownik o 10 latach doświadczenia - proponowana pensja 120291,8 $ +/- 5788 $\npracownik o 11 latach doświadczenia - proponowana pensja 129741,8 $ +/- 5788 $\n\nW powyższym przykładzie prognozowane wartości pojawiły się w konsoli, natomiast w sytuacji, w której chcielibyśmy wykonać predykcję dla bardzo wielu nowych wartości to warto te prognozowane wartości umieścić od razu w zbiorze danych. Można to zrobić w następujący sposób:\n\nnowiPracownicy &lt;- nowiPracownicy %&gt;% \n  mutate(salary_pred=predict(object = salary_model, newdata = .))\n\nnowiPracownicy\n\n  YearsExperience salary_pred\n1               8    101391.9\n2              10    120291.8\n3              11    129741.8\n\n\nW powyższym kodzie symbol . oznacza analizowany zbiór danych, zatem nie ma potrzeby powtarzania jego nazwy.\n\n4.2.1 Zadanie\nDla danych dotyczących sklepu nr 77 opracuj model zależności sprzedaży od liczby klientów. Ile wynosi teoretyczna sprzedaż w dniach, w których liczba klientów będzie wynosiła 560, 740, 811 oraz 999 osób?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresja</span>"
    ]
  },
  {
    "objectID": "04-regresja.html#regresja-wieloraka",
    "href": "04-regresja.html#regresja-wieloraka",
    "title": "4  Regresja",
    "section": "4.3 Regresja wieloraka",
    "text": "4.3 Regresja wieloraka\nOgólna postać regresji wielorakiej jest następująca:\n\\[\\hat{y}_{i}=b_{1}x_{1i}+b_{2}x_{2i}+...+b_{k}x_{ki}+b_{0}\\]\nW tym przypadku nie wyznaczamy prostej tylko \\(k\\)-wymiarową przestrzeń.\nNa podstawie danych dotyczących zatrudnienia opracuj model, w którym zmienną zależną jest bieżące wynagrodzenie. Za pomocą regresji określimy jakie cechy mają istotny wpływ na to wynagrodzenie.\nOpis zbioru:\n\nid - kod pracownika\nplec - płeć pracownika (0 - mężczyzna, 1 - kobieta)\ndata_urodz - data urodzenia\nedukacja - wykształcenie (w latach nauki)\nkat_pracownika - grupa pracownicza (1 - specjalista, 2 - menedżer, 3 - konsultant)\nbwynagrodzenie - bieżące wynagrodzenie\npwynagrodzenie - początkowe wynagrodzenie\nstaz - staż pracy (w miesiącach)\ndoswiadczenie - poprzednie zatrudnienie (w miesiącach)\nzwiazki - przynależność do związków zawodowych (0 - nie, 1 - tak)\nwiek - wiek (w latach)\n\nRozpoczynamy od wczytania danych,\n\npracownicy &lt;- read_xlsx(\"data/pracownicy.xlsx\")\n\npracownicy2 &lt;- pracownicy %&gt;%\n  filter(!is.na(wiek)) %&gt;%\n  select(-id, -data_urodz) %&gt;%\n  mutate(plec=as.factor(plec),\n         kat_pracownika=as.factor(kat_pracownika),\n         zwiazki=as.factor(zwiazki))\n\nsummary(pracownicy2)\n\n plec       edukacja     kat_pracownika bwynagrodzenie   pwynagrodzenie \n 0:257   Min.   : 8.00   1:362          Min.   : 15750   Min.   : 9000  \n 1:216   1st Qu.:12.00   2: 27          1st Qu.: 24000   1st Qu.:12450  \n         Median :12.00   3: 84          Median : 28800   Median :15000  \n         Mean   :13.49                  Mean   : 34418   Mean   :17009  \n         3rd Qu.:15.00                  3rd Qu.: 37050   3rd Qu.:17490  \n         Max.   :21.00                  Max.   :135000   Max.   :79980  \n      staz       doswiadczenie    zwiazki      wiek      \n Min.   :63.00   Min.   :  0.00   0:369   Min.   :24.00  \n 1st Qu.:72.00   1st Qu.: 19.00   1:104   1st Qu.:30.00  \n Median :81.00   Median : 55.00           Median :33.00  \n Mean   :81.14   Mean   : 95.95           Mean   :38.67  \n 3rd Qu.:90.00   3rd Qu.:139.00           3rd Qu.:47.00  \n Max.   :98.00   Max.   :476.00           Max.   :66.00  \n\n\nW zmiennej wiek występował brak danych, który został usunięty. Usunięto także kolumny, które nie przydadzą się w modelowaniu. Ponadto dokonujemy przekształcenia typu cech, które są jakościowe (płeć, kat_pracownika, zwiazki) z typu liczbowego na czynnik/faktor. Taka modyfikacja powoduje, że ta cecha będzie przez model traktowana jako zmienna dychotomiczna (zerojedynkowa). Proces trasformacji takiej cechy jest pokazany poniżej.\nOryginalny zbiór\n\n\n\nid\nstanowisko\n\n\n\n\n1\nspecjalista\n\n\n2\nmenedżer\n\n\n3\nspecjalista\n\n\n4\nkonsultant\n\n\n5\nkonsultant\n\n\n\nZmienna zerojedynkowa\n\n\n\nid\nmenedżer\nkonsultant\n\n\n\n\n1\n0\n0\n\n\n2\n1\n0\n\n\n3\n0\n0\n\n\n4\n0\n1\n\n\n5\n0\n1\n\n\n\nW modelu zmienna zależna to bwynagrodzenie, natomiast jako zmienne niezależne bierzemy pod uwagę wszystkie pozostałe cechy. W celu uniknięcia notacji naukowej w uzyskiwanych wynikach dodajemy opcję options(scipen = 5).\n\noptions(scipen = 5)\n\nmodel &lt;- lm(bwynagrodzenie ~ ., pracownicy2)\nsummary(model)\n\n\nCall:\nlm(formula = bwynagrodzenie ~ ., data = pracownicy2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-23185  -3041   -705   2591  46295 \n\nCoefficients:\n                   Estimate  Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -4764.87418  3590.49652  -1.327  0.18514    \nplec1           -1702.43743   796.51779  -2.137  0.03309 *  \nedukacja          482.43603   160.83977   2.999  0.00285 ** \nkat_pracownika2  6643.17910  1638.06138   4.056 5.87e-05 ***\nkat_pracownika3 11169.64519  1372.73990   8.137 3.77e-15 ***\npwynagrodzenie      1.34021     0.07317  18.315  &lt; 2e-16 ***\nstaz              154.50876    31.65933   4.880 1.46e-06 ***\ndoswiadczenie     -15.77375     5.78369  -2.727  0.00663 ** \nzwiazki1        -1011.55276   787.80884  -1.284  0.19978    \nwiek              -64.78787    47.88015  -1.353  0.17668    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6809 on 463 degrees of freedom\nMultiple R-squared:  0.8444,    Adjusted R-squared:  0.8414 \nF-statistic: 279.1 on 9 and 463 DF,  p-value: &lt; 2.2e-16\n\n\nTak zbudowany model wyjaśnia 84,4% zmienności bieżącego wynagrodzenia, ale nie wszystkie zmienne są w tym modelu istotne.\nParametry regresji mają następujące interpretacje:\n\nplec1 - kobiety zarabiają przeciętnie o 1702,44 zł mniej niż mężczyźni,\nedukacja - wzrost liczby lat nauki o rok powoduje średni wzrost bieżącego wynagrodzenia o 482,44 zł\nkat_pracownika2 - pracownicy o kodzie 2 (menedżer) zarabiają średnio o 6643,18 zł więcej niż pracownik o kodzie 1 (specjalista)\nkat_pracownika2 - pracownicy o kodzie 3 (konsultant) zarabiają średnio o 11169,65 zł więcej niż pracownik o kodzie 1 (specjalista)\npwynagrodzenie - wzrost początkowego wynagrodzenia o 1 zł powoduje przeciętny wzrost bieżącego wynagrodzenia o 1,34 zł\nstaz - wzrost stażu pracy o miesiąc skutkuje przeciętnym wzrostem bieżącego wynagrodzenia o 154,51 zł\ndoswiadcznie - wzrost doświadczenia o miesiąc powoduje średni spadek bieżącego wynagrodzenia o 15,77 zł\nzwiazki1 - pracownicy należący do związków zawodowych zarabiają średnio o 1011,55 zł mniej aniżeli pracownicy, którzy do związków nie zależą\nwiek - wzrost wieku pracownika o 1 rok to przeciętnym spadek bieżącego wynagrodzenia o 64,79 zł\n\nWszystkie te interpretacje obowiązują przy założeniu ceteris paribus - przy pozostałych warunkach niezmienionych.\nTen model wymaga oczywiście ulepszenia do czego wykorzystamy pakiet olsrr.\nPierwszą kwestią, którą się zajmiemy jest współliniowość zmiennych. W regresji zmienne objaśniające powinny być jak najbardziej skorelowane ze zmienną objaśnianą, a możliwie nieskorelowane ze sobą. W związku z tym wybieramy ze zbioru wyłącznie cechy ilościowe, dla którym wyznaczymy współczynnik korelacji liniowej Pearsona. Do wizualizacji tych wartości wykorzystamy pakiet corrplot służący do wizualizacji współczynnika korelacji.\n\nlibrary(corrplot)\nlibrary(olsrr)\n\nkorelacje &lt;- pracownicy2 %&gt;%\n  select(-c(plec, kat_pracownika, zwiazki)) %&gt;%\n  cor()\n\ncorrplot(korelacje, method = \"number\", type = \"upper\")\n\n\n\n\n\n\n\n\nMożemy zauważyć, że wartości bieżącego wynagrodzenia są najsilniej skorelowane w wartościami wynagrodzenia początkowego. Także doświadczenie i wiek są silnie ze sobą związane, co może sugerować, że obie zmienne wnoszą do modelu podobną informację.\nW związku z tym powinniśmy wyeliminować niektóre zmienne z modelu pozostawiając te najważniejsze. Wyróżnia się trzy podejścia do tego zagadnienia:\n\nekspercki dobór cech,\nbudowa wszystkich możliwych modeli i wybór najlepszego według określonego kryterium,\nregresja krokowa.\n\nW przypadku budowy wszystkich możliwych modeli należy pamiętać o rosnącej wykładniczo liczbie modeli: \\(2^p-1\\), gdzie \\(p\\) oznacza liczbę zmiennych objaśniających. w rozważanym przypadku liczba modeli wynosi 255.\n\nwszystkie_modele &lt;- ols_step_all_possible(model)\n\nW uzyskanym zbiorze danych są informacje o numerze modelu, liczbie użytych zmiennych, nazwie tych zmiennych oraz wiele miar jakości. Te, które warto wziąć pod uwagę to przede wszystkim:\n\nrsquare - współczynnik R-kwadrat,\naic - kryterium informacyjne Akaike,\nmsep - błąd średniokwadratowy predykcji.\n\nNajwyższa wartość współczynnika \\(R^2\\) związana jest z modelem zawierającym wszystkie dostępne zmienne objaśniające. Jest to pewna niedoskonałość tej miary, która rośnie wraz z liczbą zmiennych w modelu, nawet jeśli te zmienne nie są istotne.\nW przypadku kryteriów informacyjnych oraz błędu średniokwadratowego interesują nas jak najmniejsze wartości. Wówczas jako najlepszy należy wskazać model nr 219 zawierający 6 zmiennych objaśniających.\nMetodą, która także pozwoli uzyskać optymalny model, ale przy mniejszym obciążeniu obliczeniowym jest regresja krokowa polegająca na krokowym budowaniu modelu.\n\nols_step_both_aic(model)\n\n\n                                    Stepwise Summary                                    \n--------------------------------------------------------------------------------------\nStep    Variable                 AIC          SBC         SBIC        R2       Adj. R2 \n--------------------------------------------------------------------------------------\n 0      Base Model            10565.472    10573.790    9220.060    0.00000    0.00000 \n 1      pwynagrodzenie (+)     9862.260     9874.737    8518.596    0.77484    0.77436 \n 2      kat_pracownika (+)     9786.152     9806.947    8440.801    0.80992    0.80870 \n 3      doswiadczenie (+)      9743.487     9768.441    8398.512    0.82705    0.82557 \n 4      staz (+)               9719.469     9748.582    8374.863    0.83630    0.83455 \n 5      edukacja (+)           9707.338     9740.610    8363.030    0.84112    0.83907 \n 6      plec (+)               9703.188     9740.620    8359.065    0.84317    0.84081 \n--------------------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                             Model Summary                              \n-----------------------------------------------------------------------\nR                          0.918       RMSE                   6762.202 \nR-Squared                  0.843       MSE                46514089.581 \nAdj. R-Squared             0.841       Coef. Var                19.815 \nPred R-Squared             0.837       AIC                    9703.188 \nMAE                     4418.325       SBC                    9740.620 \n-----------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                      ANOVA                                        \n----------------------------------------------------------------------------------\n                        Sum of                                                    \n                       Squares         DF        Mean Square       F         Sig. \n----------------------------------------------------------------------------------\nRegression    116287161827.859          7    16612451689.694    357.149    0.0000 \nResidual       21629051655.016        465       46514089.581                      \nTotal         137916213482.875        472                                         \n----------------------------------------------------------------------------------\n\n                                         Parameter Estimates                                           \n------------------------------------------------------------------------------------------------------\n          model         Beta    Std. Error    Std. Beta      t        Sig          lower        upper \n------------------------------------------------------------------------------------------------------\n    (Intercept)    -6547.147      3402.860                 -1.924    0.055    -13234.036      139.741 \n pwynagrodzenie        1.342         0.073        0.618    18.382    0.000         1.198        1.485 \nkat_pracownika2     6734.992      1631.122        0.092     4.129    0.000      3529.708     9940.275 \nkat_pracownika3    11226.635      1368.413        0.251     8.204    0.000      8537.596    13915.674 \n  doswiadczenie      -22.302         3.571       -0.137    -6.246    0.000       -29.318      -15.285 \n           staz      147.865        31.461        0.087     4.700    0.000        86.041      209.689 \n       edukacja      501.391       160.270        0.085     3.128    0.002       186.447      816.335 \n          plec1    -1878.949       761.703       -0.055    -2.467    0.014     -3375.755     -382.143 \n------------------------------------------------------------------------------------------------------\n\n\nOtrzymany w ten sposób model jest tożsamy z modelem charakteryzującym się najlepszymi miarami jakości spośród zbioru wszystkich możliwych modeli:\n\nwybrany_model &lt;- lm(bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + doswiadczenie + staz + plec + edukacja, data = pracownicy2)\nsummary(wybrany_model)\n\n\nCall:\nlm(formula = bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + \n    doswiadczenie + staz + plec + edukacja, data = pracownicy2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-22922  -3300   -673   2537  46524 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -6547.147   3402.860  -1.924  0.05496 .  \npwynagrodzenie      1.342      0.073  18.382  &lt; 2e-16 ***\nkat_pracownika2  6734.992   1631.122   4.129 4.32e-05 ***\nkat_pracownika3 11226.635   1368.413   8.204 2.30e-15 ***\ndoswiadczenie     -22.302      3.571  -6.246 9.51e-10 ***\nstaz              147.865     31.461   4.700 3.43e-06 ***\nplec1           -1878.949    761.703  -2.467  0.01399 *  \nedukacja          501.391    160.270   3.128  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6820 on 465 degrees of freedom\nMultiple R-squared:  0.8432,    Adjusted R-squared:  0.8408 \nF-statistic: 357.1 on 7 and 465 DF,  p-value: &lt; 2.2e-16\n\n\nUzyskany model charakteryzuje się nieznacznie większym błędem resztowym od modelu ze wszystkimi zmiennymi, ale wszystkie zmienne są istotne. Wyraz wolny (Intercept) nie musi być istotny w modelu.\nWróćmy jeszcze na chwilę do tematu współliniowości zmiennych objaśniających:\n\nols_vif_tol(wybrany_model)\n\n        Variables Tolerance      VIF\n1  pwynagrodzenie 0.2979792 3.355939\n2 kat_pracownika2 0.6867111 1.456217\n3 kat_pracownika3 0.3595692 2.781106\n4   doswiadczenie 0.7054176 1.417600\n5            staz 0.9862198 1.013973\n6           plec1 0.6831049 1.463904\n7        edukacja 0.4607529 2.170361\n\n\nWspółczynnik tolerancji wskazuje na procent niewyjaśnionej zmienności danej zmiennej przez pozostałe zmienne objaśniające. Przykładowo współczynnik tolerancji dla początkowego wynagrodzenia wynosi 0,2980, co oznacza, że 30% zmienności początkowego wynagrodzenia nie jest wyjaśnione przez pozostałe zmienne w modelu. Z kolei współczynnik VIF jest obliczany na podstawie wartości współczynnika tolerancji i wskazuje o ile wariancja szacowanego współczynnika regresji jest podwyższona z powodu współliniowości danej zmiennej objaśniającej z pozostałymi zmiennymi objaśniającymi. Wartość współczynnika VIF powyżej 4 należy uznać za wskazującą na współliniowość. W analizowanym przypadku takie zmienne nie występują.\nOcena siły wpływu poszczególnych zmiennych objaśniających na zmienną objaśnianą w oryginalnej postaci modelu nie jest możliwa. Należy wyznaczyć standaryzowane współczynniki beta, które wyliczane są na danych standaryzowanych, czyli takich, które są pozbawione jednostek i cechują się średnią równą 0, a odchyleniem standardowym równym 1. Standaryzacja ma sens tylko dla cech numerycznych, w związku z czym korzystamy z funkcji mutate_if(), która jako pierwszy argument przyjmuje warunek, który ma być spełniony, aby była zastosowane przekształcenie podawane jako drugi argument.\n\npracownicy2_std &lt;- pracownicy2 %&gt;%\n  mutate_if(is.numeric, scale)\n\nwybrany_model_std &lt;- lm(bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + \n                          doswiadczenie + staz + plec + edukacja, data = pracownicy2_std)\nsummary(wybrany_model_std)\n\n\nCall:\nlm(formula = bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + \n    doswiadczenie + staz + plec + edukacja, data = pracownicy2_std)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.34098 -0.19306 -0.03939  0.14841  2.72171 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.08893    0.03144  -2.828  0.00488 ** \npwynagrodzenie   0.61842    0.03364  18.382  &lt; 2e-16 ***\nkat_pracownika2  0.39400    0.09542   4.129 4.32e-05 ***\nkat_pracownika3  0.65677    0.08005   8.204 2.30e-15 ***\ndoswiadczenie   -0.13657    0.02187  -6.246 9.51e-10 ***\nstaz             0.08691    0.01849   4.700 3.43e-06 ***\nplec1           -0.10992    0.04456  -2.467  0.01399 *  \nedukacja         0.08464    0.02706   3.128  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.399 on 465 degrees of freedom\nMultiple R-squared:  0.8432,    Adjusted R-squared:  0.8408 \nF-statistic: 357.1 on 7 and 465 DF,  p-value: &lt; 2.2e-16\n\n\nSpośród cech ilościowych największy wpływ na zmienną objaśnianą mają wartości wynagrodzenia początkowego, staż, edukacja i na końcu doświadczenie.\nReszty czyli różnice pomiędzy obserwowanymi wartościami zmiennej objaśnianej, a wartościami wynikającymi z modelu w klasycznej metodzie najmniejszych kwadratów powinny być zbliżone do rozkładu normalnego. Oznacza to, że najwięcej reszt powinno skupiać się wokół zerowych różnic, natomiast jak najmniej powinno być wartości modelowych znacznie różniących się od tych rzeczywistych.\n\nols_plot_resid_hist(wybrany_model)\n\n\n\n\n\n\n\n\nReszty w naszym modelu wydają się być zbliżone do rozkładu normalnego. Jednoznaczną odpowiedź da jednak odpowiedni test.\n\nols_test_normality(wybrany_model)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.868          0.0000 \nKolmogorov-Smirnov        0.1092         0.0000 \nCramer-von Mises         42.5504         0.0000 \nAnderson-Darling         13.0233         0.0000 \n-----------------------------------------------\n\n\nHipoteza zerowa w tych testach mówi o zgodności rozkładu reszt z rozkładem normalnym. Na podstawie wartości p, które są mniejsze od \\(\\alpha=0,05\\) stwierdzamy, że są podstawy do odrzucenia tej hipotezy czyli reszty z naszego modelu nie mają rozkładu normalnego. W diagnostyce przyczyn takiego stanu rzeczy pomoże nam wykres kwantyl-kwantyl:\n\nols_plot_resid_qq(wybrany_model)\n\n\n\n\n\n\n\n\nGdyby wszystkie punkty leżały na prostej to oznaczałoby to normalność rozkładu reszt. Tymczasem po lewej i prawej stronie tego wykresu znajdują się potencjalne wartości odstające, które znacznie wpływają na rozkład reszt modelu.\nWartości odstające można ustalić na podstawie wielu kryteriów. Do jednych z najbardziej popularnych należy odległość Cooka:\n\ncook &lt;- ols_plot_cooksd_bar(wybrany_model)\n\n\n\n\n\n\n\n\nPrzypisanie tej funkcji do obiektu zwraca nam tabelę z numerami zidentyfikowanych obserwacji wpływowych. W przypadku odległości Cooka jest to 35 obserwacji. Granica przynależności do grupy wartości odstających jest wyznaczana według wzoru: \\(4/n\\), gdzie \\(n\\) to liczba wszystkich obserwacji.\nInną miarą są reszty studentyzowane.\n\nstud3 &lt;- ols_plot_resid_stud(wybrany_model)\n\n\n\n\n\n\n\n\nWyżej wykorzystana funkcja jako kryterium odstawania przyjmuje wartość 3 identyfikując 10 obserwacji wpływowych. Z kolei dodanie do powyższej funkcji przyrostka fit powoduje przyjęcie jako granicy wartości równej 2.\n\nstud2 &lt;- ols_plot_resid_stud_fit(wybrany_model)\n\n\n\n\n\n\n\n\nW ten sposób zostało zidentyfikowanych 22 obserwacji odstających. Korzystając z odległości Cooka wyeliminujemy obserwacje odstające ze zbioru:\n\noutliers &lt;- cook$data$obs[cook$data$color == \"outlier\"]\n\npracownicy_out &lt;- pracownicy2[-outliers,]\n\nwybrany_model_out &lt;- lm(bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + doswiadczenie + staz + plec + edukacja, \n                        data = pracownicy_out)\nsummary(wybrany_model_out)\n\n\nCall:\nlm(formula = bwynagrodzenie ~ pwynagrodzenie + kat_pracownika + \n    doswiadczenie + staz + plec + edukacja, data = pracownicy_out)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12591.2  -2696.4   -564.8   2263.0  14704.1 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -3756.8288  2381.5361  -1.577 0.115420    \npwynagrodzenie      1.3696     0.0676  20.260  &lt; 2e-16 ***\nkat_pracownika2  6480.6971  1049.9539   6.172 1.56e-09 ***\nkat_pracownika3  9791.5000  1059.4980   9.242  &lt; 2e-16 ***\ndoswiadczenie     -18.5808     2.3229  -7.999 1.17e-14 ***\nstaz              116.3809    20.9704   5.550 5.01e-08 ***\nplec1           -1616.6030   505.1391  -3.200 0.001475 ** \nedukacja          397.7652   105.8146   3.759 0.000194 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4271 on 430 degrees of freedom\nMultiple R-squared:  0.894, Adjusted R-squared:  0.8923 \nF-statistic: 518.3 on 7 and 430 DF,  p-value: &lt; 2.2e-16\n\n\nModel dopasowany na takim zbiorze charakteryzuje się dużo mniejszym błędem standardowym oraz wyższym współczynnikiem \\(R^2\\) w porównaniu do poprzedniego. Sprawdźmy w takim razie normalność reszt.\n\nols_plot_resid_qq(wybrany_model_out)\n\n\n\n\n\n\n\n\nWykres kwantyl-kwantyl wygląda już dużo lepiej i w zasadzie nie obserwuje tu się dużych odstępstw od rozkładu normalnego. Przeprowadźmy jeszcze odpowiednie testy statystyczne.\n\nols_test_normality(wybrany_model_out)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.9696         0.0000 \nKolmogorov-Smirnov        0.0667         0.0405 \nCramer-von Mises         38.1644         0.0000 \nAnderson-Darling          3.5421         0.0000 \n-----------------------------------------------\n\n\nTylko test Kołmogorova-Smirnova na poziomie istotności \\(\\alpha=0,01\\) wskazał na brak podstaw do odrzucenia hipotezy zerowej.\n\n4.3.1 Zadanie\nNa podstawie zbioru dotyczącego 50 startupów określ jakie czynniki wpływają na przychód startupów.\n\nstartupy &lt;- read_xlsx(\"data/startups.xlsx\")\n\nstartupy &lt;- janitor::clean_names(startupy)\n\nsummary(startupy)\n\n   r_d_spend      administration   marketing_spend     state          \n Min.   :     0   Min.   : 51283   Min.   :     0   Length:50         \n 1st Qu.: 39936   1st Qu.:103731   1st Qu.:129300   Class :character  \n Median : 73051   Median :122700   Median :212716   Mode  :character  \n Mean   : 73722   Mean   :121345   Mean   :211025                     \n 3rd Qu.:101603   3rd Qu.:144842   3rd Qu.:299469                     \n Max.   :165349   Max.   :182646   Max.   :471784                     \n     profit      \n Min.   : 14681  \n 1st Qu.: 90139  \n Median :107978  \n Mean   :112013  \n 3rd Qu.:139766  \n Max.   :192262",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Regresja</span>"
    ]
  },
  {
    "objectID": "05-grupowanie.html",
    "href": "05-grupowanie.html",
    "title": "5  Grupowanie",
    "section": "",
    "text": "5.1 Wprowadzenie\nGrupowanie polega na przypisanie obiektów do określonych grup/klastrów/skupień/segmentów, w których znajdą się jednostki najbardziej do siebie podobne, a powstałe grupy będą się między sobą różnić. Całe utrudnienie polega na tym, że nie wiemy ile tych grup ma powstać.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Grupowanie</span>"
    ]
  },
  {
    "objectID": "05-grupowanie.html#metoda-k-średnich",
    "href": "05-grupowanie.html#metoda-k-średnich",
    "title": "5  Grupowanie",
    "section": "5.2 Metoda k-średnich",
    "text": "5.2 Metoda k-średnich\nNajpopularniejszą metodą grupowania jest metoda k-średnich. Do jej zalet należy zaliczyć to, że dobrze działa zarówno na małych, jak i dużych zbiorach i jest bardzo efektywny - zwykle osiąga zbieżność w mniej niż 10 iteracjach. Z wad należy wskazać losowy wybór początkowych centrów skupień, co może skutkować nieprawidłowym przypisaniem obiektów do grup.\nAlgorytm postępowania jest następujący:\n\nWskaż liczbę grup \\(k\\).\nWybierz dowolne \\(k\\) punktów jako centra grup.\nPrzypisz każdą z obserwacji do najbliższego centroidu.\nOblicz nowe centrum grupy.\nPrzypisz każdą z obserwacji do nowych centroidów. Jeśli któraś obserwacja zmieniła grupę - przejdź do kroku nr 4, a w przeciwnym przypadku zakończ algorytm.\n\nWykorzystamy informacje ze zbioru zawierającego informacje o klientach sklepu i dokonamy grupowania tych klientów.\nOpis zbioru:\n\nklientID - identyfikator klienta\nplec - płeć\nwiek - wiek\nroczny_dochod - roczny dochód wyrażony w tys. dolarów\nwskaznik_wydatkow - klasyfikacja sklepu od 1 do 100\n\nWczytujemy zbiór danych i sprawdzamy czy pomiędzy zmiennymi są widoczne jakieś zależności.\n\nlibrary(tidyverse)\n\nklienci &lt;- read.csv(\"data/klienci.csv\")\n\nggplot(klienci, aes(x=wiek, y=roczny_dochod)) +\n  geom_point()\n\n\n\n\n\n\n\n\nPomiędzy wiekiem a rocznym dochodem nie widać zależności.\n\nggplot(klienci, aes(x=wiek, y=wskaznik_wydatkow)) +\n  geom_point()\n\n\n\n\n\n\n\n\nW przypadku wieku i wskaźnika wydatków moglibyśmy się pokusić o podział zbioru na dwie grupy za pomocą przekątnej.\n\nggplot(klienci, aes(x=wskaznik_wydatkow, y=roczny_dochod)) +\n  geom_point()\n\n\n\n\n\n\n\n\nPo zestawieniu rocznego dochodu i wskaźnika wydatków wyłania się 5 potencjalnych grup, zatem te dwie cechy wykorzystamy do grupowania. Jednak przed zastosowaniem algorytmu musimy te dane przygotować normalizując zakres obu cech - w tym przypadku za pomocą standaryzacji.\n\nklienci_z &lt;- klienci %&gt;%\n  select(roczny_dochod, wskaznik_wydatkow) %&gt;%\n  scale()\n\nhead(klienci_z)\n\n     roczny_dochod wskaznik_wydatkow\n[1,]     -1.734646        -0.4337131\n[2,]     -1.734646         1.1927111\n[3,]     -1.696572        -1.7116178\n[4,]     -1.696572         1.0378135\n[5,]     -1.658498        -0.3949887\n[6,]     -1.658498         0.9990891\n\n\nW przypadku, gdy podział na grupy nie jest tak oczywisty lub bierzemy pod uwagę więcej niż dwa kryteria to wówczas w wyznaczeniu optymalnej liczby skupień może pomóc wykres osypiska (ang. elbow method). Polega to na przeprowadzeniu grupowania (z wykorzystaniem funkcji kmeans()) dla różniej liczby grup i porównanie wariancji wewnątrz-grupowej. Dane do stworzenia wykresu osypiska możemy obliczyć w pętli:\n\nzm_w_gr &lt;- numeric(15)\n\n# wprowadzenie pętli\n\nfor(i in 1:length(zm_w_gr)) {\n  set.seed(14)\n  gr &lt;- kmeans(klienci_z, centers = i)\n  zm_w_gr[i] &lt;- gr$tot.withinss\n}\n\nplot(1:15, zm_w_gr, type=\"b\")\n\n\n\n\n\n\n\n\nWybieramy liczbę skupień po której nie następuje już gwałtowny spadek wartości wariancji wewnątrz-grupowej. Według tego kryterium powinniśmy wybrać wartość 6 zamiast 5. Sprawdźmy zatem jakie otrzymamy przyporządkowanie do grup. Następnie informację o tym przypisaniu umieszczamy w oryginalnym zbiorze danych i przedstawiamy na wykresie. W celu uzyskania powtarzalnych wyników zastosujemy stałe ziarno generatora.\n\nset.seed(12)\ngrupy &lt;- kmeans(x = klienci_z, centers = 5)\n\nklienci$grupa &lt;- as.factor(grupy$cluster)\n\nggplot(klienci, aes(x=wskaznik_wydatkow, \n                    y=roczny_dochod,\n                    color=grupa)) +\n  geom_point()\n\n\n\n\n\n\n\n\nJak zauważamy ten podział nie jest właściwy. Ze względu na losowy przydział centrów skupień na początku algorytmu istnieje spora szansa, że rozwiązanie nie będzie optymalne. Rozwiązaniem tego problemu jest użycie algorytmu kmeans++ do początkowego ustalenia centrów. Ta metoda jest zaimplementowana w pakiecie ClusterR. Ponadto jest tam także funkcja do ustalenia optymalnej liczby skupień na podstawie wykresu osypiska.\n\nlibrary(ClusterR)\n\nOptimal_Clusters_KMeans(data = klienci_z, max_clusters = 15, criterion = \"WCSSE\")\n\n\n\n\n\n\n\n\n [1] 398.00000 274.82588 156.91549 108.68209  65.24057  55.10046  49.76380\n [8]  47.09658  40.30544  30.31240  27.17200  25.79340  24.41910  22.23143\n[15]  21.81531\n\n\nWybieramy liczbę skupień po której nie następuje już gwałtowny spadek wartości wariancji wewnątrz-grupowej. W analizowanym przypadku będzie to 5 grup.\nDodatkowo można obliczyć jedno z wielu kryteriów dobroci podziału obiektów na grupy. Jednym z najpopularniejszych jest kryterium Calińskiego-Harabasza zaimplementowane m.in. w pakiecie clusterCrit. Podobnie jak w przypadku wykresu osypiska należy policzyć wartość tego kryterium dla różnej liczby segemntów i wybrać liczbę grup wskazaną przez wartość maksymalną.\n\nlibrary(clusterCrit)\n\nkryt_ch &lt;- numeric(15)\n\nfor(i in 1:length(kryt_ch)) {\n  gr &lt;- KMeans_rcpp(klienci_z, clusters = i)\n  kryt_ch[i] &lt;- intCriteria(traj = klienci_z, part = as.integer(gr$clusters), crit = \"cal\")\n}\n\nplot(1:15, kryt_ch, type=\"b\")\n\n\n\n\n\n\n\n\nNajwyższe wartości indeksu Calińskiego-Harabasza obserwujemy dla 5 i 9 skupień. Należy jednak pamiętać, że w grupowaniu bardzo ważna jest ocena badacza pod kątem sensowności otrzymanego podziału - łatwiej stworzyć 5 różnych kampanii reklamowych aniżeli 9.\nNastępnie korzystamy z funkcji KMeans_rcpp do wyznaczenia przynależności do grup. Ta funkcja domyślnie korzysta z algorytmu kmeans++, zatem nie ma niebezpieczeństwa, że uzyskamy niewłaściwe przyporządkowanie.\n\ngrupy2 &lt;- KMeans_rcpp(data = klienci_z, clusters = 5)\n\nklienci$grupa2 &lt;- as.factor(grupy2$clusters)\n\nggplot(klienci, aes(x=wskaznik_wydatkow, \n                    y=roczny_dochod,\n                    color=grupa2)) +\n  geom_point()\n\n\n\n\n\n\n\n\nOstatnim etapem analizy jest odpowiednia charakterystyka uzyskanych klastrów - najczęściej wyznacza się średnie wartości cech w ramach każdej grupy.\n\nklienci %&gt;%\n  select(-klientID, -plec, -grupa) %&gt;%\n  group_by(grupa2) %&gt;%\n  summarise_all(.funs = \"mean\")\n\n# A tibble: 5 × 4\n  grupa2  wiek roczny_dochod wskaznik_wydatkow\n  &lt;fct&gt;  &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 1       25.3          25.7              79.4\n2 2       32.7          86.5              82.1\n3 3       41.1          88.2              17.1\n4 4       42.7          55.3              49.5\n5 5       45.2          26.3              20.9\n\n\nW grupie pierwszej znalazły się osoby z niskimi dochodami i wysokim wskaźnikiem wydatków. Grupa druga to klienci o niskich dochodach i wydatkach - ich przeciwieństwem jest grupa 4. W grupie 3 są osoby z wysokimi dochodami, ale niskimi wydatkami. Grupa 5 to z kolei średniacy - klienci o średnich dochodach i wydatkach.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Grupowanie</span>"
    ]
  },
  {
    "objectID": "05-grupowanie.html#metoda-hierarchiczna",
    "href": "05-grupowanie.html#metoda-hierarchiczna",
    "title": "5  Grupowanie",
    "section": "5.3 Metoda hierarchiczna",
    "text": "5.3 Metoda hierarchiczna\nAlternatywną metodą grupowania jest metoda hierarchiczna. Do jej zalet zaliczymy prosty sposób ustalenia liczby grup oraz praktyczny sposób wizualizacji. Niestety nie jest to metoda odpowiednia dla dużych zbiorów danych.\nAlgorytm postępowania:\n\nKażda obserwacji stanowi jedną z \\(N\\) pojedynczych grup.\nNa podstawie macierzy odległości połącz dwie najbliżej leżące obserwacje w jedną grupę (\\(N-1\\) grup).\nPołącz dwa najbliżej siebie leżące grupy w jedną (\\(N-2\\) grup).\nPowtórz krok nr 3, aż do uzyskania jednej grupy.\n\nDla tych samych danych przeprowadzimy grupowanie, ale tym razem metodą hierarchiczną. W metodzie hierarchicznej bazuje się na macierzy odległości pomiędzy obserwacjami. Można zastosować wiele miar odległości, ale najczęściej wykorzystywana jest odległość euklidesowa. Druga zmienna, na którą mamy wpływ to metoda łączenia skupień - w tym przypadku najlepsze rezultaty daje metoda Warda. Z kolei wyniki grupowania metodą hierarchiczną są prezentowane na dendrogramie.\n\nmacierz_odl &lt;- dist(klienci_z)\n\ndendrogram &lt;- hclust(macierz_odl, method = \"ward.D\")\n\nplot(dendrogram, xlab=\"Klienci\", ylab=\"Odległość euklidesowa\")\n\n\n\n\n\n\n\n\nNa podstawie dendrogramu identyfikujemy największe różnice odległości opisane na osi Y. Także w tym przypadku identyfikujemy 5 grup. Istnieje także wiele kryteriów, które mają na celu wyznaczyć optymalną liczbę grup - link.\n\nplot(dendrogram, xlab=\"Klienci\", ylab=\"Odległość euklidesowa\")\nrect.hclust(dendrogram, k=5, border=\"red\")\n\n\n\n\n\n\n\n\nNastępnie dopisujemy do oryginalnego zbioru danych etykiety utworzonych grup.\n\ngrupy_dendro &lt;- cutree(dendrogram, 5)\n\nklienci$grupa3 &lt;- as.factor(grupy_dendro)\n\nggplot(klienci, aes(x=wskaznik_wydatkow, \n                    y=roczny_dochod,\n                    color=grupa3)) +\n  geom_point()\n\n\n\n\n\n\n\n\nUzyskane wyniki są bardzo zbliżone do tych otrzymanych za pomocą algorytmu k-średnich.\n\nklienci %&gt;%\n  select(-klientID, -plec, -grupa, -grupa2) %&gt;%\n  group_by(grupa3) %&gt;%\n  summarise_all(.funs = \"mean\")\n\n# A tibble: 5 × 4\n  grupa3  wiek roczny_dochod wskaznik_wydatkow\n  &lt;fct&gt;  &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1 1       45.2          26.3              20.9\n2 2       25.3          25.1              80.0\n3 3       42.5          55.8              49.1\n4 4       32.7          86.5              82.1\n5 5       41            89.4              15.6\n\n\nMetoda hierarchiczna zastosowała inną numerację grup. Liczebności tych grup nieznacznie się różnią, ale charakterystyki wewnątrz grupowe są bardzo podobne do tych określonych na podstawie metody k-średnich.\nTworząc tabelę krzyżową możemy zobaczyć, że tylko 4 obserwacje zmieniły przypisanie do grup.\n\ntable(klienci$grupa2, klienci$grupa3)\n\n   \n     1  2  3  4  5\n  1  0 21  1  0  0\n  2  0  0  0 39  0\n  3  0  0  3  0 32\n  4  0  0 81  0  0\n  5 23  0  0  0  0\n\n\nPorównajmy jeszcze wyniki działania tych dwóch metod na wykresach:\n\nklienci %&gt;%\n  select(roczny_dochod, wskaznik_wydatkow, grupa2, grupa3) %&gt;%\n  gather(metoda, grupa, -roczny_dochod, -wskaznik_wydatkow) %&gt;%\n  ggplot(aes(x=wskaznik_wydatkow, y=roczny_dochod)) +\n  geom_point(aes(color=grupa)) +\n  facet_wrap(~ metoda)\n\n\n\n\n\n\n\n\nProblematyczne obserwacje pochodziły z grupy klientów o przeciętnych dochodach oraz wydatkach.\n\n5.3.1 Zadania\n\nDokonaj grupowania danych dotyczących 32 samochodów według następujących zmiennych: pojemność, przebieg, lata oraz cena.\nRozpoznawanie czynności na podstawie danych z przyspieszeniomierza w telefonie: User Identification From Walking Activity Data Set",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Grupowanie</span>"
    ]
  }
]