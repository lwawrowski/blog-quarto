[
  {
    "objectID": "projekty.html",
    "href": "projekty.html",
    "title": "Projekty",
    "section": "",
    "text": "Aplikacje\nMapa dostępności stacji rowerów miejskich w Katowicach, City Bike Hack, 2.03.2019.\nGra memory do nauki statystyki\nGra turowa z algorytmem sztucznej inteligencji - w ramach projektu GAMEINN: Opracowanie silnika sztucznej inteligencji w strategicznej grze ekonomicznej w celu urealnienia i udoskonalenia interakcji graczy z NPC\nAplikacja do szacowania modelu Faya-Herriota\n\n\nPakiety\nopenPoznan - otwarte dane na temat Poznania\n\n\nRaporty\nAnaliza walorów turystycznych powiatów i ich bezpośredniego otoczenia na podstawie danych statystycznych m. in. z zakresu bazy noclegowej, kultury i dziedzictwa narodowego oraz przyrodniczych obszarów chronionych\nDezagregacja wskaźników strategii Europa 2020 na poziom NTS 2 z zakresu pomiaru ubóstwa i wykluczenia społecznego\nMapy ubóstwa na poziomie podregionów w Polsce z wykorzystaniem estymacji pośredniej\nPomiar ubóstwa na poziomie powiatów (LAU 1)"
  },
  {
    "objectID": "posts/2020-02-18-smog-wiatr/smog-wiatr.html",
    "href": "posts/2020-02-18-smog-wiatr/smog-wiatr.html",
    "title": "Smog w Pszczynie a kierunek i siła wiatru",
    "section": "",
    "text": "Istotnym czynnikiem mającym wpływ na jakość powietrza jest także siła i kierunek wiatru. Instytut Meteorologii i Gospodarki Wodnej gromadzi takie dane w odstępach 10 minutowych i można je pobrać z tej strony dla wszystkich stacji. Dane archiwalne są w plikach .zip dla każdego miesiąca osobno. Pobrałem ręcznie 60 plików i je rozpakowałem, a następnie wczytałem je do R za pomocą poniższego skryptu.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nwczytaj_pliki &lt;- function(nazwa){\n  \n  d &lt;- read_csv2(str_c(\"wiatr/\",nazwa), col_names = F) %&gt;% \n    filter(X1==249180010) # stacja w Pszczynie\n  \n  return(d)\n}\n\n# kierunek wiatru\n\npliki_wiatr_kierunek &lt;- str_c(sort(rep(str_c(\"B00202A_\",2015:2019),12)),\n                              \"_\",sprintf(\"%02d\",1:12),\".csv\")\n\nwiatr_kierunek &lt;- map_df(pliki_wiatr_kierunek, wczytaj_pliki)\n\nwiatr_kierunek &lt;- wiatr_kierunek %&gt;% \n  select(data_time=X3, kierunek=X4) %&gt;% \n  mutate(data=as_date(data_time),\n         kierunek8=cut(x=kierunek, breaks = seq(from = 0, to = 360, by = 22.5), \n                       labels = c(\"N\", \"NE\", \"NE\", \"E\", \"E\", \"SE\", \"SE\", \"S\", \n                                  \"S\", \"SW\", \"SW\", \"W\", \"W\", \"NW\", \"NW\", \"N\")))\n\n# siła wiatru\n\npliki_wiatr_sila &lt;- str_c(sort(rep(str_c(\"B00702A_\",2015:2019),12)),\n                          \"_\",sprintf(\"%02d\",1:12),\".csv\")\n\nwiatr_sila &lt;- map_df(pliki_wiatr_sila, wczytaj_pliki)\n\nwiatr_sila &lt;- wiatr_sila %&gt;% \n  select(data_time=X3, sila=X4)\n\nwiatr_kierunek_sila &lt;- inner_join(wiatr_kierunek, wiatr_sila, by = \"data_time\")\n\nWynikowy zbiór danych zawiera 257484 obserwacji i 5 kolumn. Kierunek wiatru jest określany na planie koła w zakresie o 0° do 360°. Dobrze to obrazuje poniższy obrazek.\n\nNa potrzeby analizy podzieliłem koło na 8 części po 45°, aby wyodrębnić najważniejsze kierunki wiatru.\n\nwiatr_kierunek_sila %&gt;% \n  count(kierunek8) %&gt;% \n  mutate(kierunek8=fct_reorder(kierunek8, n)) %&gt;% \n  ggplot(aes(x=kierunek8, y=n)) +\n  geom_col(fill = \"#6daaee\") +\n  coord_flip() +\n  ylim(0,80000) +\n  xlab(\"Kierunek wiatru\") +\n  ylab(\"Liczba pomiarów\") +\n  ggtitle(\"Liczba pomiarów kierunku wiatru - dane 10-minutowe\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nW Pszczynie wiatr najczęściej wieje z południowego-zachodu (SW). Na drugim i trzecim miejscu z podobnym rezultatem jest wiatr zachodni (W) i wschodni (E). Najrzadziej doświadczany jest wiatr północny.\nNa potrzeby analizy smogu dane dotyczące wiatru musiały zostać zagregowane do danych dziennych. W tym celu obliczyłem liczbę pomiarów kierunku wiatru każdego dnia i dla każdego dnia wybrałem ten dominujący. Jeśli jakiegoś dnia dwa kierunki występowały tyle samo razy to wybierałem ten, który wiał silniej w odniesieniu do mediany.\n\nwiatr_kierunek_sila_dzien &lt;- wiatr_kierunek_sila %&gt;% \n  group_by(data,kierunek8) %&gt;% \n  summarise(n=n(),\n            sila_med=median(sila)) %&gt;% \n  group_by(data) %&gt;% \n  top_n(1,n) %&gt;% \n  top_n(1,sila_med) %&gt;% \n  ungroup()\n\n`summarise()` has grouped output by 'data'. You can override using the\n`.groups` argument.\n\n\nRezultatem jest zbiór zawierający 1815 obserwacji. Sprawdźmy czy zmienił się rozkład występowania kierunku wiatru.\n\nwiatr_kierunek_sila_dzien %&gt;% \n  count(kierunek8) %&gt;% \n  mutate(kierunek8=fct_reorder(kierunek8, n)) %&gt;% \n  ggplot(aes(x=kierunek8, y=n)) +\n  geom_col(fill = \"#6daaee\") +\n  coord_flip() +\n  xlab(\"Kierunek wiatru\") +\n  ylab(\"Liczba pomiarów\") +\n  ggtitle(\"Liczba pomiarów kierunku wiatru - dane dzienne\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNadal dominuje wiatr południowo-zachodni, natomiast nastąpiła zamiana na drugim i trzecim miejscu - wiatru zachodniego ze wschodnim. Także na ostatnich dwóch pozycjach kategorie są odwrotnie w porównaniu do pomiarów co 10 minut.\nZobaczmy jak analizowane zjawisko kształtowało się w czasie.\n\nggplot(wiatr_kierunek_sila_dzien, aes(x=data, y=sila_med, color=kierunek8)) +\n  geom_point() +\n  scale_color_discrete(name = \"Kierunek wiatru\") +\n  xlab(\"Data\") +\n  ylab(\"Mediana siły wiatru (w m/s)\") +\n  ggtitle(\"Siła i kierunek wiatru w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\")) +\n  guides(col = guide_legend(nrow = 1))\n\n\n\n\nMożna zauważyć, że zimą wiatr wieje silniej niż w pozostałej części roku. Wśród wysokich wartości dominuje kierunek południowo-zachodni i zachodni.\nW następnej kolejności zestawimy te dane ze stężeniem pyłów PM10.\n\npm10_wiatr &lt;- inner_join(pm10_pszczyna, wiatr_kierunek_sila_dzien, by=\"data\") %&gt;% \n  filter(!is.na(pm10))\n\nggplot(pm10_wiatr, aes(x=sila_med, y=pm10, color=kierunek8)) +\n  geom_point() +\n  scale_color_discrete(name = \"Kierunek wiatru\") +\n  xlab(\"Mediana siły wiatru\") +\n  ylab(\"Stężenie PM10\") +\n  ggtitle(\"Zależność pomiędzy stężeniem PM10 i siłą wiatru w latach 2015-2019\")+\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\")) +\n  guides(col = guide_legend(nrow = 1))\n\n\n\n\nNa wykresie nie jest widoczna silna współzależność. Współczynnik korelacji liniowej pomiędzy tymi zmiennymi wynosi \\(r=-0,22\\), co oznacza, że pomiędzy siłą wiatru a stężeniem PM10 występuje słaba ujemna zależność. Także w przypadku kierunku wiatru nie można zidentyfikować wzorca zależności. Wysokie stężenie PM10 występuję dla wiatru z kierunku południowo-zachodniego (SW), wschodniego (E), a także północno-wschodniego (NE).\nW kolejnym kroku zestawimy indeks jakości powietrza z kierunkiem wiatru.\n\ntable(pm10_wiatr$indeks, pm10_wiatr$kierunek8) %&gt;% knitr::kable()\n\n\n\n\n\nN\nNE\nE\nSE\nS\nSW\nW\nNW\n\n\n\n\nbardzo dobry\n1\n9\n11\n1\n11\n133\n90\n28\n\n\ndobry\n5\n96\n187\n14\n74\n336\n149\n76\n\n\numiarkowany\n7\n28\n72\n1\n14\n89\n22\n27\n\n\ndostateczny\n4\n12\n38\n1\n4\n62\n9\n9\n\n\nzły\n3\n5\n27\n0\n3\n30\n4\n3\n\n\nbardzo zły\n3\n8\n30\n0\n4\n30\n5\n4\n\n\n\n\n\nNajwięcej dni ze złym i bardzo złym indeksem powietrza występowało razem z wiatrem wschodnim (E) i południwo-zachodnim (SW), co jest związane z faktem, że wiatr z tych kierunków najczęściej występował w Pszczynie. Powyższe dane zestawimy także w postaci względnej.\n\nround_preserve_sum &lt;- function(x, digits = 0) {\n  up &lt;- 10 ^ digits\n  x &lt;- x * up\n  y &lt;- floor(x)\n  indices &lt;- tail(order(x-y), round(sum(x)) - sum(y))\n  y[indices] &lt;- y[indices] + 1\n  y / up\n}\n\npm10_wiatr %&gt;% \n  count(indeks, kierunek8) %&gt;% \n  group_by(indeks) %&gt;% \n  mutate(proc=round_preserve_sum(n/sum(n)*100),\n         proc_label=ifelse(proc &gt; 0, proc, NA)) %&gt;% \n  ggplot(aes(x=indeks, y=proc, fill=kierunek8)) +\n  geom_col() +\n  geom_text(aes(label = proc_label), position = position_stack(0.5)) +\n  scale_fill_discrete(name = \"Kierunek wiatru\") +\n  xlab(\"Indeks jakości powietrza\") +\n  ylab(\"Odsetek dni (w %)\") +\n  coord_flip() +\n  ggtitle(\"Udział kierunku wiatru w ramach indeksu powietrza\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\")) +\n  guides(fill = guide_legend(nrow = 1))\n\n\n\n\nNa rysunku można zauważyć, że bez względu na indeks jakości powietrza największy odsetek stanowi wiatr wiejący z południowego-zachodu (SW), a na drugim miejscu jest wiatr wschodni (E). Jedynie w przypadku bardzo dobrego poziomu jakości powietrza na drugim miejscu znajduje się wiatr zachodni (W).\nNiniejsza analiza nie pozwala w sposób jednoznaczny powiązać jakości powietrza z kierunkiem wiatru. Być może analiza bardziej szczegółowych danych na temat pyłów pozwoliłaby lepiej zbadać ten wpływ."
  },
  {
    "objectID": "posts/2017-11-09-imie-papiez/imie-papiez.html",
    "href": "posts/2017-11-09-imie-papiez/imie-papiez.html",
    "title": "Najpopularniejsze imię wśród papieży",
    "section": "",
    "text": "W ostatnim czasie zastanawiałem się, które imię było najczęściej wybierane przez papieży. Jako przeciętny śmiertelnik potrafię wymienić 6 ostatnich papieży i na tej podstawie stwierdzam, że dużą popularnością cieszyło się na pewno imię Jan oraz Benedykt. Do ustalenia rankingu imion papieży wykorzystam R oraz dane z wikipedii.\nLista papieży dostępna jest w postaci tabel html w podziale na kolejne wieki naszej ery. Do pobrania zawartości tych tabel najlepszy będzie pakiet rvest. Oprócz niego ładuje także pakiet tidyverse.\n\nlibrary(tidyverse)\nlibrary(rvest)\n\nwiki &lt;- \"https://pl.wikipedia.org/wiki/Poczet_papie%C5%BCy\"\n\ntmp &lt;- wiki %&gt;%\n  read_html %&gt;%\n  html_nodes(\"table\")\n\nNa stronie zidentyfikowano 24 tabele - w 21 znajdują się dane na temat papieży. XX wiek poprzedza tabela zawierająca informację o tym, że papieżowi przysługuje tytuł Suweren Państwa Watykańskiego. Dwie ostatnie tabele to stopka strony. W związku z powyższym wybieram tylko tabele zawierające interesujące mnie dane.\n\npopes_list &lt;- map(tmp, function(x) html_table(x, fill=T, header = T))\npopes_list &lt;- popes_list[-20]\npopes_list &lt;- popes_list[-22]\npopes_list &lt;- popes_list[-22]\n\nImię danego papieża do XI wieku znajdowało się w drugiej kolumnie tabeli, natomiast później dodano do tabel także herb, przez co imię przesunięto do trzeciej kolumny. Wybierając imiona papieży trzeba wziąć to pod uwagę.\n\npopes_names &lt;- map(popes_list, function(x) x[,2]) %&gt;% \n  bind_rows()\n\nnames(popes_names)=paste0(\"papiez\",1:3)\n\npopes_names &lt;- popes_names %&gt;% \n  mutate_all(str_replace_na, replacement=\"\") %&gt;% \n  mutate(papiez=str_c(papiez1,papiez2,papiez3))\n\nPobieżna analiza imion papieży na stronie wikipedii może zasygnalizować kilka problemów, które utrudniają uzyskanie obiektu zawierającego wyłącznie imię i numer papieża. W tabelach umieszczono antypapieży, którzy nie są obiektem naszego zainteresowania. Ponadto przy niektórych imionach pojawia się skrót zakonu benedyktynów: OSB. Także kolejne przypisy zostały umieszone obok wybranych imion. Możemy się także dowiedzieć, że papież Benedykt IX papieżem zostawał trzykrotnie, za trzecim razem “(…) przy pomocy entuzjastycznego ludu i łapownictwa.”. Ostatnią zmianą jaką należy przeprowadzić jest traktowanie imienia Jan Paweł jako jednego. Wszystkie te operacje można wykonać wykorzystując wyrażenia regularne.\n\npopes_names_clean &lt;- popes_names %&gt;%\n  # wyrzucenie antypapieży\n  filter(papiez != \"\",\n         !grepl(\"Antypapież\",papiez)) %&gt;%\n  # usunięcie OSB\n  mutate(papiez=gsub(\"OSB\", \"\", papiez),\n         # usunięcie przypisów\n         papiez=gsub(\"\\\\[.*\", \"\", papiez),\n         # usunięcie wyrażenia \"po raz ...\"\n         papiez=gsub(\"Po raz.*\", \"\", papiez),\n         # Jan Paweł jako jedno imię\n         papiez=gsub(\"Jan Paweł\", \"JanPaweł\", papiez)) %&gt;%\n  distinct() %&gt;%\n  separate(papiez, into = c(\"name\", \"number\", \"adds\"), remove = F)\n\nTym sposobem uzyskaliśmy zbiór 267 imion papieży, a w osobnych kolumnach samo imię, numer oraz dodatkowe przydomki (trzech papieży miało przydomek Wielki). Na podstawie tak przygotowanego zbioru możemy przedstawić najczęściej występujące imiona papieży.\n\nnamesFreq &lt;- popes_names_clean %&gt;%\n  count(name)\n\nfilter(namesFreq, n &gt; 4) %&gt;%\n  ggplot(., aes(x=reorder(name, -n), y=n)) + \n  geom_col() + \n  coord_flip() +\n  xlab(\"Wybrane imię\") + ylab(\"\") +\n  theme_light()\n\n\n\n\nNajczęściej wybierane imię to Jan - 21 razy (po Janie XV był od razu Jan XVII, a po Janie XIX, Jan XXI), na drugim miejscu Benedykt (17 razy) wyprzedzający Grzegorza (16 razy). Za podium znajduje się Klemens, Innocenty, Leon, Pius i Stefan. Imion występujących tylko raz w ciągu ostatnich 2000 lat było 44, a wśród nich m.in. Anaklet, Eleuteriusz, Hormizdas czy Zefiryn - można się inspirować.\nKod w jednym kawałku dostępny jest na githubie."
  },
  {
    "objectID": "posts/2018-04-12-analiza-imion/analiza-imion.html",
    "href": "posts/2018-04-12-analiza-imion/analiza-imion.html",
    "title": "Analiza podobieństwa imion",
    "section": "",
    "text": "Przetwarzając dane tekstowe można zetknąć się z problemem duplikatów przybliżonych. Każdemu na pewno przydarzyło się zrobić literówkę wprowadzając dane do jakiegoś formularza. Formularz może zawierać także pytanie o dane adresowe i na przykład ulicę 3 maja można zapisać na wiele sposobów:\n\n3 maja\ntrzeciego maja\n3-go maja\n\nPodobnych przykładów można mnożyć, niemniej rezultat jest zawsze ten sam - jedna informacja i wiele jej wariantów.\n\nMiary podobieństwa\nW takiej sytuacji można skorzystać z pewnych miar podobieństwa tekstów (ang. fuzzy matching), które w R oprogramowane są w pakiecie stringdist.\nNajczęściej wykorzystywane algorytmy to:\n\nodległość Levenshteina - odległość to liczba kroków wymaganych do przekształcenia jednego ciągu znaków w drugi. Działania jakie można wykonywać to wstawienie nowego znaku, usunięcie znaku lub zamianę znaku na nowy znak.\nodległość Damerau-Levenshteina - podobnie jak odległość Levenshteina, ale wprowadza działanie polegające na zamianie miejscami sąsiadujących znaków.\nnajdłuższy wspólny podłańcuch (ang. longest common substring) - znajduje najdłuższy ciąg elementów leżących obok siebie w obu ciągach, natomiast odległość to liczba niesparowanych znaków.\nodległość Jaro - średnia z odsetka wspólnych znaków w pierwszym ciągu, odsetka wspólnych znaków w drugim ciągu oraz odsetka wspólnych znaków nie wymagających transpozycji. Przyjmuje wartości od 0 do 1, gdzie 0 to teksty identyczne, a 1 zupełnie różne.\nodległość Jaro-Winklera - zwiększenie podobieństwa ciągów, które według odległości Jaro są do siebie podobne.\n\nNa podstawie wyżej opisanych algorytmów można stwierdzić, że im mniejsza wartość danej odległości tym ciągi tekstowe będą do siebie bardziej podobne. Pakiet stringdist zawiera implementację jeszcze kilku miar, niemniej w tym poście skupiłem się na tych najbardziej popularnych.\n\n\nAnaliza imion męskich w 2015 roku\nZ racji tego, że nie dysponuję bazą zawierającą takie nieuporządkowane dane tekstowe to wykorzystamy statystyki dotyczące najczęściej nadawanych imion dzieciom dostępne na stronach Ministerstwa Cyfryzacji. Wiemy przecież, że niektóre imiona występują w kilku wariantach np. Angelika i Andżelika, więc spróbujemy je znaleźć.\nW pierwszej kolejności wczytuję pakiety oraz dane. Pakiet chorddiag przyda nam się do wizualizacji uzyskanych wyników na wykresie strunowym.\n\n# devtools::install_github(\"mattflor/chorddiag\")\n\nlibrary(tidyverse) # przetwarzanie\nlibrary(stringdist) # odległości pomiędzy tekstami\nlibrary(chorddiag) # wykres strunowy\nlibrary(grDevices) # kolory\nlibrary(RColorBrewer) # kolory\n\nkolory &lt;- grDevices::colorRampPalette(RColorBrewer::brewer.pal(9, \"Set3\"))\n\nload(\"imiona.RData\")\n\nLiczba nadawanych unikalnych imion z roku na rok prawie się podwaja, co świadczy o coraz większej wyobraźni rodziców i chęci wyróżnienia dziecka już na starcie.\n\nimiona %&gt;%\n  group_by(plec, rok) %&gt;%\n  count() %&gt;%\n  ungroup() %&gt;%\n  mutate(plec=factor(plec, labels=c(\"Kobiety\", \"Mężczyźni\")),\n         rok=as.factor(rok)) %&gt;%\n  ggplot(., aes(x=plec, y=n, fill=rok)) + \n  geom_bar(position = position_dodge(), stat = \"identity\") +\n  theme_light() +\n  xlab(\"Płeć\") + ylab(\"Liczba unikalnych imion\") +\n  theme(legend.position = \"bottom\") + \n  guides(fill=guide_legend(title=\"Rok\"))\n\n\n\n\nZ tego względu analizie poddamy imiona męskie nadane w 2015 roku. Mniejszy zbiór danych umożliwi wizualizację w czytelniejszy sposób.\n\nd &lt;- imiona %&gt;%\n  filter(rok == 2015, plec == \"m\")\n\nPoniżej znajduje się gotowa funkcja, która zwraca zbiór z parami porównywanych tekstów i odległościami pomiędzy nimi oraz macierz, którą można przedstawić na wykresie strunowym.\nFunkcja przyjmuje trzy argumenty:\n\ndane - wektor tekstów, które zostaną porównane ze sobą parami,\nmetoda - identyfikator metody z pakietu stringdist,\ngranica - przyjęty poziom podobieństwa.\n\nRezultatem działania funkcji są dwa obiekty:\n\nzbior - zawiera pary porównywalnych tekstów oraz wybraną odległość pomiędzy nimi,\nmacierz - macierz zawierająca wartości poniżej przyjętej granicy.\n\n\npodobne_teksty &lt;- function(dane, metoda, granica){\n  \n  # policzenie odległości dla wszystkich par\n  \n  m &lt;- as.matrix(stringdistmatrix(a = dane, b = dane, method = metoda))\n  \n  rownames(m) &lt;- dane\n  colnames(m) &lt;- dane\n  \n  # wybór obserwacji z odległością poniżej określonej granicy\n  \n  m_g &lt;- as.data.frame(which(m &lt;= granica, arr.ind=TRUE)) %&gt;%\n    mutate(tekst1=rownames(.)) %&gt;%\n    filter(!row==col) %&gt;%\n    mutate(tekst2=colnames(m)[col])\n  \n  d &lt;- numeric(nrow(m_g))\n  \n  for(i in 1:length(d)){\n    d[i] &lt;- m[m_g$row[i],m_g$col[i]]\n  }\n  \n  m_g$d &lt;- d\n  \n  # usunięcie duplikatów\n  \n  m_g_bd &lt;- as.data.frame(unique(t(apply(m_g[,1:2], 1, sort))))\n  names(m_g_bd) &lt;- c(\"row\", \"col\")\n  \n  m_g_bd_tekst &lt;- inner_join(m_g_bd, m_g, by=c(\"row\"=\"row\", \"col\"=\"col\"))\n  \n  # utworzenie macierzy do funkcji chorddiag\n  \n  # wybór unikalnych numerów kolumn i wierszy\n  \n  rc &lt;- unique(c(m_g_bd_tekst$row, m_g_bd_tekst$col))\n  \n  m_g_tekst &lt;- m[rc, rc]\n  \n  # zamiana odległości większej niż podana granica na 0\n  \n  m_g_tekst0 &lt;- m_g_tekst\n  \n  m_g_tekst0[m_g_tekst0 &gt; granica] &lt;- 0\n  \n  wynik &lt;- list(zbior=m_g_bd_tekst[-c(1,2)],\n                macierz=m_g_tekst0)\n  \n  return(wynik)\n  \n}\n\nW pierwszej kolejności zobaczmy jak z podobieństwem imion poradzi sobie odległość Damerau-Levenshteina. Jako granicę przyjąłem 2, co oznacza, że dopuszczam maksymalnie dwie modyfikacje pierwszego ciągu w celu otrzymania ciągu drugiego.\n\ndl &lt;- podobne_teksty(dane = d$imie, metoda = \"osa\", granica = 2)\n\ndl_df &lt;- dl$zbior\ndl_m &lt;- dl$macierz\n\nhead(dl_df) %&gt;%\n  knitr::kable()\n\n\n\n\ntekst1\ntekst2\nd\n\n\n\n\nKASPER.1\nKACPER\n1\n\n\nJONATHAN.1\nJONATAN\n1\n\n\nJONATHAN.2\nNATHAN\n2\n\n\nJANUSZ.1\nJONASZ\n2\n\n\nOSCAR.1\nOSKAR\n1\n\n\nAMADEUSZ.1\nMATEUSZ\n2\n\n\n\n\n\nNastępnie sprawdzam, które imię cechuje się największą liczbą sąsiadów i jak oni wyglądają. W przypadku tej odległości jest to imię ALAN, które ma 10 bliskich sąsiadów. Oznacza to, że zmieniając w imieniu ALAN maksymalnie dwa znaki można uzyskać 10 różnych imion.\n\nliczebnosc &lt;- colSums(dl_m != 0)\n\ntekst_max &lt;- liczebnosc[which(liczebnosc==max(liczebnosc))]\ntekst_max\n\nALAN \n  10 \n\ndl_df %&gt;%\n  filter(tekst1 %in% names(tekst_max) | tekst2 %in% names(tekst_max)) %&gt;%\n  knitr::kable()\n\n\n\n\ntekst1\ntekst2\nd\n\n\n\n\nALEK.4\nALAN\n2\n\n\nARON.2\nALAN\n2\n\n\nMILAN.1\nALAN\n2\n\n\nALEX.3\nALAN\n2\n\n\n\n\n\nDo przedstawienia uzyskanych relacji najlepszy będzie wykres strunowy (ang. chord diagram). W pakiecie chorddiag znajduje się eRowa implementacja tego wykresu pochodząca z biblioteki d3.js. Większa wersja tego wykresu znajduje się tutaj.\n\nsort &lt;- order(liczebnosc)\n\nchorddiag(dl_m[sort, sort], \n          margin = 60, \n          palette = \"Set3\", \n          showTicks = F,\n          groupPadding = 2,\n          groupnameFontsize = 10,\n          groupnamePadding = 5,\n          groupThickness = .05,\n          showZeroTooltips = F,\n          chordedgeColor = \"gray90\",\n          groupColors = kolory(nrow(dl_m)))\n\n\n\n\n\nZobaczmy jak zadziała metoda najdłuższego wspólnego podłańcucha. Tutaj granicę ustalam na poziomie 3 niepasujących liter. Imiona JAN oraz BRIAN cechują się występowaniem aż 8 najbliższych sąsiadów.\n\nlcs &lt;- podobne_teksty(dane = d$imie, metoda = \"lcs\", granica = 3)\n\nlcs_df &lt;- lcs$zbior\nlcs_m &lt;- lcs$macierz\n\nliczebnosc &lt;- colSums(lcs_m != 0)\n\ntekst_max &lt;- liczebnosc[which(liczebnosc==max(liczebnosc))]\ntekst_max\n\nBRIAN   JAN \n    8     8 \n\nlcs_df %&gt;%\n  filter(tekst1 %in% names(tekst_max) | tekst2 %in% names(tekst_max)) %&gt;%\n  knitr::kable()\n\n\n\n\ntekst1\ntekst2\nd\n\n\n\n\nJANUSZ.1\nJAN\n3\n\n\nADRIAN.1\nBRIAN\n3\n\n\nARON.3\nJAN\n3\n\n\nBRAJAN.3\nJAN\n3\n\n\nJULIAN.1\nJAN\n3\n\n\nALAN.3\nJAN\n3\n\n\n\n\n\nWyniki także przedstawione są na wykresie strunowym (większa wersja).\n\nsort &lt;- order(liczebnosc)\n\nchorddiag(lcs_m[sort, sort], \n          margin = 60, \n          palette = \"Set3\", \n          tickInterval = 10, \n          showTicks = F,\n          groupPadding = 1,\n          groupnameFontsize = 10,      \n          groupnamePadding = 5,\n          groupThickness = .05,\n          chordedgeColor = \"gray90\",\n          groupColors = kolory(nrow(lcs_m)))\n\n\n\n\n\nW przypadku odległości Jaro jako próg klasyfikacji sąsiada przyjąłem wartość 0.2. W tym przypadku mamy aż trzy najliczniejsze imiona - BRIAN, NATHAN oraz NATAN mają po pięciu sąsiadów.\n\njd &lt;- podobne_teksty(dane = d$imie, metoda = \"jw\", granica = 0.2)\n\njd_df &lt;- jd$zbior\njd_m &lt;- jd$macierz\n\nliczebnosc &lt;- colSums(jd_m != 0)\n\ntekst_max &lt;- liczebnosc[which(liczebnosc==max(liczebnosc))]\ntekst_max\n\n BRIAN NATHAN  NATAN \n     5      5      5 \n\njd_df %&gt;%\n  filter(tekst1 %in% names(tekst_max) | tekst2 %in% names(tekst_max)) %&gt;%\n  knitr::kable()\n\n\n\n\ntekst1\ntekst2\nd\n\n\n\n\nJONATHAN.2\nNATHAN\n0.0833333\n\n\nJONATHAN.4\nNATAN\n0.1250000\n\n\nADRIAN.1\nBRIAN\n0.1777778\n\n\nJONATAN.2\nNATHAN\n0.1507937\n\n\nJONATAN.3\nNATAN\n0.0952381\n\n\nNATHAN.5\nNATAN\n0.0555556\n\n\nNATHANIEL.4\nNATAN\n0.1481481\n\n\nNATANIEL.3\nNATAN\n0.1250000\n\n\n\n\n\nUzyskane wyniki przedstawione na wykresie strunowym (większa wersja).\n\nsort &lt;- order(liczebnosc)\n\nchorddiag(jd_m[sort, sort], \n          margin = 60, \n          palette = \"Set3\", \n          tickInterval = 10, \n          showTicks = F,\n          groupPadding = 1,\n          groupnameFontsize = 10,       \n          groupnamePadding = 5,\n          groupThickness = .05,\n          chordedgeColor = \"gray90\",\n          groupColors = kolory(nrow(jd_m)))\n\n\n\n\n\n\n\nAnaliza imion żeńskich w 2017 roku\nMożna jeszcze pokusić się o analizę dla roku 2017. Wybierając imiona kobiece najwięcej sąsiadów z wykorzystaniem odległości Jaro i progu podobieństwa 0.2 uzyskamy dla imienia MARINA - 45.\n\nd &lt;- imiona %&gt;%\n  filter(rok == 2017, plec == \"k\")\n\njd &lt;- podobne_teksty(dane = d$imie, metoda = \"jw\", granica = 0.2)\n\njd_df &lt;- jd$zbior\njd_m &lt;- jd$macierz\n\nliczebnosc &lt;- colSums(jd_m != 0)\n\ntekst_max &lt;- liczebnosc[which(liczebnosc==max(liczebnosc))]\ntekst_max\n\nMARINA \n    45 \n\njd_df %&gt;%\n  filter(tekst1 %in% names(tekst_max) | tekst2 %in% names(tekst_max)) %&gt;%\n  head(n=12) %&gt;%\n  knitr::kable()\n\n\n\n\ntekst1\ntekst2\nd\n\n\n\n\nMARIETTA.8\nMARINA\n0.1805556\n\n\nMALWINA.7\nMARINA\n0.1507937\n\n\nKARINA.9\nMARINA\n0.1111111\n\n\nROMINA.5\nMARINA\n0.1777778\n\n\nMARIKA.11\nMARINA\n0.1111111\n\n\nALINA.17\nMARINA\n0.1777778\n\n\nMARIAME.8\nMARINA\n0.1507937\n\n\nMARIE.6\nMARINA\n0.1777778\n\n\nMARIJA.11\nMARINA\n0.1111111\n\n\nAMIRA.7\nMARINA\n0.1888889\n\n\nMARZENA.3\nMARINA\n0.1507937\n\n\nAMNA.5\nMARINA\n0.1944444\n\n\n\n\n\nMacierz odległości dla podobnych imion męskich w 2015 roku miała rozmiar 124 x 124, natomiast imiona kobiece w 2017 roku znajdują się w macierzy wielkości 686 na 686. W związku z tym przedstawienie tych danych na wykresie strunowym czyni ten wykres mało czytelnym. Wobec tego pozostaje analiza wyłącznie w oparciu o dane liczbowe.\n\n\n\nPodsumowanie\nWykorzystane miary odległości pomiędzy tekstami różnią się między sobą jeśli chodzi o główną ideę, stąd trudno wskazać najlepszą z nich. Każda pozwala na identyfikację najbardziej zbliżonych tekstów, ale ze względu na różne kryteria. W praktyce warto zastosować kilka miar i ocenić zaproponowane obserwacje.\nZ zastosowaniem opisanych metryk można analizować duże zbiory danych, niemniej w takim przypadku wizualizacja takich danych z wykorzystaniem wykresu strunowego staje się problematyczna i należałoby się skupić wyłącznie na analizie liczbowej bądź poszukać alternatywnych sposobów wizualizacji takich danych.\n\n\nBibliografia\n\nData Steve - d3/R Chord Diagram of White House Petition Data\nKrzysztof Jankiewicz - Hurtownie Danych, ETL – wybrane zagadnienia\n\nKod i dane dostępne są także na githubie"
  },
  {
    "objectID": "posts/2017-12-28-google-forms-kod/google-forms-kod.html",
    "href": "posts/2017-12-28-google-forms-kod/google-forms-kod.html",
    "title": "Tworzenie ankiet Google z poziomu kodu",
    "section": "",
    "text": "Google udostępnia bardzo przyjazne i darmowe narzędzie do tworzenia ankiet: Google Forms. Dostępnych jest bardzo wiele typów pytań, które tworzone są w wygodnym edytorze. Istnieje jednak także inny sposób tworzenia ankiet - z wykorzystaniem kodu javascript.\nAby rozpocząć przygodę z kodowaniem ankiet (lub innych produktów Google) należy zapoznać się z dokumentacją. Z kolei napisane programy wykonujemy w edytorze skryptów.\n\nProsta ankieta\nNa początku utworzymy prostą ankietę składającą się z dwóch pytań - o płeć i ulubiony słodycz.\n\nfunction stworzAnkiete() {\n  \n  var form = FormApp.create('Ankieta');\n  \n  form.addMultipleChoiceItem()\n  .setTitle('Płeć')\n  .setChoiceValues(['Kobieta', 'Mężczyzna'])\n  .setRequired(true);\n  \n  form.addCheckboxItem()\n  .setTitle('Ulubiony słodycz')\n  .setChoiceValues(['Czekolada', 'Żelki', 'Batony', 'Owoce'])\n  .showOtherOption(true);\n  \n}\nW pierwszym kroku tworzymy zmienną (w tym przypadku o nazwie form), która jest odpowiedzialna na utworzenie ankiety, a do której będziemy dodawać kolejne elementy. Jako argument funkcji podajemy nazwę ankiety.\nNastępnie na obiekcie form wywołujemy funkcje dodające kolejne pytania. Funkcja addMultipleChoiceItem() to pytanie jednokrotnego wyboru. Ustalamy nazwę pytania oraz warianty odpowiedzi. Za pomocą funkcji setRequired() decydujemy czy odpowiedź na to pytanie ma być obowiązkowa.\nZ kolei funkcja addCheckboxItem() dodaje pytanie wielokrotnego wyboru. Tytuł i warianty odpowiedzi definiuje się tak samo, jak w poprzednim przypadku, ale dodatkowo mamy możliwość włączenia odpowiedzi inne za pomocą funkcji showOtherOption(true).\nW przypadku niektórych pytań niezbędna jest walidacja wprowadzonej przez respondenta wartości. Dodajmy do naszej ankiety pytanie o wzrost.\nfunction stworzAnkiete() {\n  \n  var form = FormApp.create('Ankieta');\n  \n  form.addMultipleChoiceItem()\n  .setTitle('Płeć')\n  .setChoiceValues(['Kobieta', 'Mężczyzna'])\n  .setRequired(true);\n  \n  form.addCheckboxItem()\n  .setTitle('Ulubiony słodycz')\n  .setChoiceValues(['Czekolada', 'Żelki', 'Batony', 'Owoce'])\n  .showOtherOption(true);\n  \n  var wzrost = form.addTextItem().setTitle('Wzrost (w cm)');\n  \n  var wzrostWalidacja = FormApp.createTextValidation()\n  .setHelpText('Wzrost musi być liczbą z przedziału 120 cm a 220 cm')\n  .requireNumberBetween(120,220)\n  .build();\n  \n  wzrost.setValidation(wzrostWalidacja);\n  \n}\nWalidacja pytania o wzrost wiąże się z koniecznością przypisania samego pytania do zmiennej. Tworzona jest zmienna wzrost, która jest pytaniem typu tekstowego (addTextItem()), w związku z czym ankietowany mógłby wpisać tam dowolną wartość.\nOgraniczenie tego typu praktyk jest możliwe dzięki utworzeniu nowego obiektu (np. o nazwie wzrostWalidacja) zawierającego kryteria walidacji. Możemy określić tekstową podpowiedź dotyczącą wartości (setHelpText), a także zdefiniować jakie wartości są dopuszczalne. W tym przypadku wzrost musi pochodzić z przedziału od 120 do 220 cm za co odpowiada funkcja requireNumberBetween(120,220). Definiowanie kryteriów walidacji musi być zakończone funkcją build().\nUtworzone zmienne wzrost i wzrostWalidacja nie są póki co ze sobą w żaden sposób powiązane. Aby zdefiniowana przez nas walidacja została zastosowana to na obiekcie wzrost musimy wywołać funkcję setValidation(wzrostWalidacja) z argumentem zawierającym kryteria walidacji. Dzięki tak określonej składni możliwe jest przypisanie jednego sposobu walidacji do wielu pytań w ankiecie.\n\n\nPytania filtrujące\nGoogle Forms umożliwia także tworzenie ścieżek w ankiecie w zależności od zaznaczonej przez respondenta odpowiedzi. Zmodyfikujemy zatem wcześniej omówioną ankietę w taki sposób, aby płeć stanowiła pytanie filtrujące. Jeśli ankietowany wskaże, że jest mężczyzną to będzie musiał odpowiedzieć na pytanie o ulubiony słodycz, natomiast kobiety zobaczą pytanie o wzrost.\nfunction stworzAnkiete2() {\n\n  var form = FormApp.create('Ankieta 2');\n    \n  var plec = form.addMultipleChoiceItem();\n  \n  var m = form.addPageBreakItem().setTitle('Mężczyzna');\n  \n  form.addCheckboxItem()\n  .setTitle('Ulubiony słodycz')\n  .setChoiceValues(['Czekolada', 'Żelki', 'Batony', 'Owoce'])\n  .showOtherOption(true);\n  \n  var k = form.addPageBreakItem().setTitle('Kobieta');\n  \n  var wzrost = form.addTextItem().setTitle('Wzrost (w cm)');\n  \n  var wzrostWalidacja = FormApp.createTextValidation()\n  .setHelpText('Wzrost musi być liczbą z przedziału 120 cm a 220 cm')\n  .requireNumberBetween(120,220)\n  .build();\n  \n  wzrost.setValidation(wzrostWalidacja);\n  \n  plec.setTitle('Płeć')\n  .setChoices([\n    plec.createChoice('Kobieta', k), \n    plec.createChoice('Mężczyzna', m)])\n  .setRequired(true)\n   \n}\nTo co od razu rzuca się w oczy to dużo większa liczba zmiennych. Przede wszystkim pytanie filtrujące musi być zmienną, stąd jest przypisane do zmiennej o nazwie plec. Dodatkowo dodaliśmy zmienne (m i k), które przechowują podział sekcji (addPageBreakItem()) i mają odpowiednio nadane nazwy nowych stron. Pytania, które mają być zadane respondentowi po przejściu pytania filtrującego muszą się znaleźć poniżej podziału sekcji.\nSposób zachowywania się ankiety określamy na samym końcu kodu. Na obiekcie zawierającym pytanie filtrujące wywołujemy funkcję setChoices (wcześniej używaliśmy setChoicesValues). Jako argumenty tej funkcji definiujemy pary: odpowiedź w pytaniu filtrującym i zmienna podziału sekcji do której respondent ma być przekierowany.\n\n\nTworzenie podobnych pytań w funkcji\nCzasem w ankiecie pojawia się wiele podobnych pytań np. w ankiecie mającej na celu ocenę przedmiotów na studiach będą zmieniały się tylko nazwy przedmiotów bądź prowadzący. Można oczywiście skopiować dane pytanie n razy, ale w przypadku, gdy będziemy chcieli coś zmienić będzie się to wiązało ze zmianą w n przypadkach.\nUtworzenie własnej funkcji, która będzie tworzyć pytanie zaoszczędzi nam kłopotów w przypadku jakichkolwiek zmian oraz zmniejszy liczbę linii kodu.\n\nfunction main() {\n  \n  function zrozumialosc(przedmiot) {\n  \n    var p = przedmiot + ' - zrozumiałość treści'\n  \n    form.addMultipleChoiceItem()\n    .setTitle(p)\n    .setChoiceValues(['Bardzo zrozumiałe', \n                      'Zrozumiałe', \n                      'Nie zrozumiałe', \n                      'Bardzo nie zrozumiałe']);\n  \n  }\n  \n  var form = FormApp.create('Badanie jakości kształcenia');\n  \n  zrozumialosc('Przedmiot A');\n  zrozumialosc('Przedmiot B');\n  zrozumialosc('Przedmiot C');\n  \n}\n\nW powyższym kodzie funkcja zrozumialosc zawiera pytanie jednokrotnego wyboru o zrozumiałość treści danego przedmiotu, który deklaruje się jako argument funkcji. Żeby utworzyć takie pytanie wywołujemy funkcję zrozumialosc podając w nawiasie nazwę przedmiotu. Dzięki takiemu rozwiązaniu dodanie kolejnego wariantu odpowiedzi wiąże się ze zmianą tylko w jednym miejscu - funkcji zawierającej deklarację pytania.\nFunkcje, w których deklarujemy pytania w ankiecie mogą być dużo bardziej rozbudowane i mogą zawierać więcej argumentów niż tylko jeden. Wszystko zależy od aktualnych potrzeb.\n\n\nPodsumowanie\nTworzenie prostych ankiet z poziomu kodu raczej nie ma większego sensu, ale zastosowanie tego narzędzia w przypadku złożonych kwestionariuszy składających się z podobnych pytań może zdecydowanie ułatwić nam pracę."
  },
  {
    "objectID": "posts/2020-01-12-domy-hogwartu/domy-hogwartu.html",
    "href": "posts/2020-01-12-domy-hogwartu/domy-hogwartu.html",
    "title": "Domy Hogwartu - częstość występowania w książkach",
    "section": "",
    "text": "Do dziś pamiętam dzień, w którym dostałem do ręki pierwsze książki o przygodach Harrego Pottera. Wiele lat później zainteresowanie serią powróciło, ale tym razem od strony statystycznej. Czy Gryffindor jako dom głównego bohatera jest najczęściej wymienianym domem w książkach sagi? Przekonajmy się.\n\nDane\nWyszukując w Google frazę “harry potter txt” znalazłem wiele stron z tekstową wersją książki. Ostatecznie dane do analizy pobrałem stąd. Posługiwanie się angielską wersją książki będzie prostsze z punktu widzenia analizy, ponieważ nie trzeba będzie rozważać wszystkich odmian i przypadków nazwy domu.\nW celu wydobycia nazw domów z tekstu wszystkich książek napisałem funkcję, która w całkiem elegancki sposób się tym zajmuje i może także posłużyć w innych analizach tekstu.\n\nlibrary(tidyverse)\nlibrary(tidytext)\n\nwords_in_book &lt;- function(book, selected_words){\n  \n  hp &lt;- read_lines(book) # wczytanie tekstu\n  \n  df &lt;- data.frame(org_line=as.character(hp)) %&gt;% \n    filter(str_length(org_line) &gt; 0) %&gt;% # usunięcie pustych wierszy\n    mutate(book_name = book %&gt;% \n             str_replace_all(\"txt/\", \"\") %&gt;% # usunięcie ścieżki z nazwy książki\n             str_replace_all(\".txt\", \"\"), # usunięcie rozszerzenia z nazwy książki\n           line = org_line %&gt;% \n             str_replace_all(\"[^[:alnum:] ]\", \" \")) %&gt;% # usunięcie znaków niealfanumerycznych\n    unnest_tokens(output = words, input = line) # rodzielenie tekstu książki na pojedyńcze wyrazy\n  \n  houses &lt;- df %&gt;% \n    mutate(house=str_match(words, selected_words)) %&gt;% # identyfikacja szukanych słów \n    filter(!is.na(house)) # usunięcie wszystkich pozostałych\n  \n  return(houses)\n  \n}\n\nNastępnie deklaruję jakie pliki chcę przeanalizować oraz jakie słowa mnie interesują.\n\nbooks &lt;- str_c(\"txt/\", list.files(\"txt\"))\n\nhp_houses &lt;- map_df(books, words_in_book, \n                    selected_words = \"gryffindor|hufflepuff|ravenclaw|slytherin\")\n\nWykorzystuję tutaj funkcję map_df z pakietu purrr, który jest wspaniałym narzędziem w sytuacji, w której nie chcemy bawić się w pętle. Pokazany powyżej zapis argumentu selected_words powoduje dopasowanie także liczby mnogiej nazwy domu. W rezultacie powstał zbiór zawierający 1206 obserwacji oraz 4 kolumny - z fragmentem książki, tytułem książki, pasującym słowem oraz nazwą domu w Hogwarcie.\n\n\nCzęstości\nDokonujemy jeszcze drobnych przekształceń w tekście:\n\nhp_houses &lt;- hp_houses %&gt;% \n  mutate(book=str_replace(book_name, \" - \", \" \\n\"), # tytuł książki w dwóch wierszach\n         house=str_c(str_to_upper(str_sub(house,1,1)), \n                     str_sub(house,2,str_length(house)))) # nazwa domu z wielkiej litery\n\nNastępnie zliczamy wystąpienia danego domu w ramach książki.\n\nhp_houses %&gt;% \n  group_by(book, house) %&gt;% \n  count() %&gt;% \n  ungroup() %&gt;% \n  mutate(book_fct=factor(book, levels = unique(book), ordered = T),\n         house_fct=factor(house, levels = unique(house), ordered = T)) %&gt;% \n  ggplot(aes(x = fct_rev(book_fct), y = n, fill = fct_rev(house))) +\n    geom_col(position = \"dodge\") +\n    geom_text(aes(label = n), position = position_dodge(0.95), hjust = -0.2) +\n    xlab(\"\") +\n    ylab(\"Count\") +\n    ylim(0,130) +\n    scale_fill_manual(values = rev(c(\"#670001\", \"#FF9D0A\", \"#002E5F\", \"#2E751C\")), name = \"\") +\n    coord_flip() +\n    labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n    theme_bw() +\n    theme(legend.position = \"top\", \n          plot.caption = element_text(color = \"grey80\")) +\n    guides(fill = guide_legend(reverse = TRUE)) + \n    ggtitle(\"Occurence of house name in Harry Potter books\")\n\n\n\n\nWe wszystkich częściach sagi oprócz drugiej (Komnata Tajemnic) zgodnie z oczekiwaniami dominuje Gryffindor. Slytherin zwykle jest na drugim miejscu. Wyjątek stanowi wspomniana część druga (pierwsze miejsce) oraz siódma, gdzie zajmuje trzecie miejsce przegrywając jeszcze z Ravenclawem.\nProcentowo wygląda to następująco:\n\nhp_houses %&gt;% \n  group_by(book) %&gt;%\n  count(house) %&gt;% \n  mutate(percent=n/sum(n),\n         percent_label=round_preserve_sum(percent*100)) %&gt;% \n  ungroup() %&gt;% \n  mutate(book_fct=factor(book, levels = unique(book), ordered = T),\n         house_fct=factor(house, levels = unique(house), ordered = T)) %&gt;% \n  ggplot(aes(x = fct_rev(book_fct), y = percent, fill = fct_rev(house))) +\n    geom_col() + \n    geom_text(aes(label = percent_label), position = position_stack(0.5), color = \"white\") +\n    xlab(\"\") +\n    ylab(\"Percentage\") +\n    scale_fill_manual(values = rev(c(\"#670001\", \"#FF9D0A\", \"#002E5F\", \"#2E751C\")), name = \"\") +\n    scale_y_continuous(labels=scales::percent) +\n    coord_flip() +\n    labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n    theme_bw() +\n    theme(legend.position = \"top\", \n          plot.caption = element_text(color = \"grey80\")) +\n    guides(fill = guide_legend(reverse = TRUE)) + \n    ggtitle(\"Occurence of house name in Harry Potter books (in %)\")\n\n\n\n\nNajwięcej Gryffindoru było w trzeciej części przygód Harrego Pottera, Hufflepuffu w czwartej, Ravenclawu w siódmej, a Slytherinu w drugiej części.\nDodatkowo możemy wyznaczyć wskaźnik liczby wzmianek o domach w odniesieniu do liczby stron książki danej części HP. Liczbę stron znalazłem na tej stronie i bezpośrednio dodałem do istniejących danych.\n\npages &lt;- hp_houses %&gt;% \n  distinct(book) %&gt;% \n  mutate(pages=c(223, 251, 317, 636, 766, 607, 607))\n\nhp_houses %&gt;% \n  count(book) %&gt;% \n  inner_join(., pages, by = \"book\") %&gt;% \n  mutate(house_per_page=round(n/pages,2),\n         book_fct=factor(book, levels = unique(book), ordered = T)) %&gt;% \n  ggplot(aes(x = fct_rev(book_fct), y = house_per_page)) +\n    geom_col(fill = \"#FFC001\") +\n    geom_text(aes(label = house_per_page), hjust = -0.2) +\n    coord_flip() +\n    xlab(\"\") +\n    ylab(\"House per page\") +\n    ylim(0,1.05) +\n    labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n    theme_bw() +\n    theme(plot.caption = element_text(color = \"grey80\")) +\n    ggtitle(\"House name mentioned per page in Harry Potter books\")\n\n\n\n\nW pierwszych dwóch częściach nazwa domu padała prawie raz na stronę. W Więźniu z Azkabanu średnio na co drugiej stronie znajdowało się odwołanie do nazwy domu. Z kolei w ostatnich czterech częściach wartość ta spadła do około 1/4 zapewne ze względu na przeniesienie akcji poza Hogwart.\nWszystkie kody są na githubie."
  },
  {
    "objectID": "posts/2020-06-26-moodle-pandemia/moodle-pandemia.html",
    "href": "posts/2020-06-26-moodle-pandemia/moodle-pandemia.html",
    "title": "Dydaktyka w czasie pandemii - analiza logów z systemu moodle",
    "section": "",
    "text": "W semestrze letnim roku akademickiego 2019/2020 miałem okazję prowadzić przedmiot Statystyka na kierunku Rachunkowość i Finanse Przedsiębiorstw. Pandemia COVID19 spowodowała, że większość nauczycieli musiała z dnia na dzień zmienić sposób prowadzenia zajęć ze stacjonarnego na zdalny. Trzeba było szybko przestawić się na nowy tryb nauki i znaleźć optymalny sposób prowadzenia zajęć oraz metodę weryfikacji umiejętności studentów. W moim przypadku zdecydowałem się na nagrywanie screencastów - widok ekranu komputera z komentarzem głosowym. Przy okazji doskonaliłem skrypt ze statystyki. Elementem weryfikacji wiedzy były quizy stworzone za pomocą świetnego pakietu exams. Dodatkowo na zakończenie semestru studenci mieli przygotować prosty projekt dotyczący regresji liniowej.\nWszystkie wyżej wymienione elementy umieszczałem na platformie moodle. System ten zbiera informacje o wszystkich aktywnościach użytkowników i analiza tych aktywności będzie tematem tego wpisu.\n\nPrzygotowanie danych\nRaport aktywności z systemu moodle zawiera następujące elementy:\n\nstempel czasu,\ntwórcę aktywności,\nkogo ta aktywność dotyczyła,\nkontekst zdarzenia,\nskładnik,\nnazwa zdarzenia,\nopis,\npochodzenia (strona internetowa lub inne),\nadres IP.\n\nW ciągu semestru 83 studentów oraz ja wygenerowaliśmy 49264 różnych aktywności. Oryginalny zbiór danych zawierał dane osobowe, zatem przeprowadziłem anomizację i w miejsce imion i nazwisk wstawiłem losowe ciągi znaków. Zostawiłem tylko swoje inicjały, żeby odróżnić aktywności nauczyciela od aktywności studentów.\n\n\nAnaliza częstości\nDo analizy przydadzą się trzy pakiety: tidyverse - do przetwarzania i wizualizacji, lubridate - do operacji na datach oraz ggTimeSeries - do wizualizacji kalendarza. Na pierwszy ogień weźmy najczęściej wyświetlany typ składnika kursu.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggTimeSeries)\n\nload(\"logs_anonim.RData\")\n\nlogs_anonim %&gt;% \n  filter(nazwa_anonim != \"ŁW\") %&gt;% \n  count(skladnik) %&gt;% \n  arrange(n) %&gt;% \n  filter(n &gt; 1000) %&gt;% \n  mutate(skladnik=fct_reorder(skladnik,n)) %&gt;% \n  ggplot(aes(x=skladnik, y=n)) + \n  geom_col(fill = \"#6daaee\") + \n  geom_text(aes(label = n), hjust = 1.1) +\n  xlab(\"Liczba wyświetleń\") +\n  ylab(\"Typ składnika kursu\") +\n  coord_flip() +\n  ggtitle(\"Typ składnika kursu wg liczby wyświetleń - powyżej 1000\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa pierwszym miejscu jest Test (Quiz), który studenci musieli rozwiązać, aby uzyskać punkty niezbędne do zaliczenia przedmiotu. System na drugim miejscu to wyświetlenie strony kursu. Adres URL zawierał link do skryptów oraz screencastów, natomiast Plik to plik Excela zawierający rozwiązanie omawianego problemu. Zadanie to z kolei zasób z projektem.\nSprawdźmy zatem jaki konkretny składnik kursu był najczęściej klikany.\n\nlogs_anonim %&gt;% \n  filter(nazwa_anonim != \"ŁW\") %&gt;% \n  filter(kontekst_zdarzenia != \"Kurs: Statystyka - stacj. I st. - RiFP - (lab.) - dr Łukasz Wawrowski\") %&gt;% \n  count(kontekst_zdarzenia) %&gt;% \n  mutate(kontekst_zdarzenia=gsub(\" - quiz\", \"\", kontekst_zdarzenia)) %&gt;% \n  filter(n &gt; 1000) %&gt;% \n  mutate(kontekst_zdarzenia=fct_reorder(kontekst_zdarzenia,n)) %&gt;% \n  ggplot(aes(x=kontekst_zdarzenia, y=n)) + \n  geom_col(fill = \"#6daaee\") + \n  geom_text(aes(label = n), hjust = 1.1) +\n  xlab(\"Liczba wyświetleń\") +\n  ylab(\"Składnik kursu\") +\n  coord_flip() +\n  ggtitle(\"Składniki kursu wg liczby wyświetleń - powyżej 1000\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNiespodzianek nie ma - najczęściej wyświetlane były quizy. Studenci najpierw odwiedzali ten zasób, żeby quiz rozwiązać, a po tygodniu, żeby sprawdzić poprawność odpowiedzi. Pierwsze 4 miejsca to 4 tygodnie zajęć od rozpoczęcia pandemii. Na piątym miejscu jest projekt zaliczeniowy.\nMateriały z jakiego tematu były najczęściej otwierane? Poniżej analiza częstości wyłącznie adresów URL.\n\nlogs_anonim %&gt;% \n  filter(nazwa_anonim != \"ŁW\") %&gt;% \n  filter(grepl(\"Adres URL:\", kontekst_zdarzenia)) %&gt;% \n  mutate(kontekst_zdarzenia=gsub(\"Adres URL: \", \"\", kontekst_zdarzenia)) %&gt;% \n  count(kontekst_zdarzenia) %&gt;% \n  mutate(kontekst_zdarzenia=fct_reorder(kontekst_zdarzenia,n)) %&gt;% \n  ggplot(aes(x=kontekst_zdarzenia, y=n)) + \n  geom_col(fill = \"#6daaee\") + \n  geom_text(aes(label = n), hjust = 1.1) +\n  xlab(\"Liczba wyświetleń\") +\n  ylab(\"Składnik kursu\") +\n  coord_flip() +\n  ggtitle(\"Adresy URL wg liczby wyświetleń\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa pierwszym miejscu link do konkretnego rozdziału skryptu poświęconemu miarom klasycznym. Jedna wyświetlenie mniej ma cały skrypt. Dwa następne miejsca to przedziały ufności. Kolejne pozycje pokazują, że studenci woleli oglądać screencast aniżeli czytać rozdział skryptu.\n\n\nAnaliza czasowa\nZajęcia w planie zajęć miałem zaplanowane na wtorki i zawsze starałem się tego dnia udostępnić materiały z nowego tematu. Niestety dwa razy mi się to nie udało - zajęcia zostały udostępnione w środę. Zobaczmy jak wyglądała aktywność studentów w tygodniu przed pandemią, kiedy zajęcia odbyły się stacjonarnie po raz ostatni - 9-15.03.2020.\n\nlogs_anonim %&gt;% \n  filter(czas &gt;= \"2020-03-09 00:00:00\", czas &lt;= \"2020-03-15 23:59:00\", nazwa_anonim != \"ŁW\") %&gt;% \n  count(czas) %&gt;% \n  ggplot(aes(x=czas, y=n)) + \n  geom_point(color = \"#6daaee\") +\n  ylab(\"Liczba akcji\") +\n  ylim(0,50) +\n  ggtitle(\"Liczba aktywności w ciągu tygodnia 9-15.03.2020\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nWidoczny jest pik aktywności we wtorek - w pozostałe dni pojedyncze logowania do kursu. Weźmy teraz pod uwagę wybrany tydzień w kwietniu.\n\nlogs_anonim %&gt;% \n  filter(czas &gt;= \"2020-04-20 00:00:00\", czas &lt;= \"2020-04-26 23:59:00\", nazwa_anonim != \"ŁW\") %&gt;% \n  count(czas) %&gt;% \n  ggplot(aes(x=czas, y=n)) +\n  geom_point(color = \"#6daaee\") +\n  ylab(\"Liczba akcji\") +\n  ylim(0,20) +\n  ggtitle(\"Liczba aktywności w ciągu tygodnia 20-26.04.2020\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nWidać wyraźnie, że aktywność studentów rozłożyła się na pozostałe dni tygodnia. Szczególnie na poniedziałek i wtorek, kiedy mijał termin rozwiązania quizu. Aktywność w całym semestrze widoczna jest poniżej.\n\nlogs_anonim %&gt;% \n  filter(godz &gt;= \"2020-03-09 01:00:00\", godz &lt;= \"2020-06-14 23:00:00\", nazwa_anonim != \"ŁW\") %&gt;% \n  count(czas) %&gt;% \n  ggplot(aes(x=czas, y=n)) + \n  geom_point(color = \"#6daaee\") +\n  ylab(\"Liczba akcji\") +\n  ylim(0,50) +\n  ggtitle(\"Liczba aktywności w czasie semestru\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa zakończenie wizualizacja tych danych w formie kalendarza.\n\nlogs_anonim %&gt;% \n  filter(nazwa_anonim != \"ŁW\", czas &gt;= \"2020-03-09 00:00:00\") %&gt;% \n  mutate(dzien=ymd(format(round(czas, units=\"days\"), format=\"%Y-%m-%d\"))) %&gt;% \n  count(dzien) %&gt;% \n  ggTimeSeries::ggplot_calendar_heatmap(dtDateValue = ., \n                                        cDateColumnName = \"dzien\", \n                                        cValueColumnName = \"n\") +\n  scale_fill_continuous(low = \"#ffffbf\", high = \"#d73027\", name = \"Liczba\\naktywności\") +\n  xlab(\"Miesiąc\") + ylab(\"Dzień tygodnia\") +\n  ggtitle(\"Liczba aktywności w czasie semestru\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa tej wizualizacji wyraźnie widać większą aktywność w środy - w ten dzień były publikowane wyniki z quizu. Czerwone kwadraty w czwartki w marcu i kwietniu wskazują na realizację trudniejszych zagadnień i próby prawidłowego rozwiązania quizu. Z kolei 9 czerwca upływał termin złożenia projektu i widać wyraźną mobilizację już od trzeciego czerwca. W ostatnim analizowanym tygodniu odbył się egzamin i także można zaobserwować większą aktywność studentów w poprzedzających dniach.\n\n\nPodsumowanie\nDane z logów systemu moodle pozwalają na całkiem ciekawą analizę zachowań. Można to zrobić globalnie albo analizować konkretnego studenta. Jeżeli zajęcia w kolejnym roku akademickim będą odbywały się stacjonarnie to będzie okazja do analizy porównawczej."
  },
  {
    "objectID": "posts/2020-01-24-smog-temperatura/smog-temperatura.html",
    "href": "posts/2020-01-24-smog-temperatura/smog-temperatura.html",
    "title": "Smog w Pszczynie a temperatura powietrza",
    "section": "",
    "text": "Zjawisko smogu oraz niskiej emisji jest w ostatnim czasie tematem bardzo często poruszanym przez wszystkie media. Na stronach Głównego Inspektoratu Ochrony Środowiska można znaleźć historyczne dane dotyczące zanieczyszczenia powietrza. Dodatkowo Instytut Meteorologii i Gospodarki Wodnej publikuje archiwalne dane pogodowe. Są one dostępne także poprzez pakiet R o nazwie imgw. Można także na bieżąco monitorować sytuację pogodową korzystając z API.\nDane są, zatem na przykładzie miasta Pszczyna w województwie śląskim zobaczymy co można z nich wydobyć.\n\nJakość powietrza\nStacja w Pszczynie zbiera dane od 2015 roku i znajduje się przy ulicy Bogedaina. Niestety zbiera informacje wyłącznie na temat stężenia pyłu PM10. Dane o jakości powietrza udostępniane są w postaci plików Excela i niezbędne informacje trzeba z nich wydobyć. Zrobiłem to ręcznie i do R wczytałem jeden plik ze wszystkimi danymi. Przy okazji uporządkowałem daty i na podstawie wartości norm pyłów sklasyfikowałem odpowiednie wartości.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(lubridate)\n\npm10_2015 &lt;- read_xlsx(\"data/pm10_pszczyna.xlsx\", sheet = \"2015\")\npm10_2016 &lt;- read_xlsx(\"data/pm10_pszczyna.xlsx\", sheet = \"2016\")\npm10_2017 &lt;- read_xlsx(\"data/pm10_pszczyna.xlsx\", sheet = \"2017\")\npm10_2018 &lt;- read_xlsx(\"data/pm10_pszczyna.xlsx\", sheet = \"2018\")\npm10_2019 &lt;- read_xlsx(\"data/pm10_pszczyna.xlsx\", sheet = \"2019\") %&gt;% \n  mutate(pm10=as.numeric(str_replace(pm10, \",\", \".\")))\n\npm10_pszczyna &lt;- union_all(pm10_2015, pm10_2016) %&gt;% \n  union_all(., pm10_2017) %&gt;% \n  union_all(., pm10_2018) %&gt;% \n  union_all(., pm10_2019) %&gt;% \n  mutate(data=as_date(data),\n         dzien_tyg=factor(wday(data),\n                          levels = 1:7, \n                          labels = c(\"Pn\", \"Wt\", \"Śr\", \"Czw\", \"Pt\", \"Sb\", \"Nd\"),\n                          ordered = T),\n         kwartal=str_c(\"Kwartał \", quarter(data)),\n         rok=as.factor(year(data)),\n         poziom=cut(x = pm10,\n                    breaks = c(0,50,200,300,Inf), \n                    labels = c(\"w normie\", \"dopuszczalny\", \"informowania\", \"alarmowy\"), \n                    ordered_result = T))\n\nW pierwszej kolejności ocenimy kompletność danych pod kątem liczby braków danych w każdym z analizowanych lat.\n\npm10_pszczyna %&gt;% \n  filter(is.na(pm10)) %&gt;% \n  count(rok) %&gt;% \n  knitr::kable()\n\n\n\n\nrok\nn\n\n\n\n\n2015\n7\n\n\n2016\n17\n\n\n2017\n2\n\n\n2018\n3\n\n\n2019\n7\n\n\n\n\n\nNiestety z niewiadomych przyczyn brakuje pomiarów dla kilku dni w każdym roku. Aż 17 dni w roku 2016 nie była zbierana informacja o stężeniu PM10. Był to okres od 3 do 18 marca. W pozostałych latach są to pojedyncze dni i nie stanowią znaczącej wartości względem całego roku. Te obserwacje zostaną z analizy wyeliminowane. Alternatywnie można zastosować wybraną metodę imputacji danych.\nNastępnie przeanalizujemy wartości stężenia PM10 w ujęciu czasowym.\n\np &lt;- pm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  ggplot(aes(x=data, y = pm10, color = poziom)) +\n  geom_point() +\n  scale_color_manual(name = \"Poziom\", values = c(\"#1a9850\", \"#fdae61\", \"#d73027\", \"#67001f\")) +\n  xlab(\"Data\") +\n  ylab(\"Stężenie PM10\") +\n  ggtitle(\"Stężenie pyłu PM10 w Pszczynie w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\np\n\n\n\n\nDla lat 2015-2019 wyraźnie widać kolejne pory roku - zimą stężenie pyłów PM10 jest wysokie, a latem niskie i nie przekracza normatywnego poziomu. Pod kątem rekordowo wysokich wartości warty odnotowania jest rok 2016 i 2017. Trzy najwyższe wartości występowały kolejno w dniach 9, 10, 11 stycznia 2017 roku.\nWarto bliżej się przyjrzeć zdefiniowanym normom i częstości występowania.\n\npm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  count(rok, poziom) %&gt;% \n  complete(rok, poziom, fill = list(n = 0)) %&gt;% \n  ggplot(aes(x = poziom, y = rok, fill = n)) +\n  geom_tile() +\n  geom_text(aes(label=n)) +\n  scale_fill_gradientn(colours = RColorBrewer::brewer.pal(4, \"RdYlGn\") , name = \"Liczba dni\") +\n  xlab(\"Poziom\") +\n  ylab(\"Rok\") +\n  ggtitle(\"Liczba dni w roku z danym poziomem stężenia pyłu PM10\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nW ostatnich 5 latach nie nastąpiła żadne spektakularne zwiększenie liczby dni ze stężeniem PM10 w normie. Można jedynie odnotować, że w latach 2018-2019 PM10 nie przekroczyły poziomu alarmowego.\nSpójrzmy jeszcze na te same dane w ujęciu procentowych.\n\npm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  count(rok, poziom) %&gt;% \n  complete(rok, poziom, fill = list(n = 0)) %&gt;% \n  group_by(rok) %&gt;% \n  mutate(proc=round(n/sum(n)*100,1)) %&gt;% \n  ggplot(aes(x = poziom, y = rok, fill = proc)) +\n  geom_tile() +\n  geom_text(aes(label=proc)) +\n  scale_fill_gradientn(colours = RColorBrewer::brewer.pal(4, \"RdYlGn\"), name = \"Odsetek\") +\n  xlab(\"Poziom\") +\n  ylab(\"Rok\") +\n  ggtitle(\"Odsetek dni w roku z danym poziomem stężenia pyłu PM10\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nWniosek z powyższego wykresu nie jest zbyt pokrzepiający - tylko przez co najwyżej 71% dni w roku można cieszyć się w miarę czystym powietrzem. Z kolei 1/3 dni w roku to dni z poziomem powyżej 50 jednostek.\nGłówny Inspektorat Ochrony Środowiska publikuje także dane na temat jakości powietrza w postaci indeksu jakości powietrza, którzy przyporządkowuje stężenie pyłu do jednej z sześciu kategorii.\n\n\n\nIndeks jakości powietrza\nPM10\n\n\n\n\nBardzo dobry\n0-20\n\n\nDobry\n20,1-50\n\n\nUmiarkowany\n50,1-80\n\n\nDostateczny\n80,1-110\n\n\nZły\n110,1-150\n\n\nBardzo zły\n&gt; 150\n\n\n\nKolejny wykres przedstawia liczbę dni z danym indeksem jakości powietrza zgodnie z powyższą klasyfikacją.\n\npm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  count(rok, indeks) %&gt;% \n  complete(rok, indeks, fill = list(n = 0)) %&gt;% \n  ggplot(aes(x = rok, y = n, fill = indeks)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label=n), position = position_dodge(width = 0.9), vjust = -0.3) +\n  scale_fill_manual(values = c(\"#57B108\",\"#B0DD10\",\"#FFD911\",\"#E58100\",\"#E50000\",\"#990000\"), \n                    name = \"Indeks powietrza\") +\n  xlab(\"Rok\") +\n  ylab(\"Liczba dni\") +\n  ylim(0,220) +\n  ggtitle(\"Liczba dni w roku z danym indeksem stężenia pyłu PM10\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nW roku 2019 dni z bardzo dobrym i dobrym indeksem powietrza było w sumie 251, w roku 2018 - 236, a w 2017 - 259. Z kolei takich dni, kiedy odnotowano zły i bardzo zły indeks powietrza było odpowiednio 20 (2019), 46 (2018) i 35 (2017).\nTe same dane można także przedstawić w ujęciu procentowym.\n\npm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  count(rok, indeks) %&gt;% \n  complete(rok, indeks, fill = list(n = 0)) %&gt;% \n  group_by(rok) %&gt;% \n  mutate(proc=round(n/sum(n)*100)) %&gt;% \n  ggplot(aes(x = rok, y = proc, fill = indeks)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label=proc), position = position_dodge(width = 0.9), vjust = -0.3) +\n  scale_fill_manual(values = c(\"#57B108\",\"#B0DD10\",\"#FFD911\",\"#E58100\",\"#E50000\",\"#990000\"), \n                    name = \"Indeks powietrza\") +\n  xlab(\"Rok\") +\n  ylab(\"Odsetek dni\") +\n  ylim(0,60) +\n  ggtitle(\"Odsetek dni w roku z danym indeksem stężenia pyłu PM10\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa podstawie danych z powyższego wykresu można stwierdzić, że 60% dni w roku 2019 jakość powietrza była dobra lub bardzo dobra. W roku 2018 te dni stanowiły 65%, a w 2017 - 72%. Z kolei zły i bardzo zły indeks powietrza dotyczył 6% dni 2019 roku, 13% dni 2018 roku oraz 10% dni dla roku 2017. Na podstawie tak krótkiego szeregu czasowego nie można jednak wnioskować na temat istnienia trendu.\nKolejny wykres przedstawia wykresy pudełkowe stężenia PM10 w podziale na lata i kwartały. Wykresy pudełkowe przedstawiają rozkład w postaci wartości minimalnej, kwartyla dolnego (25% obserwacji poniżej, 75% obserwacji powyżej), mediany (50% obserwacji poniżej, 50% obserwacji powyżej), kwartyla górnego (75% obserwacji poniżej, 25% obserwacji powyżej) oraz maksimum.\n\npm10_pszczyna %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  ggplot(aes(x=rok, y=pm10, fill=kwartal)) +\n  geom_boxplot() +\n  scale_fill_discrete(name = \"Kwartał\") +\n  xlab(\"Rok\") +\n  ylab(\"Stężenie PM10\") +\n  ggtitle(\"Rozkład stężenia pyłu PM10 w ujęciu kwartalnym w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nWykres przedstawia wyraźną tendencję występowania wysokich wartości PM10 w pierwszym i czwartym kwartale. Z kolei w drugich i czwartych kwartałach wartości maksymalne nieznacznie wykraczają ponad wartość 100, natomiast mediana nigdy nie przekracza przyjętego jako norma poziomu 50.\n\n\nTemperatura powietrza\nW drugiej części analizy wykorzystamy dane dotyczące temperatury.\n\nlibrary(imgw)\n\nd &lt;- meteo_daily(rank = \"climate\", year = 2015:2019)\n\nimgw_pszczyna &lt;- d %&gt;% \n  filter(station == \"PSZCZYNA\") %&gt;% \n  mutate(data=as_date(str_c(yy,\"-\",mm,\"-\",day)))\n\nZbiór danych zawiera wiele różnych cech takich jak: temperatura, wilgotność, wysokość pokrywy śnieżnej czy stopień zachmurzenia. Szczęśliwie zbiór nie zawiera braków danych. Interesującą nas zmienną będzie średnia dobowa temperatura, której wykres przedstawiony jest poniżej.\n\nggplot(imgw_pszczyna, aes(x=data, y=t2m_mean_daily)) +\n  geom_point(color = \"deepskyblue2\") +\n  xlab(\"Rok\") +\n  ylab(\"Średnia dobowa temperatura\") +\n  ggtitle(\"Średnia dobowa temperatura w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nPodobnie jak w przypadku stężenia PM10 dla średniej dobowej temperatury można stwierdzić występowanie sezonowości.\n\n\nJakość powietrza a temperatura powietrza\nWykorzystamy teraz zebrane dane do poszukania zależności pomiędzy stężeniem PM10 a temperaturą. W pierwszej kolejności zestawimy te dane na wykresie punktowym.\n\npm10_imgw &lt;- select(imgw_pszczyna, data, t2m_mean_daily) %&gt;% \n  inner_join(., select(pm10_pszczyna, data, pm10), by=\"data\") %&gt;% \n  filter(!is.na(pm10))\n\nggplot(pm10_imgw, aes(y=pm10, x=t2m_mean_daily)) +\n  geom_point() +\n  ylab(\"Stężenie PM10\") +\n  xlab(\"Średnia dobowa temperatura\") +\n  ggtitle(\"Zależność pomiędzy stężeniem PM10 i średnią dobową temperaturą \\nw latach 2015-2019\")+\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNie można tu wskazać zależności liniowej - są dni z niską temperaturą i niskim stężeniem PM10, a także takie, gdzie to stężenie jest bardzo wysokie. Faktem jest natomiast, że w upalne dni stężenie PM10 jest niskie. Obliczenie współczynnika korelacji liniowej Pearsona wskazuje na wartość \\(r=-0,53\\) czyli umiarkowaną zależność ujemną. Przy czym jest to tylko informacja o współwystępowaniu obu zjawisk, a nie zależności przyczynowo-skutkowej.\nNa kolejnym wykresie zestawimy obie analizowane cechy z uwzględnieniem czasu.\n\npm10_imgw %&gt;% \n  pivot_longer(-data) %&gt;% \n  mutate(name=factor(name, \n                     levels = c(\"pm10\", \"t2m_mean_daily\"), \n                     labels = c(\"PM10\", \"Temperatura\"))) %&gt;% \n  ggplot(aes(x=data, y=value, color=name)) +\n  geom_point(alpha = 0.5) +\n  scale_color_manual(name = \"\", values = c(\"grey50\", \"deepskyblue2\")) +\n  xlab(\"Rok\") +\n  ylab(\"Wartość cechy\") +\n  ggtitle(\"Stężenie PM10 i średnia dobowa temperatura w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nWidoczne jest wzajemne przeciwstawianie się obserwacji, natomiast ze względu na to, że te cechy są mierzone na różnych skalach to trudno te wielkości porównywać. Zatem zostanie zastosowana standaryzacja, co umożliwi porównanie wartości cech.\n\npm10_imgw_std &lt;- select(imgw_pszczyna, data, t2m_mean_daily) %&gt;% \n  inner_join(., select(pm10_pszczyna, data, pm10), by=\"data\") %&gt;% \n  filter(!is.na(pm10)) %&gt;% \n  mutate_if(is.numeric, scale) %&gt;% \n  pivot_longer(-data) %&gt;% \n  mutate(name=factor(name, \n                     levels = c(\"pm10\", \"t2m_mean_daily\"), \n                     labels = c(\"PM10\", \"Temperatura\")))\n\nggplot(pm10_imgw_std, aes(x=data, y=value, color=name)) +\n  geom_point(alpha = 0.5) +\n  # geom_smooth(se = F, size = 2) +\n  scale_color_manual(name = \"\", values = c(\"grey50\", \"deepskyblue2\")) +\n  xlab(\"Rok\") +\n  ylab(\"Standaryzowana wartość cechy\") +\n  ggtitle(\"Stężenie PM10 i średnia dobowa temperatura w latach 2015-2019\") +\n  labs(caption = \"Łukasz Wawrowski - wawrowski.edu.pl\") +\n  theme_light() +\n  theme(legend.position = \"bottom\",\n        plot.caption = element_text(color = \"grey80\"))\n\n\n\n\nNa powyższej grafice widać wyraźnie, że podczas dni z wysoką temperaturą powietrza poziom stężenia PM10 utrzymuje się na niskim poziomie. Z kolei spadek temperatury w większości przypadków pociąga za sobą wzrost stężenia PM10.\n\n\nPodsumowanie\nW tym poście zostały przedstawione tylko dwie cechy - poziom pyłów PM10 oraz średnia dobowa temperatura powietrza. W analizie smogu należałoby uwzględnić także inne cechy związane z warunkami atmosferycznymi. Na podstawie wielu takich czynników można pokusić się o stworzenie modelu prognozującego stężenie pyłów PM10.\nZbiory danych i kody użyte w analizie znajdują się w repozytorium na githubie."
  },
  {
    "objectID": "posts/2017-10-30-analiza-danych-lotto/analiza-danych-lotto.html",
    "href": "posts/2017-10-30-analiza-danych-lotto/analiza-danych-lotto.html",
    "title": "Analiza danych lotto",
    "section": "",
    "text": "Impulsem do przeprowadzenia analizy danych pochodzących z losowań Lotto była konieczność przedstawienia uczniom liceum przykładu losowania prostego. Czy faktycznie każda liczba ma takie samo prawdopodobieństwo wylosowania?\nLotto, dawniej Toto-Lotek oraz Duży Lotek to losowanie 6 liczb z 49. Dużo ciekawych informacji na temat samej gry oraz jej historii można znaleźć na stronie wikipedii.\nDane w formacie tekstowym można pobrać ze strony mbnet.com.pl. Po wczytaniu do R dokonałem niezbędnego przekształcenia danych.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(zoo)\n\nd &lt;- read.table(\"http://www.mbnet.com.pl/dl.txt\", sep=\" \", dec=\".\")\nnames(d) &lt;- c(\"obs\", \"date\", \"numbers\")\n\nd &lt;- d %&gt;%\n  separate(numbers, into = c(\"number1\", \"number2\", \"number3\", \"number4\", \"number5\", \"number6\")) %&gt;%\n  separate(date, into = c(\"day\", \"month\", \"year\"), remove = F) %&gt;%\n  mutate(date_f=as.POSIXct(strptime(date, \"%d.%m.%Y\"))) %&gt;%\n  filter(year&lt;2018)\n\nDo końca 2017 roku przeprowadzono 6041 losowań. Żadna sekwencja liczb się nie powtórzyła.\nW pierwszej kolejności sprawdźmy ile razy dana liczba została już wylosowana.\n\nfreqs &lt;- d %&gt;%\n  select(starts_with(\"number\")) %&gt;%\n  gather(number, value) %&gt;%\n  mutate(value=as.numeric(value))\n\ncounts_num &lt;- freqs %&gt;%\n  count(value)\n  \nggplot(counts_num, aes(x=as.factor(value), n)) +\n  geom_bar(stat=\"identity\", fill=\"#FCBD10\") +\n  xlab(\"Liczba\") + ylab(\"Częstość\") + \n  theme_light() +\n  theme(axis.text.x = element_text(size = 7))\n\n\n\n\nOkazuje się, że rozkład liczb losowanych w Lotto jest zbliżony do rozkładu jednostajnego. Najrzadziej losowana była liczba 43 - 667 razy, a najczęściej liczba 34 - 807 razy.\nZ kolei do przedstawienia rozkładu liczb na poszczególnych pozycjach najlepiej sprawdzi się wykres skrzypcowy - połączenie wykresu pudełkowego z rozkładem gęstości.\n\nggplot(freqs, aes(x=number, y=value)) + \n  geom_violin(fill=\"#FCBD10\") +\n  xlab(\"Pozycja liczby\") + ylab(\"Wylosowana liczba\") + \n  scale_x_discrete(labels = 1:6) +\n  theme_light()\n\n\n\n\nRozkład liczb według pozycji nie zaskakuje. Na pierwszym miejscu dominują liczby z początku przedziału [1;49], a na miejscu 6 z końca tego przedziału. Niemniej jednak zdarzyły się takie losowania, jak to z 15 października 2015 roku, w którym padły liczby wyłącznie z czwartej dziesiątki: 40, 41, 42, 46, 47, 49. Z drugiej strony można przywołać 5 grudnia 1987 roku i wylosowane liczby: 1, 2, 8, 10, 11, 12.\nDane z losowań Lotto pokazują także jak zmieniała się liczba losowań w ciągu ostatnich 60 lat.\n\ndraw_y &lt;- d %&gt;%\n  filter(year!=\"2017\") %&gt;%\n  group_by(year) %&gt;%\n  count()\n\nggplot(draw_y, aes(x=year, y=n, group = 1)) +\n  geom_line(color=\"#FCBD10\", size=2) +\n  geom_point(color=\"#FCBD10\", size=3) +\n  xlab(\"Rok\") + ylab(\"Liczba losowań\") + \n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7))\n\n\n\n\nW latach 1957-1974 losowania odbywały się raz w tygodniu. Od 1975 wprowadzono drugie losowanie, a w roku 2007 trzecie.\nNa podstawie tych samych danych można jeszcze sprawdzić w jakie dni tygodnia odbywały się te losowania.\n\nweek_d &lt;- d %&gt;%\n  mutate(weekday=weekdays(date_f)) %&gt;%\n  group_by(year, weekday) %&gt;%\n  count() %&gt;%\n  ungroup() %&gt;%\n  mutate(weekday=ordered(weekday, \n                         levels=rev(c(\"wtorek\", \"środa\", \"czwartek\", \"sobota\", \"niedziela\")),\n                         labels=rev(c(\"wtorek\", \"środa\", \"czwartek\", \"sobota\", \"niedziela\"))))\n  \nggplot(week_d, aes(year, weekday)) + \n  geom_tile(aes(fill = n), colour = \"white\") +\n  # geom_text(aes(label = n), size=3, color = \"white\") +\n  scale_fill_gradient(\"Liczba losowań\", limits=c(0,max(week_d$n))) + \n  xlab(\"Rok\") + ylab(\"Dzień tygodnia\") +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n        axis.text.y = element_text(angle = 90, hjust = 0.5),\n        legend.position=\"bottom\")\n\n\n\n\nW pierwszych latach losowania były przeprowadzane w niedzielę. W 1984 roku przeniesiono je na sobotę, a w 1991 roku dodano losowane środowe. Od 2007 roku losowania Lotto odbywają się we wtorki, czwartki i soboty.\nPodobne zestawienie można utworzyć dla dni i miesięcy.\n\ndraw_dm &lt;- d %&gt;%\n  group_by(month, day) %&gt;%\n  count()\n\nggplot(draw_dm, aes(day, month)) + \n  geom_tile(aes(fill = n), colour = \"white\") +\n  geom_text(aes(label = n), size=3, color = \"white\") +\n  scale_fill_gradient(\"Liczba losowań\", limits=c(0,max(draw_dm$n))) + \n  xlab(\"Dzień\") + ylab(\"Miesiąc\") +\n  theme_light() +\n  theme(legend.position=\"bottom\")\n\n\n\n\nW tym przypadku bez niespodzianek - żadna data nie była dyskryminowana. Oprócz 29 lutego - w ciągu 60 lat w ten dzień odbyły się tylko 3 losowania.\nSzansa trafienia “szóstki” w Lotto to 1 do 13 983 816 czyli bardzo mało. Z wykorzystaniem analizy częstości można wytypować najczęściej losowane “dwójki” czy “trójki”, ale należy pamiętać, że wylosowanie każdej kombinacji liczb jest tak samo prawdopodobne. Ciekawie na ten temat mówi Dan Gilbert w prezentacji Dlaczego podejmujemy złe decyzje.\nKod w jednym kawałku dostępny jest na githubie."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Łukasz Wawrowski",
    "section": "",
    "text": "machine learning\nexplainable artificial intelligence\nsmall area estimation\npoverty analysis"
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "Łukasz Wawrowski",
    "section": "",
    "text": "machine learning\nexplainable artificial intelligence\nsmall area estimation\npoverty analysis"
  },
  {
    "objectID": "about.html#current-employment",
    "href": "about.html#current-employment",
    "title": "Łukasz Wawrowski",
    "section": "Current employment",
    "text": "Current employment\nResearch Network Łukasiewicz | Data analyst"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Łukasz Wawrowski",
    "section": "Education",
    "text": "Education\nPoznań University of Economics and Business\nPh.D. in statistics | September 2013 - October 2017\nPoznań University of Economics and Business\nB.Sc & M.Sc. in informatics and econometrics | September 2008 - July 2013"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Łukasz Wawrowski",
    "section": "",
    "text": "Dydaktyka w czasie pandemii - analiza logów z systemu moodle\n\n\n\n\n\nAnaliza aktywności studentów na podstawie logów systemu moodle.\n\n\n\n\n\n\nJun 26, 2020\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nSmog w Pszczynie a kierunek i siła wiatru\n\n\n\n\n\nW poszukiwaniu związku pomiędzy stężeniem PM10 a wiatrem w Pszczynie.\n\n\n\n\n\n\nFeb 18, 2020\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nSmog w Pszczynie a temperatura powietrza\n\n\n\n\n\nW poszukiwaniu związku pomiędzy stężeniem PM10 a temperaturą powietrza w Pszczynie.\n\n\n\n\n\n\nJan 24, 2020\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nDomy Hogwartu - częstość występowania w książkach\n\n\n\n\n\nSprawdźmy który dom Hogwartu jest najczęściej wymieniany w serii o Harrym Potterze.\n\n\n\n\n\n\nJan 12, 2020\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nAnaliza podobieństwa imion\n\n\n\n\n\nWykorzystanie miar podobieństwa tekstów na przykładzie polskich imion.\n\n\n\n\n\n\nApr 12, 2018\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nTworzenie ankiet Google z poziomu kodu\n\n\n\n\n\nZamiast wyklikiwania ankiet w Google Forms można wykorzystać JavaScript i zautomatyzować swoją pracę.\n\n\n\n\n\n\nDec 28, 2017\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nNajpopularniejsze imię wśród papieży\n\n\n\n\n\nIlościowa analiza imion papieży na podstawie danych z Wikipedii.\n\n\n\n\n\n\nNov 9, 2017\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\n  \n\n\n\n\nAnaliza danych lotto\n\n\n\n\n\nJakie liczby wypadają w losowaniach Lotto najrzadziej, a jakie najczęściej? Analiza danych z 60 lat.\n\n\n\n\n\n\nOct 30, 2017\n\n\nŁukasz Wawrowski\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dydaktyka.html",
    "href": "dydaktyka.html",
    "title": "Dydaktyka",
    "section": "",
    "text": "Metody przetwarzania i analizy danych w R\nPodstawy programowania w R\nStatystyka opisowa\nMetoda reprezentacyjna\nGeomarketing\nPrzetwarzanie danych w SAS"
  },
  {
    "objectID": "publikacje.html",
    "href": "publikacje.html",
    "title": "Publikacje",
    "section": "",
    "text": "Google Scholar\n\nResearchGate\n\nORCID\n\n\nExplainable artificial intelligence\nJanusz, A., Zalewska, A., Wawrowski, Ł., Biczyk, P., Ludziejewski, J., Sikora, M., Ślęzak, D., 2023, BrightBox — A rough set based technology for diagnosing mistakes of machine learning models, Applied Soft Computing, 110285.\nBiczyk P., Wawrowski, Ł., 2023, Detecting and Isolating Adversarial Attacks Using Characteristics of the Surrogate Model Framework. Applied Sciences. 13(17):9698.\nBiczyk, P., Wawrowski, Ł., 2023, Towards automated detection of adversarial attacks on tabular data, In: Ganzha, M., Maciaszek, L., Paprzycki, M., Ślęzak, D. (eds) Proceedings of the 18th Conference on Computer Science and Intelligence Systems. Annals of Computer Science and Information Systems, vol. 35, pp. 247–251. https://doi.org/10.15439/2023F3838\n\n\nCyberbezpieczeństwo i wykrywanie anomalii\nMichalak, M., Biczyk, P., Adamczyk, B., Brzęczek, M., Hermansa, M., Kostorz, I., Wawrowski, Ł., Czerwiński, M. 2023, A New Data Model for Behavioral Based Anomaly Detection in IoT Device Monitoring. In International Joint Conference on Rough Sets, pp. 599-611. Cham: Springer Nature Switzerland.\nCzerwiński, M., Michalak, M., Biczyk, P., Adamczyk, B., Iwanicki, D., Kostorz, I., Brzęczek, M., Janusz, A., Hermansa, M., Wawrowski, Ł., Kozłowski, A., 2023, Cybersecurity threat detection in the behavior of IOT devices: Analysis of data mining competition results, In: Ganzha, M., Maciaszek, L., Paprzycki, M., Ślęzak, D. (eds) Proceedings of the 18th Conference on Computer Science and Intelligence Systems. Annals of Computer Science and Information Systems, vol. 35, pp. 1289–1293. http://dx.doi.org/10.15439/2023F3089\nWawrowski Ł, Białas A, Kajzer A, Kozłowski A, Kurianowicz R, Sikora M, Szymańska-Kwiecień A, Uchroński M, Białczak M, Olejnik M, Michalak M., 2023, Anomaly Detection Module for Network Traffic Monitoring in Public Institutions, Sensors, 23(6):2974. https://doi.org/10.3390/s23062974\nAdamczyk, B., Brzȩczek, M., Michalak, M., Kostorz, I., Wawrowski, Ł., Hermansa, M., Czerwiński, M., Jamiołkowski, A., 2022, Dataset Generation Framework for Evaluation of IoT Linux Host–Based Intrusion Detection Systems, 2022 IEEE International Conference on Big Data, 6179-6187, DOI: 10.1109/BigData55660.2022.10020442\nWawrowski Ł., Michalak M., Białas A., Kurianowicz R., Sikora M., Uchroński M., Kajzer A.: Detecting Anomalies and Attacks in Network Traffic Monitoring with Classification Methods and XAI-based Explainability, Procedia Computer Science, 192:2259-2268, 2021\nMichalak M., Wawrowski Ł., Sikora M., Kurianowicz R., Kozłowski A., Białas A.: Open-source-based Environment for Network Traffic Anomaly Detection, (Proceedings of the 16th International Conference on Dependability of Computer Systems DepCoS-RELCOMEX), Advances in Intelligent Systems and Computing, 1389:284-295, 2021\nMichalak, M., Wawrowski, Ł., Sikora, M., Kurianowicz, R., Kozłowski, A., Białas, A., 2021, Outlier Detection in Network Traffic Monitoring, In Proceedings of the 10th International Conference on Pattern Recognition Applications and Methods - Volume 1: ICPRAM, ISBN 978-989-758-486-2, s. 523-530, DOI: 10.5220/0010238205230530\n\n\nUbóstwo i wykluczenie społeczne\nWawrowski, Ł., Beręsewicz, M., 2021, Small area estimates of the low work intensity indicator at voivodeship level in Poland, Statistics in Transition New Series, vol. 22(2), s. 155-172\nWawrowski, Ł., 2020, Estymacja pośrednia wskaźników ubóstwa na poziomie powiatów, Wiadomości Statystyczne, vol. 65 (8), s. 7-26\nBeręsewicz, M., Marchetti, S., Salvati, N., Szymkowiak, M., Wawrowski, Ł., 2018, The use of a three level M-quantile model to map poverty at LAU 1 in Poland, Journal of the Royal Statistical Society Series A, 181 (4), s. 1077-1104.\nMłodak, A., Szymkowiak, M., Wawrowski, Ł., 2017, Mapping poverty at the level of subregions in Poland using indirect estimation, Statistics in Transition - new series, 18 (4), s. 609-635.\nWawrowski, Ł., 2016, The Spatial Fay-Herriot Model in Poverty Estimation, Folia Oeconomica Stetinensia, 16 (2), s. 191-202.\nMłodak, A., Józefowski, T., Wawrowski, Ł., 2016, Zastosowanie metod taksonomicznych w estymacji wskaźników ubóstwa, Wiadomości Statystyczne, 2, s. 1-24.\nWawrowski, Ł., 2015, Estymacja zasięgu ubóstwa w podregionach Polski z wykorzystaniem modelu Faya-Herriota w: Dorota Appenzeller (red.), Matematyka i informatyka na usługach ekonomii : analityka gospodarcza, metody i narzędzia., Wydawnictwo Uniwersytetu Ekonomicznego w Poznaniu (UEP), Poznań, s. 171-182.\nWawrowski, Ł., 2014, Wykorzystanie metod statystyki małych obszarów do tworzenia map ubóstwa w Polsce, Wiadomości Statystyczne, Zakład Wydawnictw Statystycznych, numer 9 (640), Warszawa.\nWawrowski, Ł., 2012, Analiza ubóstwa w przekroju powiatów w województwie wielkopolskim z wykorzystaniem metod statystyki małych obszarów, Przegląd Statystyczny, Numer specjalny 2, PAN Warszawska Drukarnia Naukowa, Warszawa.\n\n\nEstymacja dla małych przedsiębiorstw\nDehnel, G., Wawrowski, Ł., 2020, Robust estimation of wages in small enterprises: the application to Poland’s districts, Statistics in Transition new series, vol. 21 (1), s. 137-157\nDehnel, G., Wawrowski, Ł., 2019, Estimation of the average wage in Polish small companies using the robust approach, Przegląd Statystyczny, Statistical Review, vol. 66 (3), s. 200-213\nDehnel, G., Wawrowski, Ł., 2019, Unit level models in the assessment of monthly wages of small enterprises employees, in M. Papież and S. Śmiech (Eds.), The 13th Professor Aleksander Zelias International Conference on Modelling and Forecasting of Socio-Economic Phenomena. Conference Proceedings. Warszawa: Wydawnictwo C.H. Beck, s. 35-43. ISBN: 978-83-8158-734-1 (pdf)\nDehnel, G., Wawrowski, Ł., 2018, Robust estimation of revenues of Polish small companies by NACE section and province, The Socio-Economic Modelling and Forecasting, 1, s. 110-119.\nDehnel, G., Pietrzak, M., Wawrowski, Ł., 2017, An evaluation of company performance using Fay-Herriot model, Argumenta Oeconomica Cracoviensia, 16, s. 23-36.\nDehnel, G., Pietrzak, M., Wawrowski, Ł., 2017, Estymacja przychodu przedsiębiorstw na podstawie modelu Faya-Herriota, Przegląd Statystyczny, 64 (1), s. 79-94.\n\n\nPozostałe\nHenzel J., Wawrowski Ł., Kubina A., Sikora M., Wróbel Ł., 2022, Demand forecasting in the fashion business — an example of customized nearest neighbour and linear mixed model approaches, 2022 17th Conference on Computer Science and Intelligence Systems (FedCSIS), pp. 61-65, doi: 10.15439/2022F256.\nKulis J., Wawrowski Ł., Sędek Ł., Wróbel Ł., Słota Ł., van der Velden VHJ, Szczepański T., Sikora M., 2022, Machine Learning Based Analysis of Relations between Antigen Expression and Genetic Aberrations in Childhood B-Cell Precursor Acute Lymphoblastic Leukaemia, Journal of Clinical Medicine, 11(9):2281. https://doi.org/10.3390/jcm11092281\nSoniewicki, M., Wawrowski, Ł., 2017, The use of Knowledge Management Systems in the Polish Private and State Owned Companies, Journal of Information and Organizational Sciences, 41 (2), s. 263-282.\nŁysoń, P., Szymkowiak, M., Wawrowski, Ł., 2016, Badania porównawcze atrakcyjności turystycznej powiatów z uwzględnieniem ich otoczenia, Wiadomości Statystyczne, 12, s. 45-57\nSoniewicki, M. and Wawrowski, Ł., 2015, The use of external knowledge sources by Polish private-owned and state-owned enterprises in the internationalization process, Journal of Economics and Management, 22, s. 75-96\nWawrowski, Ł., 2012, Wykorzystanie metod porządkowania liniowego do analizy lokalnego rynku pracy, I Ogólnopolska Konferencja Naukowa ,,Narzędzia Analityczne w Naukach Ekonomicznych’’ - recenzowane materiały pokonferencyjne, Fundacja Uniwersytetu Ekonomicznego w Krakowie, Kraków."
  }
]